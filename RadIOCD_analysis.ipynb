{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RadIOCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T09:23:27.422265Z",
     "start_time": "2024-02-05T09:23:27.417620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download RadIOCD\n",
    "# !wget https://zenodo.org/records/10554924/files/RadIOCD.zip\n",
    "# !wget https://zenodo.org/records/10554924/files/df_4_annotation.csv\n",
    "# !unzip RadIOCD.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T09:23:29.590707Z",
     "start_time": "2024-02-05T09:23:29.585663Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T09:23:35.095730Z",
     "start_time": "2024-02-05T09:23:35.089435Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'dataset/'\n",
    "subjects = ['subject_1/', 'subject_2/', 'subject_3/', 'subject_4/', 'subject_5/', \n",
    "            'subject_6/', 'subject_7/', 'subject_8/', 'subject_9/', 'subject_10/'] \n",
    "\n",
    "classes = {'wall':4,'human':3,'desk':2,'chair':1,'backpack':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-05T09:23:36.483049Z",
     "start_time": "2024-02-05T09:23:35.901861Z"
    }
   },
   "outputs": [],
   "source": [
    "# read df_4_annotation.csv to simplify io and feature extraction\n",
    "df_4_annotation = pd.read_csv('df_4_annotation.csv')\n",
    "dfs_csvs = df_4_annotation['CSV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T09:27:15.558218Z",
     "start_time": "2024-01-23T09:22:30.798734Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5776/5776 [04:44<00:00, 20.28it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold_frames = 8 #how many frames in a csv file should contain the object of interest\n",
    "threshold_gap = 2 #how many frames in a specified window should not contain the object of interest\n",
    "window = 10 #how many frames contitute a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "# init features daraframe\n",
    "features = pd.DataFrame({'object':[],'subject':[],'min_X':[],'max_X':[],'mean_X':[],'std_X':[],'env':[],\n",
    "                                        'min_Y':[],'max_Y':[],'mean_Y':[],'std_Y':[],\n",
    "                                        'min_Z':[],'max_Z':[],'mean_Z':[],'std_Z':[],'csv':[]})\n",
    "\n",
    "for i in tqdm(dfs_csvs):\n",
    "    cdf = pd.read_csv(i)\n",
    "    splits = i.split('/')\n",
    "    current_subject = splits[1]\n",
    "    current_env = splits[2]\n",
    "    current_object = splits[3]\n",
    "    current_csv = i\n",
    "    \n",
    "    #check if there is an object detected in the current csv file\n",
    "    if 'Presence' in cdf.columns:\n",
    "        #check how many frames contain an object \n",
    "        un_frames = cdf['Frame #'][cdf['Presence']==1].unique()\n",
    "\n",
    "        if len(un_frames)<threshold_frames:\n",
    "            continue\n",
    "        else:\n",
    "            for gr in un_frames:\n",
    "                array = np.arange(gr,gr+window)\n",
    "                set1 = set(array) \n",
    "                set2 = set(un_frames) \n",
    "                \n",
    "                #if the are more \"empty\" frames than the defined threshold continue\n",
    "                if len(set1.union(set2))-len(set2)>threshold_gap:\n",
    "                    continue\n",
    "                elif (array[-1] not in cdf['Frame #'].values):\n",
    "                    continue\n",
    "                else:\n",
    "                    current_cdf = cdf[(cdf['Frame #']>=array[0]) & (cdf['Frame #']<=array[-1])]\n",
    "                    if len(current_cdf['Frame #'].unique())<(window-1):\n",
    "                        print('Error less than expected frames')\n",
    "                    points_df = current_cdf[current_cdf['Presence']==1]\n",
    "                    \n",
    "                    #Get values only if the object is closer than 2.5 meters\n",
    "                    if np.mean(points_df['Y'])<2.5:\n",
    "                        current_features = pd.DataFrame({'object':[current_object],'subject':[current_subject],'env':[current_env],\n",
    "                                                         'min_X':[np.min(points_df['X'])],'max_X':[np.max(points_df['X'])],\n",
    "                                                         'mean_X':[np.mean(points_df['X'])],'std_X':[np.std(points_df['X'])],\n",
    "                                                         'min_Y':[np.min(points_df['Y'])],'max_Y':[np.max(points_df['Y'])],\n",
    "                                                         'mean_Y':[np.mean(points_df['Y'])],'std_Y':[np.std(points_df['Y'])],\n",
    "                                                         'min_Z':[np.min(points_df['Z'])],'max_Z':[np.max(points_df['Z'])],\n",
    "                                                         'mean_Z':[np.mean(points_df['Z'])],'std_Z':[np.std(points_df['Z'])],\n",
    "                                                        'n_points':[current_cdf['Presence'].sum()],'csv':[current_csv]})\n",
    "                        features = pd.concat([features,current_features],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:28.274200Z",
     "start_time": "2024-01-23T16:43:27.530891Z"
    }
   },
   "outputs": [],
   "source": [
    "# store features to csv file\n",
    "features.to_csv('features.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:29.666671Z",
     "start_time": "2024-01-23T16:43:29.541344Z"
    }
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:30.451854Z",
     "start_time": "2024-01-23T16:43:30.391804Z"
    }
   },
   "outputs": [],
   "source": [
    "features = features.replace(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:31.524203Z",
     "start_time": "2024-01-23T16:43:31.418612Z"
    }
   },
   "outputs": [],
   "source": [
    "features = features.replace({'subject_1': 1,'subject_2': 2,'subject_3': 3,\n",
    "                            'subject_4': 4,'subject_5': 5,'subject_6': 6,\n",
    "                            'subject_7': 7,'subject_8': 8,'subject_9': 9,'subject_10': 10, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:32.344159Z",
     "start_time": "2024-01-23T16:43:32.338858Z"
    }
   },
   "outputs": [],
   "source": [
    "#define set of features\n",
    "feats = ['min_X', 'max_X', 'mean_X', 'std_X','min_Y', 'max_Y', 'mean_Y', 'std_Y', 'min_Z', \n",
    "         'max_Z', 'mean_Z','std_Z','n_points']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML subject-dependent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:33.667550Z",
     "start_time": "2024-01-23T16:43:33.283859Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:34.787439Z",
     "start_time": "2024-01-23T16:43:34.782354Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:43:35.738082Z",
     "start_time": "2024-01-23T16:43:35.731691Z"
    }
   },
   "outputs": [],
   "source": [
    "def z_score(X_train, X_test):\n",
    "    x_means = np.zeros(len(feats))\n",
    "    x_stds = np.zeros(len(feats))\n",
    "    for i in range(len(feats)):\n",
    "        x_means[i] = np.mean(X_train[:,i])\n",
    "        x_stds[i] = np.std(X_train[:,i])\n",
    "\n",
    "        X_train[:,i] = (X_train[:,i]-x_means[i])/x_stds[i]\n",
    "        X_test[:,i] = (X_test[:,i]-x_means[i])/x_stds[i]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:44:35.705917Z",
     "start_time": "2024-01-23T16:43:44.280694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9897923525408766\n",
      "Test: 0.7400193174500966\n",
      "0.7241721191681713\n",
      "Train: 0.9892019555947654\n",
      "Test: 0.7475303643724697\n",
      "0.7277495383038914\n",
      "Train: 0.9889526081570859\n",
      "Test: 0.7226828667597077\n",
      "0.6980424458036256\n",
      "Train: 0.9898085549429885\n",
      "Test: 0.73265860281412\n",
      "0.7122155595068923\n",
      "Train: 0.9892190929041698\n",
      "Test: 0.7397933254277486\n",
      "0.7243215544869559\n",
      "Train: 0.9884282819391339\n",
      "Test: 0.7269348310894523\n",
      "0.697011272957233\n",
      "Train: 0.9902358075399736\n",
      "Test: 0.7443360343972217\n",
      "0.7208201528130234\n",
      "Train: 0.9895281248569728\n",
      "Test: 0.7270363224561698\n",
      "0.6997663540483413\n",
      "Train: 0.9902534648749164\n",
      "Test: 0.7198099841653471\n",
      "0.7069764561367288\n",
      "Train: 0.9893428715550052\n",
      "Test: 0.7233533921358503\n",
      "0.7011941373503583\n",
      "\n",
      "0.7324155041068183\n",
      "0.711226959057522\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "conf_mats = []\n",
    "f1s = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(features['csv'].unique())):\n",
    "    csv_train = features['csv'].unique()[train_index]\n",
    "    csv_test = features['csv'].unique()[test_index]\n",
    "\n",
    "    X_train = features[feats][(features['csv'].isin(csv_train))].to_numpy()\n",
    "    y_train = features['object'][(features['csv'].isin(csv_train))].to_numpy()\n",
    "\n",
    "    X_test = features[feats][(features['csv'].isin(csv_test))].to_numpy()\n",
    "    y_test = features['object'][(features['csv'].isin(csv_test))].to_numpy()\n",
    "    \n",
    "    X_train, X_test = z_score(X_train, X_test)\n",
    "      \n",
    "    # change this accordingly\n",
    "    rf = RandomForestClassifier(n_estimators=30, max_depth=20)\n",
    "    rf.fit(X_train,y_train)\n",
    "    \n",
    "    test_acc.append(np.mean(rf.predict(X_test)==y_test))\n",
    "    preds = rf.predict(X_test)\n",
    "    \n",
    "    print('Train:',np.mean(rf.predict(X_train)==y_train))\n",
    "    print('Test:',np.mean(rf.predict(X_test)==y_test))\n",
    "    print(f1_score(y_test, preds, average='macro'))\n",
    "\n",
    "    conf_mats.append(confusion_matrix(y_test,preds, labels=[0,1,2,3,4]))\n",
    "    f1s.append(f1_score(y_test, preds, average='macro'))\n",
    "print()\n",
    "print(np.mean(test_acc))\n",
    "print(np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:46:59.492553Z",
     "start_time": "2024-01-23T16:46:58.841802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 1000\n",
    "%matplotlib inline\n",
    "classe = [\"Backpack\", \"Chair\", \"Desk\", \"Human\", \"Wall\"]\n",
    "\n",
    "mean_cf = np.mean([i for i in conf_mats],0)\n",
    "mean_cf\n",
    "\n",
    "df_cm = pd.DataFrame(mean_cf.astype(int), index = [i for i in classe],columns = [i for i in classe])\n",
    "plt.figure(figsize = (6,5))\n",
    "\n",
    "\n",
    "ax = sn.heatmap(df_cm, annot=True,fmt='g',cmap='Blues')\n",
    "\n",
    "ax.set_xlabel(xlabel='Actual class', fontsize=14, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel(ylabel='Predicted class', fontsize=14, fontdict=dict(weight='bold'))\n",
    "\n",
    "plt.savefig('conf_mat_ppg_cross_attention.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Subject Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:36:09.417189Z",
     "start_time": "2024-01-23T16:35:18.292691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9891361521343314\n",
      "Test: 0.7920738814668699\n",
      "0.7835653279239463\n",
      "Train: 0.9903763246460657\n",
      "Test: 0.6316976027654618\n",
      "0.5767967641156023\n",
      "Train: 0.9917501364187084\n",
      "Test: 0.68458176799816\n",
      "0.6432691165139571\n",
      "Train: 0.9896561094699194\n",
      "Test: 0.5222175201015659\n",
      "0.5069299110996992\n",
      "Train: 0.9883179251052197\n",
      "Test: 0.6691073219658977\n",
      "0.6177318424493948\n",
      "Train: 0.9892493348025909\n",
      "Test: 0.6238868517548455\n",
      "0.5688031806424405\n",
      "Train: 0.9900323636136694\n",
      "Test: 0.6304565144951683\n",
      "0.5763580285302033\n",
      "Train: 0.9893872024299647\n",
      "Test: 0.6518037761294673\n",
      "0.6297823808515872\n",
      "Train: 0.9913930943744461\n",
      "Test: 0.6162641863640242\n",
      "0.602698964648189\n",
      "Train: 0.9873152458134747\n",
      "Test: 0.8744744744744745\n",
      "0.87308499445102\n",
      "\n",
      "0.6696563897515934\n",
      "0.637902051122604\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "conf_mats = []\n",
    "f1s = []\n",
    "for s in range(1,11):\n",
    "    X_train = features[feats][(features['subject']!=s)].to_numpy()\n",
    "    y_train = features['object'][(features['subject']!=s)].to_numpy()\n",
    "\n",
    "    X_test = features[feats][(features['subject']==s)].to_numpy()\n",
    "    y_test = features['object'][(features['subject']==s )].to_numpy()\n",
    "    \n",
    "    X_train, X_test = z_score(X_train, X_test)\n",
    "        \n",
    "    rf = RandomForestClassifier(n_estimators=30, max_depth=20)\n",
    "    rf.fit(X_train,y_train)\n",
    "    \n",
    "    test_acc.append(np.mean(rf.predict(X_test)==y_test))\n",
    "    preds = rf.predict(X_test)\n",
    "    \n",
    "    print('Train:',np.mean(rf.predict(X_train)==y_train))\n",
    "    print('Test:',np.mean(rf.predict(X_test)==y_test))\n",
    "    print(f1_score(y_test, preds, average='macro'))\n",
    "\n",
    "    conf_mats.append(confusion_matrix(y_test,preds, labels=[0,1,2,3,4]))\n",
    "    f1s.append(f1_score(y_test, preds, average='macro'))\n",
    "print()\n",
    "print(np.mean(test_acc))\n",
    "print(np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-23T16:46:42.172839Z",
     "start_time": "2024-01-23T16:46:42.163842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1489.5,  292.3,   44.8,  132.4,   62.1],\n",
       "       [ 299. , 1886.3,  128.4,  310.3,  113.7],\n",
       "       [  77.8,  197.8,  621.7,  211.6,  120.4],\n",
       "       [  51.8,  250.1,   93.7, 2751.7,  294.9],\n",
       "       [  32.6,   84.4,   61.6,  382. , 2125.7]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(conf_mats),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Subject Dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T14:26:05.814730Z",
     "start_time": "2024-01-19T14:25:51.860990Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 16:25:53.454468: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-19 16:25:55.225377: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-19 16:26:00.948024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten, Dense, Dropout, GlobalMaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T22:00:11.547657Z",
     "start_time": "2023-12-12T20:56:07.400269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (108742, 256)             3584      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (108742, 256)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (108742, 256)             65792     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (108742, 256)             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (108742, 128)             32896     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (108742, 128)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (108742, 128)             16512     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (108742, 128)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (108742, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.8877 - accuracy: 0.6552\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68545, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 5ms/step - loss: 0.8869 - accuracy: 0.6556 - val_loss: 0.7940 - val_accuracy: 0.6854\n",
      "Epoch 2/1000\n",
      "840/850 [============================>.] - ETA: 0s - loss: 0.7454 - accuracy: 0.7105\n",
      "Epoch 2: val_accuracy improved from 0.68545 to 0.71772, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.7449 - accuracy: 0.7107 - val_loss: 0.7087 - val_accuracy: 0.7177\n",
      "Epoch 3/1000\n",
      "846/850 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.7313\n",
      "Epoch 3: val_accuracy improved from 0.71772 to 0.72618, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.6898 - accuracy: 0.7312 - val_loss: 0.6993 - val_accuracy: 0.7262\n",
      "Epoch 4/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.6503 - accuracy: 0.7480\n",
      "Epoch 4: val_accuracy improved from 0.72618 to 0.74107, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.6498 - accuracy: 0.7482 - val_loss: 0.6591 - val_accuracy: 0.7411\n",
      "Epoch 5/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.6206 - accuracy: 0.7595\n",
      "Epoch 5: val_accuracy did not improve from 0.74107\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.6206 - accuracy: 0.7595 - val_loss: 0.6633 - val_accuracy: 0.7403\n",
      "Epoch 6/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.7691\n",
      "Epoch 6: val_accuracy improved from 0.74107 to 0.74928, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.5941 - accuracy: 0.7687 - val_loss: 0.6495 - val_accuracy: 0.7493\n",
      "Epoch 7/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.5709 - accuracy: 0.7795\n",
      "Epoch 7: val_accuracy improved from 0.74928 to 0.75394, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.5713 - accuracy: 0.7794 - val_loss: 0.6511 - val_accuracy: 0.7539\n",
      "Epoch 8/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.5544 - accuracy: 0.7859\n",
      "Epoch 8: val_accuracy did not improve from 0.75394\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.5546 - accuracy: 0.7859 - val_loss: 0.6532 - val_accuracy: 0.7502\n",
      "Epoch 9/1000\n",
      "841/850 [============================>.] - ETA: 0s - loss: 0.5365 - accuracy: 0.7930\n",
      "Epoch 9: val_accuracy did not improve from 0.75394\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.5363 - accuracy: 0.7931 - val_loss: 0.6611 - val_accuracy: 0.7527\n",
      "Epoch 10/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7983\n",
      "Epoch 10: val_accuracy improved from 0.75394 to 0.76352, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.5227 - accuracy: 0.7982 - val_loss: 0.6492 - val_accuracy: 0.7635\n",
      "Epoch 11/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.5089 - accuracy: 0.8024\n",
      "Epoch 11: val_accuracy did not improve from 0.76352\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.5089 - accuracy: 0.8024 - val_loss: 0.6596 - val_accuracy: 0.7580\n",
      "Epoch 12/1000\n",
      "838/850 [============================>.] - ETA: 0s - loss: 0.4956 - accuracy: 0.8066\n",
      "Epoch 12: val_accuracy did not improve from 0.76352\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4954 - accuracy: 0.8068 - val_loss: 0.6760 - val_accuracy: 0.7576\n",
      "Epoch 13/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.8115\n",
      "Epoch 13: val_accuracy improved from 0.76352 to 0.76529, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4853 - accuracy: 0.8115 - val_loss: 0.6495 - val_accuracy: 0.7653\n",
      "Epoch 14/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.4771 - accuracy: 0.8143\n",
      "Epoch 14: val_accuracy did not improve from 0.76529\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4771 - accuracy: 0.8144 - val_loss: 0.6767 - val_accuracy: 0.7641\n",
      "Epoch 15/1000\n",
      "841/850 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.8207\n",
      "Epoch 15: val_accuracy did not improve from 0.76529\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4656 - accuracy: 0.8209 - val_loss: 0.6726 - val_accuracy: 0.7645\n",
      "Epoch 16/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.4587 - accuracy: 0.8219\n",
      "Epoch 16: val_accuracy did not improve from 0.76529\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4586 - accuracy: 0.8220 - val_loss: 0.6702 - val_accuracy: 0.7635\n",
      "Epoch 17/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.8247\n",
      "Epoch 17: val_accuracy improved from 0.76529 to 0.76835, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4487 - accuracy: 0.8247 - val_loss: 0.6537 - val_accuracy: 0.7684\n",
      "Epoch 18/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.4417 - accuracy: 0.8293\n",
      "Epoch 18: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4417 - accuracy: 0.8293 - val_loss: 0.6752 - val_accuracy: 0.7627\n",
      "Epoch 19/1000\n",
      "850/850 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.8313\n",
      "Epoch 19: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4340 - accuracy: 0.8313 - val_loss: 0.7029 - val_accuracy: 0.7592\n",
      "Epoch 20/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.4268 - accuracy: 0.8332\n",
      "Epoch 20: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4269 - accuracy: 0.8332 - val_loss: 0.6934 - val_accuracy: 0.7606\n",
      "Epoch 21/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8348\n",
      "Epoch 21: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4249 - accuracy: 0.8347 - val_loss: 0.7110 - val_accuracy: 0.7639\n",
      "Epoch 22/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8364\n",
      "Epoch 22: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4196 - accuracy: 0.8365 - val_loss: 0.7104 - val_accuracy: 0.7662\n",
      "Epoch 23/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8396\n",
      "Epoch 23: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4136 - accuracy: 0.8396 - val_loss: 0.7009 - val_accuracy: 0.7611\n",
      "Epoch 24/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.4086 - accuracy: 0.8406\n",
      "Epoch 24: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4082 - accuracy: 0.8408 - val_loss: 0.7189 - val_accuracy: 0.7598\n",
      "Epoch 25/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.4032 - accuracy: 0.8433\n",
      "Epoch 25: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.4033 - accuracy: 0.8432 - val_loss: 0.7175 - val_accuracy: 0.7572\n",
      "Epoch 26/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8453\n",
      "Epoch 26: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3998 - accuracy: 0.8453 - val_loss: 0.7109 - val_accuracy: 0.7656\n",
      "Epoch 27/1000\n",
      "840/850 [============================>.] - ETA: 0s - loss: 0.3937 - accuracy: 0.8469\n",
      "Epoch 27: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3941 - accuracy: 0.8468 - val_loss: 0.7228 - val_accuracy: 0.7578\n",
      "Epoch 28/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.3908 - accuracy: 0.8477\n",
      "Epoch 28: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3908 - accuracy: 0.8477 - val_loss: 0.7265 - val_accuracy: 0.7602\n",
      "Epoch 29/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8500\n",
      "Epoch 29: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3888 - accuracy: 0.8499 - val_loss: 0.7244 - val_accuracy: 0.7576\n",
      "Epoch 30/1000\n",
      "841/850 [============================>.] - ETA: 0s - loss: 0.3839 - accuracy: 0.8511\n",
      "Epoch 30: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3840 - accuracy: 0.8510 - val_loss: 0.7303 - val_accuracy: 0.7597\n",
      "Epoch 31/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8534\n",
      "Epoch 31: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3768 - accuracy: 0.8536 - val_loss: 0.7356 - val_accuracy: 0.7656\n",
      "Epoch 32/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.3784 - accuracy: 0.8527\n",
      "Epoch 32: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3784 - accuracy: 0.8527 - val_loss: 0.7298 - val_accuracy: 0.7638\n",
      "Epoch 33/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.3738 - accuracy: 0.8555\n",
      "Epoch 33: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3740 - accuracy: 0.8554 - val_loss: 0.7425 - val_accuracy: 0.7635\n",
      "Epoch 34/1000\n",
      "841/850 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8559\n",
      "Epoch 34: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3695 - accuracy: 0.8560 - val_loss: 0.7657 - val_accuracy: 0.7576\n",
      "Epoch 35/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8570\n",
      "Epoch 35: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3656 - accuracy: 0.8570 - val_loss: 0.7659 - val_accuracy: 0.7604\n",
      "Epoch 36/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8587\n",
      "Epoch 36: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3653 - accuracy: 0.8587 - val_loss: 0.7490 - val_accuracy: 0.7619\n",
      "Epoch 37/1000\n",
      "838/850 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8584\n",
      "Epoch 37: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3649 - accuracy: 0.8583 - val_loss: 0.7488 - val_accuracy: 0.7645\n",
      "Epoch 38/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8606\n",
      "Epoch 38: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3589 - accuracy: 0.8605 - val_loss: 0.7477 - val_accuracy: 0.7645\n",
      "Epoch 39/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8611\n",
      "Epoch 39: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3586 - accuracy: 0.8610 - val_loss: 0.7464 - val_accuracy: 0.7638\n",
      "Epoch 40/1000\n",
      "850/850 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.8623\n",
      "Epoch 40: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3573 - accuracy: 0.8623 - val_loss: 0.7332 - val_accuracy: 0.7646\n",
      "Epoch 41/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8621\n",
      "Epoch 41: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3546 - accuracy: 0.8621 - val_loss: 0.7566 - val_accuracy: 0.7624\n",
      "Epoch 42/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8649\n",
      "Epoch 42: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3498 - accuracy: 0.8651 - val_loss: 0.7544 - val_accuracy: 0.7639\n",
      "Epoch 43/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8645\n",
      "Epoch 43: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3519 - accuracy: 0.8644 - val_loss: 0.7505 - val_accuracy: 0.7660\n",
      "Epoch 44/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.3454 - accuracy: 0.8667\n",
      "Epoch 44: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3453 - accuracy: 0.8667 - val_loss: 0.7835 - val_accuracy: 0.7580\n",
      "Epoch 45/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.3438 - accuracy: 0.8664\n",
      "Epoch 45: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3441 - accuracy: 0.8663 - val_loss: 0.7757 - val_accuracy: 0.7606\n",
      "Epoch 46/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.3414 - accuracy: 0.8676\n",
      "Epoch 46: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3414 - accuracy: 0.8676 - val_loss: 0.7726 - val_accuracy: 0.7624\n",
      "Epoch 47/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.8691\n",
      "Epoch 47: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3386 - accuracy: 0.8692 - val_loss: 0.7753 - val_accuracy: 0.7593\n",
      "Epoch 48/1000\n",
      "846/850 [============================>.] - ETA: 0s - loss: 0.3391 - accuracy: 0.8689\n",
      "Epoch 48: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3389 - accuracy: 0.8689 - val_loss: 0.7900 - val_accuracy: 0.7597\n",
      "Epoch 49/1000\n",
      "850/850 [==============================] - ETA: 0s - loss: 0.3352 - accuracy: 0.8701\n",
      "Epoch 49: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3352 - accuracy: 0.8701 - val_loss: 0.7801 - val_accuracy: 0.7632\n",
      "Epoch 50/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.8713\n",
      "Epoch 50: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3335 - accuracy: 0.8711 - val_loss: 0.7619 - val_accuracy: 0.7632\n",
      "Epoch 51/1000\n",
      "846/850 [============================>.] - ETA: 0s - loss: 0.3351 - accuracy: 0.8708\n",
      "Epoch 51: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3354 - accuracy: 0.8706 - val_loss: 0.7884 - val_accuracy: 0.7601\n",
      "Epoch 52/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8725\n",
      "Epoch 52: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3316 - accuracy: 0.8726 - val_loss: 0.7893 - val_accuracy: 0.7611\n",
      "Epoch 53/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8738\n",
      "Epoch 53: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3291 - accuracy: 0.8737 - val_loss: 0.7953 - val_accuracy: 0.7648\n",
      "Epoch 54/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8721\n",
      "Epoch 54: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3315 - accuracy: 0.8722 - val_loss: 0.7844 - val_accuracy: 0.7618\n",
      "Epoch 55/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8742\n",
      "Epoch 55: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3254 - accuracy: 0.8742 - val_loss: 0.7782 - val_accuracy: 0.7605\n",
      "Epoch 56/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.8726\n",
      "Epoch 56: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3278 - accuracy: 0.8726 - val_loss: 0.7822 - val_accuracy: 0.7617\n",
      "Epoch 57/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.8745\n",
      "Epoch 57: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3250 - accuracy: 0.8747 - val_loss: 0.8002 - val_accuracy: 0.7635\n",
      "Epoch 58/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8756\n",
      "Epoch 58: val_accuracy did not improve from 0.76835\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3238 - accuracy: 0.8756 - val_loss: 0.7855 - val_accuracy: 0.7653\n",
      "Epoch 59/1000\n",
      "840/850 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8733\n",
      "Epoch 59: val_accuracy improved from 0.76835 to 0.77004, saving model to best_weights_dense_0.hdf5\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3236 - accuracy: 0.8734 - val_loss: 0.7931 - val_accuracy: 0.7700\n",
      "Epoch 60/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8749\n",
      "Epoch 60: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3223 - accuracy: 0.8750 - val_loss: 0.7762 - val_accuracy: 0.7697\n",
      "Epoch 61/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3211 - accuracy: 0.8756\n",
      "Epoch 61: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3212 - accuracy: 0.8756 - val_loss: 0.8065 - val_accuracy: 0.7593\n",
      "Epoch 62/1000\n",
      "846/850 [============================>.] - ETA: 0s - loss: 0.3143 - accuracy: 0.8795\n",
      "Epoch 62: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3144 - accuracy: 0.8794 - val_loss: 0.7996 - val_accuracy: 0.7686\n",
      "Epoch 63/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8782\n",
      "Epoch 63: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3160 - accuracy: 0.8781 - val_loss: 0.8065 - val_accuracy: 0.7638\n",
      "Epoch 64/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8790\n",
      "Epoch 64: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3137 - accuracy: 0.8790 - val_loss: 0.8123 - val_accuracy: 0.7627\n",
      "Epoch 65/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.8776\n",
      "Epoch 65: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3168 - accuracy: 0.8775 - val_loss: 0.7899 - val_accuracy: 0.7585\n",
      "Epoch 66/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8807\n",
      "Epoch 66: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3106 - accuracy: 0.8807 - val_loss: 0.8394 - val_accuracy: 0.7580\n",
      "Epoch 67/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.8804\n",
      "Epoch 67: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3124 - accuracy: 0.8805 - val_loss: 0.8178 - val_accuracy: 0.7577\n",
      "Epoch 68/1000\n",
      "839/850 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.8807\n",
      "Epoch 68: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3121 - accuracy: 0.8807 - val_loss: 0.8274 - val_accuracy: 0.7642\n",
      "Epoch 69/1000\n",
      "840/850 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8804\n",
      "Epoch 69: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3099 - accuracy: 0.8804 - val_loss: 0.8319 - val_accuracy: 0.7605\n",
      "Epoch 70/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.8809\n",
      "Epoch 70: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3110 - accuracy: 0.8808 - val_loss: 0.8331 - val_accuracy: 0.7625\n",
      "Epoch 71/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.3080 - accuracy: 0.8826\n",
      "Epoch 71: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3079 - accuracy: 0.8826 - val_loss: 0.8182 - val_accuracy: 0.7615\n",
      "Epoch 72/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8815\n",
      "Epoch 72: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 5ms/step - loss: 0.3058 - accuracy: 0.8815 - val_loss: 0.8308 - val_accuracy: 0.7613\n",
      "Epoch 73/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8831\n",
      "Epoch 73: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3066 - accuracy: 0.8831 - val_loss: 0.8329 - val_accuracy: 0.7601\n",
      "Epoch 74/1000\n",
      "848/850 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8825\n",
      "Epoch 74: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3053 - accuracy: 0.8826 - val_loss: 0.8318 - val_accuracy: 0.7617\n",
      "Epoch 75/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8842\n",
      "Epoch 75: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3035 - accuracy: 0.8842 - val_loss: 0.8129 - val_accuracy: 0.7679\n",
      "Epoch 76/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8831\n",
      "Epoch 76: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3009 - accuracy: 0.8831 - val_loss: 0.8324 - val_accuracy: 0.7630\n",
      "Epoch 77/1000\n",
      "843/850 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8845\n",
      "Epoch 77: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.3006 - accuracy: 0.8845 - val_loss: 0.8080 - val_accuracy: 0.7633\n",
      "Epoch 78/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.8852\n",
      "Epoch 78: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2993 - accuracy: 0.8852 - val_loss: 0.8405 - val_accuracy: 0.7636\n",
      "Epoch 79/1000\n",
      "838/850 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.8851\n",
      "Epoch 79: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2992 - accuracy: 0.8851 - val_loss: 0.8275 - val_accuracy: 0.7626\n",
      "Epoch 80/1000\n",
      "846/850 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.8864\n",
      "Epoch 80: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2991 - accuracy: 0.8864 - val_loss: 0.8346 - val_accuracy: 0.7608\n",
      "Epoch 81/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8848\n",
      "Epoch 81: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2982 - accuracy: 0.8848 - val_loss: 0.8321 - val_accuracy: 0.7598\n",
      "Epoch 82/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8863\n",
      "Epoch 82: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2969 - accuracy: 0.8862 - val_loss: 0.8281 - val_accuracy: 0.7615\n",
      "Epoch 83/1000\n",
      "840/850 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8862\n",
      "Epoch 83: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2956 - accuracy: 0.8862 - val_loss: 0.8401 - val_accuracy: 0.7643\n",
      "Epoch 84/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8860\n",
      "Epoch 84: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2976 - accuracy: 0.8860 - val_loss: 0.8444 - val_accuracy: 0.7632\n",
      "Epoch 85/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8861\n",
      "Epoch 85: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2968 - accuracy: 0.8859 - val_loss: 0.8277 - val_accuracy: 0.7596\n",
      "Epoch 86/1000\n",
      "840/850 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8857\n",
      "Epoch 86: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2948 - accuracy: 0.8856 - val_loss: 0.8607 - val_accuracy: 0.7597\n",
      "Epoch 87/1000\n",
      "838/850 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8857\n",
      "Epoch 87: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2952 - accuracy: 0.8858 - val_loss: 0.8573 - val_accuracy: 0.7619\n",
      "Epoch 88/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.8889\n",
      "Epoch 88: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2906 - accuracy: 0.8887 - val_loss: 0.8527 - val_accuracy: 0.7634\n",
      "Epoch 89/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8880\n",
      "Epoch 89: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2905 - accuracy: 0.8880 - val_loss: 0.8708 - val_accuracy: 0.7604\n",
      "Epoch 90/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8887\n",
      "Epoch 90: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2905 - accuracy: 0.8888 - val_loss: 0.8392 - val_accuracy: 0.7669\n",
      "Epoch 91/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8881\n",
      "Epoch 91: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2911 - accuracy: 0.8880 - val_loss: 0.8603 - val_accuracy: 0.7641\n",
      "Epoch 92/1000\n",
      "841/850 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8895\n",
      "Epoch 92: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2896 - accuracy: 0.8895 - val_loss: 0.8576 - val_accuracy: 0.7631\n",
      "Epoch 93/1000\n",
      "840/850 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.8896\n",
      "Epoch 93: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2895 - accuracy: 0.8896 - val_loss: 0.8396 - val_accuracy: 0.7646\n",
      "Epoch 94/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8892\n",
      "Epoch 94: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2878 - accuracy: 0.8891 - val_loss: 0.8495 - val_accuracy: 0.7620\n",
      "Epoch 95/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8892\n",
      "Epoch 95: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2881 - accuracy: 0.8892 - val_loss: 0.8563 - val_accuracy: 0.7626\n",
      "Epoch 96/1000\n",
      "838/850 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.8905\n",
      "Epoch 96: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2850 - accuracy: 0.8903 - val_loss: 0.8516 - val_accuracy: 0.7619\n",
      "Epoch 97/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.8903\n",
      "Epoch 97: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2877 - accuracy: 0.8902 - val_loss: 0.8723 - val_accuracy: 0.7608\n",
      "Epoch 98/1000\n",
      "850/850 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.8906\n",
      "Epoch 98: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2862 - accuracy: 0.8906 - val_loss: 0.8376 - val_accuracy: 0.7598\n",
      "Epoch 99/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.8900\n",
      "Epoch 99: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2875 - accuracy: 0.8900 - val_loss: 0.8493 - val_accuracy: 0.7652\n",
      "Epoch 100/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.8921\n",
      "Epoch 100: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2822 - accuracy: 0.8921 - val_loss: 0.8466 - val_accuracy: 0.7638\n",
      "Epoch 101/1000\n",
      "849/850 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.8928\n",
      "Epoch 101: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2833 - accuracy: 0.8928 - val_loss: 0.8453 - val_accuracy: 0.7622\n",
      "Epoch 102/1000\n",
      "845/850 [============================>.] - ETA: 0s - loss: 0.2834 - accuracy: 0.8915\n",
      "Epoch 102: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2835 - accuracy: 0.8915 - val_loss: 0.8608 - val_accuracy: 0.7632\n",
      "Epoch 103/1000\n",
      "842/850 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8936\n",
      "Epoch 103: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2787 - accuracy: 0.8937 - val_loss: 0.8689 - val_accuracy: 0.7649\n",
      "Epoch 104/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.2817 - accuracy: 0.8915\n",
      "Epoch 104: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2816 - accuracy: 0.8915 - val_loss: 0.8487 - val_accuracy: 0.7603\n",
      "Epoch 105/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.2817 - accuracy: 0.8921\n",
      "Epoch 105: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2816 - accuracy: 0.8921 - val_loss: 0.8430 - val_accuracy: 0.7677\n",
      "Epoch 106/1000\n",
      "844/850 [============================>.] - ETA: 0s - loss: 0.2799 - accuracy: 0.8934\n",
      "Epoch 106: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2799 - accuracy: 0.8934 - val_loss: 0.8373 - val_accuracy: 0.7667\n",
      "Epoch 107/1000\n",
      "846/850 [============================>.] - ETA: 0s - loss: 0.2800 - accuracy: 0.8931\n",
      "Epoch 107: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2800 - accuracy: 0.8931 - val_loss: 0.8426 - val_accuracy: 0.7637\n",
      "Epoch 108/1000\n",
      "847/850 [============================>.] - ETA: 0s - loss: 0.2750 - accuracy: 0.8946\n",
      "Epoch 108: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2751 - accuracy: 0.8945 - val_loss: 0.8374 - val_accuracy: 0.7646\n",
      "Epoch 109/1000\n",
      "838/850 [============================>.] - ETA: 0s - loss: 0.2769 - accuracy: 0.8937\n",
      "Epoch 109: val_accuracy did not improve from 0.77004\n",
      "850/850 [==============================] - 4s 4ms/step - loss: 0.2770 - accuracy: 0.8936 - val_loss: 0.8808 - val_accuracy: 0.7583\n",
      "389/389 [==============================] - 0s 785us/step\n",
      "0.7577156302060611\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (108816, 256)             3584      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (108816, 256)             0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (108816, 256)             65792     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (108816, 256)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (108816, 128)             32896     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (108816, 128)             0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (108816, 128)             16512     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (108816, 128)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (108816, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.8885 - accuracy: 0.6533\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69765, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.8883 - accuracy: 0.6533 - val_loss: 0.7768 - val_accuracy: 0.6977\n",
      "Epoch 2/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.7433 - accuracy: 0.7110\n",
      "Epoch 2: val_accuracy improved from 0.69765 to 0.71951, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.7432 - accuracy: 0.7111 - val_loss: 0.7300 - val_accuracy: 0.7195\n",
      "Epoch 3/1000\n",
      "850/851 [============================>.] - ETA: 0s - loss: 0.6877 - accuracy: 0.7315\n",
      "Epoch 3: val_accuracy improved from 0.71951 to 0.73895, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.6877 - accuracy: 0.7315 - val_loss: 0.6904 - val_accuracy: 0.7389\n",
      "Epoch 4/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.6482 - accuracy: 0.7480\n",
      "Epoch 4: val_accuracy improved from 0.73895 to 0.74915, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.6481 - accuracy: 0.7480 - val_loss: 0.6681 - val_accuracy: 0.7491\n",
      "Epoch 5/1000\n",
      "851/851 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.7594\n",
      "Epoch 5: val_accuracy did not improve from 0.74915\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.6198 - accuracy: 0.7594 - val_loss: 0.6719 - val_accuracy: 0.7490\n",
      "Epoch 6/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.5958 - accuracy: 0.7687\n",
      "Epoch 6: val_accuracy improved from 0.74915 to 0.75879, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.5959 - accuracy: 0.7687 - val_loss: 0.6521 - val_accuracy: 0.7588\n",
      "Epoch 7/1000\n",
      "839/851 [============================>.] - ETA: 0s - loss: 0.5729 - accuracy: 0.7790\n",
      "Epoch 7: val_accuracy did not improve from 0.75879\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.5728 - accuracy: 0.7789 - val_loss: 0.6617 - val_accuracy: 0.7560\n",
      "Epoch 8/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.5550 - accuracy: 0.7867\n",
      "Epoch 8: val_accuracy improved from 0.75879 to 0.76283, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.5551 - accuracy: 0.7866 - val_loss: 0.6428 - val_accuracy: 0.7628\n",
      "Epoch 9/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7907\n",
      "Epoch 9: val_accuracy did not improve from 0.76283\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.5415 - accuracy: 0.7908 - val_loss: 0.6477 - val_accuracy: 0.7566\n",
      "Epoch 10/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.5267 - accuracy: 0.7965\n",
      "Epoch 10: val_accuracy did not improve from 0.76283\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.5266 - accuracy: 0.7967 - val_loss: 0.6567 - val_accuracy: 0.7625\n",
      "Epoch 11/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.5116 - accuracy: 0.8018\n",
      "Epoch 11: val_accuracy did not improve from 0.76283\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.5120 - accuracy: 0.8017 - val_loss: 0.6559 - val_accuracy: 0.7624\n",
      "Epoch 12/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.8074\n",
      "Epoch 12: val_accuracy improved from 0.76283 to 0.76397, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.5004 - accuracy: 0.8075 - val_loss: 0.6546 - val_accuracy: 0.7640\n",
      "Epoch 13/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.4896 - accuracy: 0.8101\n",
      "Epoch 13: val_accuracy did not improve from 0.76397\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.4898 - accuracy: 0.8101 - val_loss: 0.6812 - val_accuracy: 0.7606\n",
      "Epoch 14/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.4782 - accuracy: 0.8149\n",
      "Epoch 14: val_accuracy improved from 0.76397 to 0.77036, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4782 - accuracy: 0.8149 - val_loss: 0.6508 - val_accuracy: 0.7704\n",
      "Epoch 15/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.4693 - accuracy: 0.8176\n",
      "Epoch 15: val_accuracy did not improve from 0.77036\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4695 - accuracy: 0.8176 - val_loss: 0.6546 - val_accuracy: 0.7678\n",
      "Epoch 16/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.4588 - accuracy: 0.8226\n",
      "Epoch 16: val_accuracy did not improve from 0.77036\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.4591 - accuracy: 0.8225 - val_loss: 0.6626 - val_accuracy: 0.7700\n",
      "Epoch 17/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.4549 - accuracy: 0.8229\n",
      "Epoch 17: val_accuracy did not improve from 0.77036\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4545 - accuracy: 0.8231 - val_loss: 0.6819 - val_accuracy: 0.7678\n",
      "Epoch 18/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.8264\n",
      "Epoch 18: val_accuracy improved from 0.77036 to 0.77045, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4464 - accuracy: 0.8263 - val_loss: 0.6606 - val_accuracy: 0.7704\n",
      "Epoch 19/1000\n",
      "836/851 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8291\n",
      "Epoch 19: val_accuracy did not improve from 0.77045\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4401 - accuracy: 0.8295 - val_loss: 0.7146 - val_accuracy: 0.7641\n",
      "Epoch 20/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.4336 - accuracy: 0.8332\n",
      "Epoch 20: val_accuracy did not improve from 0.77045\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4338 - accuracy: 0.8331 - val_loss: 0.6911 - val_accuracy: 0.7678\n",
      "Epoch 21/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.4268 - accuracy: 0.8352\n",
      "Epoch 21: val_accuracy did not improve from 0.77045\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4267 - accuracy: 0.8352 - val_loss: 0.6835 - val_accuracy: 0.7703\n",
      "Epoch 22/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.4185 - accuracy: 0.8370\n",
      "Epoch 22: val_accuracy did not improve from 0.77045\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.4186 - accuracy: 0.8369 - val_loss: 0.6924 - val_accuracy: 0.7694\n",
      "Epoch 23/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.4156 - accuracy: 0.8384\n",
      "Epoch 23: val_accuracy did not improve from 0.77045\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.4159 - accuracy: 0.8382 - val_loss: 0.6824 - val_accuracy: 0.7662\n",
      "Epoch 24/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.4082 - accuracy: 0.8416\n",
      "Epoch 24: val_accuracy improved from 0.77045 to 0.77150, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.4087 - accuracy: 0.8414 - val_loss: 0.6766 - val_accuracy: 0.7715\n",
      "Epoch 25/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8430\n",
      "Epoch 25: val_accuracy did not improve from 0.77150\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4051 - accuracy: 0.8429 - val_loss: 0.6837 - val_accuracy: 0.7714\n",
      "Epoch 26/1000\n",
      "849/851 [============================>.] - ETA: 0s - loss: 0.4035 - accuracy: 0.8436\n",
      "Epoch 26: val_accuracy did not improve from 0.77150\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.4033 - accuracy: 0.8437 - val_loss: 0.7027 - val_accuracy: 0.7704\n",
      "Epoch 27/1000\n",
      "849/851 [============================>.] - ETA: 0s - loss: 0.3996 - accuracy: 0.8436\n",
      "Epoch 27: val_accuracy improved from 0.77150 to 0.77385, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3995 - accuracy: 0.8436 - val_loss: 0.7003 - val_accuracy: 0.7738\n",
      "Epoch 28/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.3939 - accuracy: 0.8474\n",
      "Epoch 28: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3938 - accuracy: 0.8474 - val_loss: 0.7077 - val_accuracy: 0.7711\n",
      "Epoch 29/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8478\n",
      "Epoch 29: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3926 - accuracy: 0.8479 - val_loss: 0.7068 - val_accuracy: 0.7694\n",
      "Epoch 30/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.8508\n",
      "Epoch 30: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3860 - accuracy: 0.8509 - val_loss: 0.7216 - val_accuracy: 0.7699\n",
      "Epoch 31/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.3863 - accuracy: 0.8501\n",
      "Epoch 31: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3859 - accuracy: 0.8504 - val_loss: 0.7136 - val_accuracy: 0.7679\n",
      "Epoch 32/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8535\n",
      "Epoch 32: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3797 - accuracy: 0.8532 - val_loss: 0.7166 - val_accuracy: 0.7683\n",
      "Epoch 33/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.8539\n",
      "Epoch 33: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3769 - accuracy: 0.8541 - val_loss: 0.7231 - val_accuracy: 0.7726\n",
      "Epoch 34/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.3718 - accuracy: 0.8548\n",
      "Epoch 34: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3720 - accuracy: 0.8547 - val_loss: 0.7245 - val_accuracy: 0.7712\n",
      "Epoch 35/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.8561\n",
      "Epoch 35: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3729 - accuracy: 0.8560 - val_loss: 0.7085 - val_accuracy: 0.7687\n",
      "Epoch 36/1000\n",
      "846/851 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8571\n",
      "Epoch 36: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3692 - accuracy: 0.8571 - val_loss: 0.7344 - val_accuracy: 0.7642\n",
      "Epoch 37/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.3633 - accuracy: 0.8598\n",
      "Epoch 37: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3630 - accuracy: 0.8599 - val_loss: 0.7267 - val_accuracy: 0.7684\n",
      "Epoch 38/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8587\n",
      "Epoch 38: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3657 - accuracy: 0.8586 - val_loss: 0.7346 - val_accuracy: 0.7730\n",
      "Epoch 39/1000\n",
      "846/851 [============================>.] - ETA: 0s - loss: 0.3621 - accuracy: 0.8602\n",
      "Epoch 39: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3619 - accuracy: 0.8602 - val_loss: 0.7455 - val_accuracy: 0.7677\n",
      "Epoch 40/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8612\n",
      "Epoch 40: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3586 - accuracy: 0.8612 - val_loss: 0.7385 - val_accuracy: 0.7717\n",
      "Epoch 41/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8624\n",
      "Epoch 41: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3553 - accuracy: 0.8622 - val_loss: 0.7261 - val_accuracy: 0.7716\n",
      "Epoch 42/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.3531 - accuracy: 0.8638\n",
      "Epoch 42: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3529 - accuracy: 0.8639 - val_loss: 0.7585 - val_accuracy: 0.7657\n",
      "Epoch 43/1000\n",
      "851/851 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.8651\n",
      "Epoch 43: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3503 - accuracy: 0.8651 - val_loss: 0.7641 - val_accuracy: 0.7687\n",
      "Epoch 44/1000\n",
      "849/851 [============================>.] - ETA: 0s - loss: 0.3501 - accuracy: 0.8642\n",
      "Epoch 44: val_accuracy did not improve from 0.77385\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3501 - accuracy: 0.8642 - val_loss: 0.7467 - val_accuracy: 0.7670\n",
      "Epoch 45/1000\n",
      "849/851 [============================>.] - ETA: 0s - loss: 0.3479 - accuracy: 0.8645\n",
      "Epoch 45: val_accuracy improved from 0.77385 to 0.77538, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3479 - accuracy: 0.8645 - val_loss: 0.7414 - val_accuracy: 0.7754\n",
      "Epoch 46/1000\n",
      "850/851 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8661\n",
      "Epoch 46: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3433 - accuracy: 0.8661 - val_loss: 0.7375 - val_accuracy: 0.7744\n",
      "Epoch 47/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.3406 - accuracy: 0.8689\n",
      "Epoch 47: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3404 - accuracy: 0.8689 - val_loss: 0.7673 - val_accuracy: 0.7625\n",
      "Epoch 48/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8683\n",
      "Epoch 48: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3418 - accuracy: 0.8683 - val_loss: 0.7555 - val_accuracy: 0.7729\n",
      "Epoch 49/1000\n",
      "841/851 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8686\n",
      "Epoch 49: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3396 - accuracy: 0.8687 - val_loss: 0.7472 - val_accuracy: 0.7707\n",
      "Epoch 50/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.3400 - accuracy: 0.8693\n",
      "Epoch 50: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3399 - accuracy: 0.8694 - val_loss: 0.7401 - val_accuracy: 0.7716\n",
      "Epoch 51/1000\n",
      "841/851 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8701\n",
      "Epoch 51: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3373 - accuracy: 0.8700 - val_loss: 0.7480 - val_accuracy: 0.7719\n",
      "Epoch 52/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.8700\n",
      "Epoch 52: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3362 - accuracy: 0.8701 - val_loss: 0.7354 - val_accuracy: 0.7730\n",
      "Epoch 53/1000\n",
      "846/851 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.8724\n",
      "Epoch 53: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3337 - accuracy: 0.8724 - val_loss: 0.7735 - val_accuracy: 0.7689\n",
      "Epoch 54/1000\n",
      "849/851 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8740\n",
      "Epoch 54: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3304 - accuracy: 0.8740 - val_loss: 0.7472 - val_accuracy: 0.7737\n",
      "Epoch 55/1000\n",
      "841/851 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.8735\n",
      "Epoch 55: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3278 - accuracy: 0.8735 - val_loss: 0.7640 - val_accuracy: 0.7738\n",
      "Epoch 56/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8739\n",
      "Epoch 56: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3284 - accuracy: 0.8738 - val_loss: 0.7550 - val_accuracy: 0.7703\n",
      "Epoch 57/1000\n",
      "851/851 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.8750\n",
      "Epoch 57: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3251 - accuracy: 0.8750 - val_loss: 0.7556 - val_accuracy: 0.7746\n",
      "Epoch 58/1000\n",
      "841/851 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8741\n",
      "Epoch 58: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3261 - accuracy: 0.8741 - val_loss: 0.7600 - val_accuracy: 0.7670\n",
      "Epoch 59/1000\n",
      "839/851 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.8744\n",
      "Epoch 59: val_accuracy did not improve from 0.77538\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3253 - accuracy: 0.8744 - val_loss: 0.7721 - val_accuracy: 0.7701\n",
      "Epoch 60/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8773\n",
      "Epoch 60: val_accuracy improved from 0.77538 to 0.77676, saving model to best_weights_dense_1.hdf5\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3204 - accuracy: 0.8773 - val_loss: 0.7629 - val_accuracy: 0.7768\n",
      "Epoch 61/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8763\n",
      "Epoch 61: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3197 - accuracy: 0.8762 - val_loss: 0.7691 - val_accuracy: 0.7665\n",
      "Epoch 62/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8750\n",
      "Epoch 62: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3222 - accuracy: 0.8750 - val_loss: 0.7900 - val_accuracy: 0.7667\n",
      "Epoch 63/1000\n",
      "839/851 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8774\n",
      "Epoch 63: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3205 - accuracy: 0.8776 - val_loss: 0.7745 - val_accuracy: 0.7683\n",
      "Epoch 64/1000\n",
      "841/851 [============================>.] - ETA: 0s - loss: 0.3183 - accuracy: 0.8772\n",
      "Epoch 64: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3185 - accuracy: 0.8770 - val_loss: 0.7765 - val_accuracy: 0.7692\n",
      "Epoch 65/1000\n",
      "850/851 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8775\n",
      "Epoch 65: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3165 - accuracy: 0.8775 - val_loss: 0.7825 - val_accuracy: 0.7694\n",
      "Epoch 66/1000\n",
      "850/851 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8789\n",
      "Epoch 66: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3151 - accuracy: 0.8789 - val_loss: 0.7958 - val_accuracy: 0.7652\n",
      "Epoch 67/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.8781\n",
      "Epoch 67: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3155 - accuracy: 0.8782 - val_loss: 0.7626 - val_accuracy: 0.7767\n",
      "Epoch 68/1000\n",
      "841/851 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8795\n",
      "Epoch 68: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3121 - accuracy: 0.8796 - val_loss: 0.8095 - val_accuracy: 0.7720\n",
      "Epoch 69/1000\n",
      "839/851 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.8785\n",
      "Epoch 69: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3144 - accuracy: 0.8787 - val_loss: 0.8064 - val_accuracy: 0.7619\n",
      "Epoch 70/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8803\n",
      "Epoch 70: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3109 - accuracy: 0.8803 - val_loss: 0.7948 - val_accuracy: 0.7695\n",
      "Epoch 71/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8803\n",
      "Epoch 71: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3109 - accuracy: 0.8802 - val_loss: 0.7834 - val_accuracy: 0.7700\n",
      "Epoch 72/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8811\n",
      "Epoch 72: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3083 - accuracy: 0.8813 - val_loss: 0.7923 - val_accuracy: 0.7685\n",
      "Epoch 73/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8810\n",
      "Epoch 73: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3067 - accuracy: 0.8811 - val_loss: 0.7873 - val_accuracy: 0.7691\n",
      "Epoch 74/1000\n",
      "841/851 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8818\n",
      "Epoch 74: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3061 - accuracy: 0.8818 - val_loss: 0.7803 - val_accuracy: 0.7708\n",
      "Epoch 75/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.3068 - accuracy: 0.8812\n",
      "Epoch 75: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3067 - accuracy: 0.8813 - val_loss: 0.8183 - val_accuracy: 0.7677\n",
      "Epoch 76/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8833\n",
      "Epoch 76: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3053 - accuracy: 0.8831 - val_loss: 0.7695 - val_accuracy: 0.7696\n",
      "Epoch 77/1000\n",
      "846/851 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8843\n",
      "Epoch 77: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3039 - accuracy: 0.8843 - val_loss: 0.7750 - val_accuracy: 0.7702\n",
      "Epoch 78/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8833\n",
      "Epoch 78: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3011 - accuracy: 0.8833 - val_loss: 0.7793 - val_accuracy: 0.7701\n",
      "Epoch 79/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8829\n",
      "Epoch 79: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3043 - accuracy: 0.8829 - val_loss: 0.7930 - val_accuracy: 0.7693\n",
      "Epoch 80/1000\n",
      "850/851 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8856\n",
      "Epoch 80: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2999 - accuracy: 0.8856 - val_loss: 0.7798 - val_accuracy: 0.7694\n",
      "Epoch 81/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8844\n",
      "Epoch 81: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.3013 - accuracy: 0.8845 - val_loss: 0.7983 - val_accuracy: 0.7686\n",
      "Epoch 82/1000\n",
      "849/851 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8855\n",
      "Epoch 82: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2980 - accuracy: 0.8855 - val_loss: 0.7799 - val_accuracy: 0.7663\n",
      "Epoch 83/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8848\n",
      "Epoch 83: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.3014 - accuracy: 0.8847 - val_loss: 0.8185 - val_accuracy: 0.7627\n",
      "Epoch 84/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8863\n",
      "Epoch 84: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2971 - accuracy: 0.8862 - val_loss: 0.7901 - val_accuracy: 0.7681\n",
      "Epoch 85/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8863\n",
      "Epoch 85: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2957 - accuracy: 0.8863 - val_loss: 0.7951 - val_accuracy: 0.7687\n",
      "Epoch 86/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8855\n",
      "Epoch 86: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2982 - accuracy: 0.8852 - val_loss: 0.7888 - val_accuracy: 0.7683\n",
      "Epoch 87/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8867\n",
      "Epoch 87: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2944 - accuracy: 0.8867 - val_loss: 0.8137 - val_accuracy: 0.7673\n",
      "Epoch 88/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8871\n",
      "Epoch 88: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2953 - accuracy: 0.8870 - val_loss: 0.8163 - val_accuracy: 0.7698\n",
      "Epoch 89/1000\n",
      "839/851 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8879\n",
      "Epoch 89: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2934 - accuracy: 0.8880 - val_loss: 0.8024 - val_accuracy: 0.7649\n",
      "Epoch 90/1000\n",
      "846/851 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8886\n",
      "Epoch 90: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2943 - accuracy: 0.8886 - val_loss: 0.8023 - val_accuracy: 0.7732\n",
      "Epoch 91/1000\n",
      "846/851 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8869\n",
      "Epoch 91: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2946 - accuracy: 0.8869 - val_loss: 0.7872 - val_accuracy: 0.7676\n",
      "Epoch 92/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.8905\n",
      "Epoch 92: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2876 - accuracy: 0.8903 - val_loss: 0.8112 - val_accuracy: 0.7679\n",
      "Epoch 93/1000\n",
      "850/851 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.8882\n",
      "Epoch 93: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2900 - accuracy: 0.8882 - val_loss: 0.8090 - val_accuracy: 0.7675\n",
      "Epoch 94/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.8880\n",
      "Epoch 94: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2911 - accuracy: 0.8882 - val_loss: 0.8212 - val_accuracy: 0.7639\n",
      "Epoch 95/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.2874 - accuracy: 0.8904\n",
      "Epoch 95: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2874 - accuracy: 0.8903 - val_loss: 0.8085 - val_accuracy: 0.7673\n",
      "Epoch 96/1000\n",
      "840/851 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8895\n",
      "Epoch 96: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2886 - accuracy: 0.8894 - val_loss: 0.8183 - val_accuracy: 0.7693\n",
      "Epoch 97/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8897\n",
      "Epoch 97: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2899 - accuracy: 0.8895 - val_loss: 0.8511 - val_accuracy: 0.7616\n",
      "Epoch 98/1000\n",
      "839/851 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.8891\n",
      "Epoch 98: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2870 - accuracy: 0.8891 - val_loss: 0.8394 - val_accuracy: 0.7651\n",
      "Epoch 99/1000\n",
      "842/851 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8908\n",
      "Epoch 99: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2858 - accuracy: 0.8908 - val_loss: 0.8287 - val_accuracy: 0.7631\n",
      "Epoch 100/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.8896\n",
      "Epoch 100: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2878 - accuracy: 0.8896 - val_loss: 0.8114 - val_accuracy: 0.7665\n",
      "Epoch 101/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.8914\n",
      "Epoch 101: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2842 - accuracy: 0.8914 - val_loss: 0.8344 - val_accuracy: 0.7654\n",
      "Epoch 102/1000\n",
      "844/851 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8897\n",
      "Epoch 102: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2856 - accuracy: 0.8897 - val_loss: 0.8342 - val_accuracy: 0.7670\n",
      "Epoch 103/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.8899\n",
      "Epoch 103: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2866 - accuracy: 0.8899 - val_loss: 0.8269 - val_accuracy: 0.7675\n",
      "Epoch 104/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.2804 - accuracy: 0.8924\n",
      "Epoch 104: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2803 - accuracy: 0.8926 - val_loss: 0.8332 - val_accuracy: 0.7657\n",
      "Epoch 105/1000\n",
      "849/851 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8920\n",
      "Epoch 105: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 4ms/step - loss: 0.2811 - accuracy: 0.8920 - val_loss: 0.8402 - val_accuracy: 0.7662\n",
      "Epoch 106/1000\n",
      "845/851 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.8918\n",
      "Epoch 106: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2839 - accuracy: 0.8919 - val_loss: 0.8166 - val_accuracy: 0.7666\n",
      "Epoch 107/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.8927\n",
      "Epoch 107: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2826 - accuracy: 0.8928 - val_loss: 0.8478 - val_accuracy: 0.7674\n",
      "Epoch 108/1000\n",
      "843/851 [============================>.] - ETA: 0s - loss: 0.2800 - accuracy: 0.8930\n",
      "Epoch 108: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2802 - accuracy: 0.8930 - val_loss: 0.8303 - val_accuracy: 0.7678\n",
      "Epoch 109/1000\n",
      "847/851 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8912\n",
      "Epoch 109: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2853 - accuracy: 0.8912 - val_loss: 0.8261 - val_accuracy: 0.7691\n",
      "Epoch 110/1000\n",
      "848/851 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.8936\n",
      "Epoch 110: val_accuracy did not improve from 0.77676\n",
      "851/851 [==============================] - 4s 5ms/step - loss: 0.2790 - accuracy: 0.8937 - val_loss: 0.8348 - val_accuracy: 0.7680\n",
      "386/386 [==============================] - 0s 651us/step\n",
      "0.7607535309306082\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (108985, 256)             3584      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (108985, 256)             0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (108985, 256)             65792     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (108985, 256)             0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (108985, 128)             32896     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (108985, 128)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (108985, 128)             16512     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (108985, 128)             0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (108985, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.8823 - accuracy: 0.6562\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69789, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.8816 - accuracy: 0.6566 - val_loss: 0.7763 - val_accuracy: 0.6979\n",
      "Epoch 2/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.7438 - accuracy: 0.7113\n",
      "Epoch 2: val_accuracy improved from 0.69789 to 0.71587, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.7431 - accuracy: 0.7117 - val_loss: 0.7127 - val_accuracy: 0.7159\n",
      "Epoch 3/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.6873 - accuracy: 0.7341\n",
      "Epoch 3: val_accuracy did not improve from 0.71587\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.6867 - accuracy: 0.7342 - val_loss: 0.7034 - val_accuracy: 0.7120\n",
      "Epoch 4/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.7497\n",
      "Epoch 4: val_accuracy improved from 0.71587 to 0.72991, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.6492 - accuracy: 0.7497 - val_loss: 0.6884 - val_accuracy: 0.7299\n",
      "Epoch 5/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.6180 - accuracy: 0.7620\n",
      "Epoch 5: val_accuracy improved from 0.72991 to 0.73073, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.6179 - accuracy: 0.7620 - val_loss: 0.6778 - val_accuracy: 0.7307\n",
      "Epoch 6/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.7704\n",
      "Epoch 6: val_accuracy improved from 0.73073 to 0.73418, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5927 - accuracy: 0.7704 - val_loss: 0.6802 - val_accuracy: 0.7342\n",
      "Epoch 7/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7801\n",
      "Epoch 7: val_accuracy improved from 0.73418 to 0.74542, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5717 - accuracy: 0.7802 - val_loss: 0.6596 - val_accuracy: 0.7454\n",
      "Epoch 8/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7875\n",
      "Epoch 8: val_accuracy improved from 0.74542 to 0.74641, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5497 - accuracy: 0.7875 - val_loss: 0.6692 - val_accuracy: 0.7464\n",
      "Epoch 9/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7928\n",
      "Epoch 9: val_accuracy improved from 0.74641 to 0.74936, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5348 - accuracy: 0.7928 - val_loss: 0.6608 - val_accuracy: 0.7494\n",
      "Epoch 10/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.5227 - accuracy: 0.7979\n",
      "Epoch 10: val_accuracy did not improve from 0.74936\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.5229 - accuracy: 0.7978 - val_loss: 0.6827 - val_accuracy: 0.7440\n",
      "Epoch 11/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.5085 - accuracy: 0.8029\n",
      "Epoch 11: val_accuracy did not improve from 0.74936\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5085 - accuracy: 0.8029 - val_loss: 0.6794 - val_accuracy: 0.7469\n",
      "Epoch 12/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4950 - accuracy: 0.8081\n",
      "Epoch 12: val_accuracy did not improve from 0.74936\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4949 - accuracy: 0.8081 - val_loss: 0.6764 - val_accuracy: 0.7466\n",
      "Epoch 13/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.4834 - accuracy: 0.8126\n",
      "Epoch 13: val_accuracy did not improve from 0.74936\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4833 - accuracy: 0.8127 - val_loss: 0.6727 - val_accuracy: 0.7488\n",
      "Epoch 14/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.4763 - accuracy: 0.8155\n",
      "Epoch 14: val_accuracy did not improve from 0.74936\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4766 - accuracy: 0.8156 - val_loss: 0.6748 - val_accuracy: 0.7465\n",
      "Epoch 15/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.4665 - accuracy: 0.8188\n",
      "Epoch 15: val_accuracy improved from 0.74936 to 0.75101, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4665 - accuracy: 0.8187 - val_loss: 0.6662 - val_accuracy: 0.7510\n",
      "Epoch 16/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.4572 - accuracy: 0.8228\n",
      "Epoch 16: val_accuracy did not improve from 0.75101\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4572 - accuracy: 0.8228 - val_loss: 0.6816 - val_accuracy: 0.7509\n",
      "Epoch 17/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.4466 - accuracy: 0.8274\n",
      "Epoch 17: val_accuracy improved from 0.75101 to 0.75486, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4470 - accuracy: 0.8273 - val_loss: 0.6705 - val_accuracy: 0.7549\n",
      "Epoch 18/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.4437 - accuracy: 0.8284\n",
      "Epoch 18: val_accuracy did not improve from 0.75486\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4437 - accuracy: 0.8283 - val_loss: 0.7010 - val_accuracy: 0.7498\n",
      "Epoch 19/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4354 - accuracy: 0.8306\n",
      "Epoch 19: val_accuracy did not improve from 0.75486\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4354 - accuracy: 0.8306 - val_loss: 0.6899 - val_accuracy: 0.7497\n",
      "Epoch 20/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8341\n",
      "Epoch 20: val_accuracy did not improve from 0.75486\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4274 - accuracy: 0.8340 - val_loss: 0.7246 - val_accuracy: 0.7502\n",
      "Epoch 21/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.4244 - accuracy: 0.8338\n",
      "Epoch 21: val_accuracy did not improve from 0.75486\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4242 - accuracy: 0.8340 - val_loss: 0.7122 - val_accuracy: 0.7441\n",
      "Epoch 22/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.4170 - accuracy: 0.8387\n",
      "Epoch 22: val_accuracy did not improve from 0.75486\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4172 - accuracy: 0.8386 - val_loss: 0.6979 - val_accuracy: 0.7501\n",
      "Epoch 23/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.4132 - accuracy: 0.8401\n",
      "Epoch 23: val_accuracy did not improve from 0.75486\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4133 - accuracy: 0.8401 - val_loss: 0.7093 - val_accuracy: 0.7498\n",
      "Epoch 24/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.4062 - accuracy: 0.8422\n",
      "Epoch 24: val_accuracy did not improve from 0.75486\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4063 - accuracy: 0.8422 - val_loss: 0.7202 - val_accuracy: 0.7531\n",
      "Epoch 25/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.8444\n",
      "Epoch 25: val_accuracy improved from 0.75486 to 0.75560, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4027 - accuracy: 0.8444 - val_loss: 0.7079 - val_accuracy: 0.7556\n",
      "Epoch 26/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8455\n",
      "Epoch 26: val_accuracy improved from 0.75560 to 0.75839, saving model to best_weights_dense_2.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3986 - accuracy: 0.8455 - val_loss: 0.7051 - val_accuracy: 0.7584\n",
      "Epoch 27/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8473\n",
      "Epoch 27: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3941 - accuracy: 0.8473 - val_loss: 0.7147 - val_accuracy: 0.7577\n",
      "Epoch 28/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3902 - accuracy: 0.8492\n",
      "Epoch 28: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3904 - accuracy: 0.8491 - val_loss: 0.7452 - val_accuracy: 0.7499\n",
      "Epoch 29/1000\n",
      "840/852 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.8513\n",
      "Epoch 29: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3877 - accuracy: 0.8513 - val_loss: 0.7195 - val_accuracy: 0.7502\n",
      "Epoch 30/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3807 - accuracy: 0.8512\n",
      "Epoch 30: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3807 - accuracy: 0.8512 - val_loss: 0.7349 - val_accuracy: 0.7497\n",
      "Epoch 31/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3806 - accuracy: 0.8533\n",
      "Epoch 31: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3807 - accuracy: 0.8533 - val_loss: 0.7485 - val_accuracy: 0.7490\n",
      "Epoch 32/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8549\n",
      "Epoch 32: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3751 - accuracy: 0.8548 - val_loss: 0.7411 - val_accuracy: 0.7522\n",
      "Epoch 33/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8558\n",
      "Epoch 33: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3744 - accuracy: 0.8559 - val_loss: 0.7389 - val_accuracy: 0.7536\n",
      "Epoch 34/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8575\n",
      "Epoch 34: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3699 - accuracy: 0.8575 - val_loss: 0.7503 - val_accuracy: 0.7499\n",
      "Epoch 35/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3665 - accuracy: 0.8572\n",
      "Epoch 35: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3664 - accuracy: 0.8573 - val_loss: 0.7502 - val_accuracy: 0.7536\n",
      "Epoch 36/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3636 - accuracy: 0.8594\n",
      "Epoch 36: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3638 - accuracy: 0.8592 - val_loss: 0.7750 - val_accuracy: 0.7484\n",
      "Epoch 37/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.3599 - accuracy: 0.8613\n",
      "Epoch 37: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3602 - accuracy: 0.8613 - val_loss: 0.7740 - val_accuracy: 0.7482\n",
      "Epoch 38/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8619\n",
      "Epoch 38: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3592 - accuracy: 0.8620 - val_loss: 0.7572 - val_accuracy: 0.7466\n",
      "Epoch 39/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3559 - accuracy: 0.8631\n",
      "Epoch 39: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3559 - accuracy: 0.8631 - val_loss: 0.7743 - val_accuracy: 0.7515\n",
      "Epoch 40/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3554 - accuracy: 0.8619\n",
      "Epoch 40: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3556 - accuracy: 0.8618 - val_loss: 0.7704 - val_accuracy: 0.7505\n",
      "Epoch 41/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3518 - accuracy: 0.8648\n",
      "Epoch 41: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3521 - accuracy: 0.8647 - val_loss: 0.7652 - val_accuracy: 0.7540\n",
      "Epoch 42/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.8660\n",
      "Epoch 42: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3493 - accuracy: 0.8658 - val_loss: 0.7681 - val_accuracy: 0.7549\n",
      "Epoch 43/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3481 - accuracy: 0.8659\n",
      "Epoch 43: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3482 - accuracy: 0.8657 - val_loss: 0.7749 - val_accuracy: 0.7559\n",
      "Epoch 44/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3447 - accuracy: 0.8673\n",
      "Epoch 44: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3452 - accuracy: 0.8671 - val_loss: 0.7671 - val_accuracy: 0.7551\n",
      "Epoch 45/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8688\n",
      "Epoch 45: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3423 - accuracy: 0.8687 - val_loss: 0.7916 - val_accuracy: 0.7465\n",
      "Epoch 46/1000\n",
      "840/852 [============================>.] - ETA: 0s - loss: 0.3440 - accuracy: 0.8666\n",
      "Epoch 46: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3436 - accuracy: 0.8668 - val_loss: 0.7744 - val_accuracy: 0.7570\n",
      "Epoch 47/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8678\n",
      "Epoch 47: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3408 - accuracy: 0.8679 - val_loss: 0.7739 - val_accuracy: 0.7521\n",
      "Epoch 48/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.3395 - accuracy: 0.8693\n",
      "Epoch 48: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3393 - accuracy: 0.8694 - val_loss: 0.7778 - val_accuracy: 0.7500\n",
      "Epoch 49/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8701\n",
      "Epoch 49: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3371 - accuracy: 0.8702 - val_loss: 0.7920 - val_accuracy: 0.7491\n",
      "Epoch 50/1000\n",
      "840/852 [============================>.] - ETA: 0s - loss: 0.3334 - accuracy: 0.8728\n",
      "Epoch 50: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3338 - accuracy: 0.8727 - val_loss: 0.7966 - val_accuracy: 0.7463\n",
      "Epoch 51/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3352 - accuracy: 0.8700\n",
      "Epoch 51: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3352 - accuracy: 0.8700 - val_loss: 0.7966 - val_accuracy: 0.7546\n",
      "Epoch 52/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.8726\n",
      "Epoch 52: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3322 - accuracy: 0.8727 - val_loss: 0.7961 - val_accuracy: 0.7533\n",
      "Epoch 53/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8732\n",
      "Epoch 53: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3304 - accuracy: 0.8732 - val_loss: 0.7791 - val_accuracy: 0.7535\n",
      "Epoch 54/1000\n",
      "839/852 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8742\n",
      "Epoch 54: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3271 - accuracy: 0.8741 - val_loss: 0.8088 - val_accuracy: 0.7519\n",
      "Epoch 55/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3274 - accuracy: 0.8733\n",
      "Epoch 55: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3274 - accuracy: 0.8733 - val_loss: 0.7989 - val_accuracy: 0.7541\n",
      "Epoch 56/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8758\n",
      "Epoch 56: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3222 - accuracy: 0.8759 - val_loss: 0.8013 - val_accuracy: 0.7502\n",
      "Epoch 57/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.8769\n",
      "Epoch 57: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3231 - accuracy: 0.8769 - val_loss: 0.8111 - val_accuracy: 0.7499\n",
      "Epoch 58/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8753\n",
      "Epoch 58: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3228 - accuracy: 0.8753 - val_loss: 0.7969 - val_accuracy: 0.7514\n",
      "Epoch 59/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8782\n",
      "Epoch 59: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3197 - accuracy: 0.8781 - val_loss: 0.7950 - val_accuracy: 0.7523\n",
      "Epoch 60/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8773\n",
      "Epoch 60: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3184 - accuracy: 0.8773 - val_loss: 0.8058 - val_accuracy: 0.7479\n",
      "Epoch 61/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8784\n",
      "Epoch 61: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3182 - accuracy: 0.8784 - val_loss: 0.8087 - val_accuracy: 0.7517\n",
      "Epoch 62/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3149 - accuracy: 0.8783\n",
      "Epoch 62: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3150 - accuracy: 0.8783 - val_loss: 0.8168 - val_accuracy: 0.7480\n",
      "Epoch 63/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.8801\n",
      "Epoch 63: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3163 - accuracy: 0.8801 - val_loss: 0.8126 - val_accuracy: 0.7485\n",
      "Epoch 64/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8788\n",
      "Epoch 64: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3124 - accuracy: 0.8787 - val_loss: 0.8197 - val_accuracy: 0.7526\n",
      "Epoch 65/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8800\n",
      "Epoch 65: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3128 - accuracy: 0.8800 - val_loss: 0.8224 - val_accuracy: 0.7556\n",
      "Epoch 66/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.8813\n",
      "Epoch 66: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3085 - accuracy: 0.8813 - val_loss: 0.8342 - val_accuracy: 0.7485\n",
      "Epoch 67/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.8820\n",
      "Epoch 67: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3098 - accuracy: 0.8820 - val_loss: 0.8213 - val_accuracy: 0.7448\n",
      "Epoch 68/1000\n",
      "840/852 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8807\n",
      "Epoch 68: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3120 - accuracy: 0.8807 - val_loss: 0.8058 - val_accuracy: 0.7530\n",
      "Epoch 69/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8815\n",
      "Epoch 69: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3111 - accuracy: 0.8815 - val_loss: 0.8237 - val_accuracy: 0.7482\n",
      "Epoch 70/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8824\n",
      "Epoch 70: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3071 - accuracy: 0.8824 - val_loss: 0.8167 - val_accuracy: 0.7456\n",
      "Epoch 71/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8822\n",
      "Epoch 71: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3082 - accuracy: 0.8821 - val_loss: 0.8097 - val_accuracy: 0.7514\n",
      "Epoch 72/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8831\n",
      "Epoch 72: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3056 - accuracy: 0.8831 - val_loss: 0.8344 - val_accuracy: 0.7480\n",
      "Epoch 73/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8816\n",
      "Epoch 73: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3059 - accuracy: 0.8816 - val_loss: 0.8220 - val_accuracy: 0.7485\n",
      "Epoch 74/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8846\n",
      "Epoch 74: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3006 - accuracy: 0.8846 - val_loss: 0.8458 - val_accuracy: 0.7471\n",
      "Epoch 75/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8843\n",
      "Epoch 75: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3011 - accuracy: 0.8843 - val_loss: 0.8520 - val_accuracy: 0.7449\n",
      "Epoch 76/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8843\n",
      "Epoch 76: val_accuracy did not improve from 0.75839\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3022 - accuracy: 0.8844 - val_loss: 0.8241 - val_accuracy: 0.7509\n",
      "381/381 [==============================] - 0s 712us/step\n",
      "0.733854314544812\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (109013, 256)             3584      \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (109013, 256)             0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (109013, 256)             65792     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (109013, 256)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (109013, 128)             32896     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (109013, 128)             0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (109013, 128)             16512     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (109013, 128)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (109013, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.8905 - accuracy: 0.6542\n",
      "Epoch 1: val_accuracy improved from -inf to 0.70139, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.8903 - accuracy: 0.6542 - val_loss: 0.7750 - val_accuracy: 0.7014\n",
      "Epoch 2/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.7471 - accuracy: 0.7108\n",
      "Epoch 2: val_accuracy improved from 0.70139 to 0.72904, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.7469 - accuracy: 0.7109 - val_loss: 0.7148 - val_accuracy: 0.7290\n",
      "Epoch 3/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.7328\n",
      "Epoch 3: val_accuracy improved from 0.72904 to 0.73274, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.6903 - accuracy: 0.7334 - val_loss: 0.6973 - val_accuracy: 0.7327\n",
      "Epoch 4/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.6486 - accuracy: 0.7481\n",
      "Epoch 4: val_accuracy improved from 0.73274 to 0.74237, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.6484 - accuracy: 0.7481 - val_loss: 0.6764 - val_accuracy: 0.7424\n",
      "Epoch 5/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.6181 - accuracy: 0.7602\n",
      "Epoch 5: val_accuracy improved from 0.74237 to 0.74582, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.6182 - accuracy: 0.7602 - val_loss: 0.6655 - val_accuracy: 0.7458\n",
      "Epoch 6/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.5916 - accuracy: 0.7707\n",
      "Epoch 6: val_accuracy improved from 0.74582 to 0.74944, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5918 - accuracy: 0.7705 - val_loss: 0.6694 - val_accuracy: 0.7494\n",
      "Epoch 7/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.5695 - accuracy: 0.7774\n",
      "Epoch 7: val_accuracy improved from 0.74944 to 0.75150, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.5694 - accuracy: 0.7773 - val_loss: 0.6660 - val_accuracy: 0.7515\n",
      "Epoch 8/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.5533 - accuracy: 0.7866\n",
      "Epoch 8: val_accuracy did not improve from 0.75150\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.5534 - accuracy: 0.7865 - val_loss: 0.6626 - val_accuracy: 0.7478\n",
      "Epoch 9/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.5353 - accuracy: 0.7911\n",
      "Epoch 9: val_accuracy did not improve from 0.75150\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5355 - accuracy: 0.7910 - val_loss: 0.6703 - val_accuracy: 0.7508\n",
      "Epoch 10/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.5211 - accuracy: 0.7987\n",
      "Epoch 10: val_accuracy improved from 0.75150 to 0.75504, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5212 - accuracy: 0.7986 - val_loss: 0.6701 - val_accuracy: 0.7550\n",
      "Epoch 11/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.5066 - accuracy: 0.8020\n",
      "Epoch 11: val_accuracy did not improve from 0.75504\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5065 - accuracy: 0.8020 - val_loss: 0.6719 - val_accuracy: 0.7524\n",
      "Epoch 12/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.4967 - accuracy: 0.8069\n",
      "Epoch 12: val_accuracy did not improve from 0.75504\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4967 - accuracy: 0.8069 - val_loss: 0.6802 - val_accuracy: 0.7513\n",
      "Epoch 13/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4835 - accuracy: 0.8132\n",
      "Epoch 13: val_accuracy did not improve from 0.75504\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4835 - accuracy: 0.8132 - val_loss: 0.6788 - val_accuracy: 0.7550\n",
      "Epoch 14/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.4750 - accuracy: 0.8152\n",
      "Epoch 14: val_accuracy improved from 0.75504 to 0.75965, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4751 - accuracy: 0.8152 - val_loss: 0.6725 - val_accuracy: 0.7596\n",
      "Epoch 15/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.8185\n",
      "Epoch 15: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4676 - accuracy: 0.8185 - val_loss: 0.6771 - val_accuracy: 0.7558\n",
      "Epoch 16/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4567 - accuracy: 0.8225\n",
      "Epoch 16: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4566 - accuracy: 0.8225 - val_loss: 0.6962 - val_accuracy: 0.7537\n",
      "Epoch 17/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.4490 - accuracy: 0.8262\n",
      "Epoch 17: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4489 - accuracy: 0.8264 - val_loss: 0.6912 - val_accuracy: 0.7581\n",
      "Epoch 18/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.4431 - accuracy: 0.8274\n",
      "Epoch 18: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4431 - accuracy: 0.8274 - val_loss: 0.6949 - val_accuracy: 0.7573\n",
      "Epoch 19/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8304\n",
      "Epoch 19: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4360 - accuracy: 0.8303 - val_loss: 0.6952 - val_accuracy: 0.7555\n",
      "Epoch 20/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.8319\n",
      "Epoch 20: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4301 - accuracy: 0.8318 - val_loss: 0.7232 - val_accuracy: 0.7564\n",
      "Epoch 21/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.4246 - accuracy: 0.8345\n",
      "Epoch 21: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4251 - accuracy: 0.8345 - val_loss: 0.7250 - val_accuracy: 0.7481\n",
      "Epoch 22/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8358\n",
      "Epoch 22: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4188 - accuracy: 0.8358 - val_loss: 0.7360 - val_accuracy: 0.7535\n",
      "Epoch 23/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.4118 - accuracy: 0.8402\n",
      "Epoch 23: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4119 - accuracy: 0.8402 - val_loss: 0.7142 - val_accuracy: 0.7579\n",
      "Epoch 24/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4085 - accuracy: 0.8401\n",
      "Epoch 24: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4084 - accuracy: 0.8402 - val_loss: 0.7179 - val_accuracy: 0.7551\n",
      "Epoch 25/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.4034 - accuracy: 0.8442\n",
      "Epoch 25: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4034 - accuracy: 0.8442 - val_loss: 0.7237 - val_accuracy: 0.7565\n",
      "Epoch 26/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.3977 - accuracy: 0.8454\n",
      "Epoch 26: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3978 - accuracy: 0.8453 - val_loss: 0.7368 - val_accuracy: 0.7546\n",
      "Epoch 27/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.8449\n",
      "Epoch 27: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3950 - accuracy: 0.8449 - val_loss: 0.7307 - val_accuracy: 0.7587\n",
      "Epoch 28/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8475\n",
      "Epoch 28: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3906 - accuracy: 0.8475 - val_loss: 0.7296 - val_accuracy: 0.7554\n",
      "Epoch 29/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8499\n",
      "Epoch 29: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3860 - accuracy: 0.8499 - val_loss: 0.7615 - val_accuracy: 0.7531\n",
      "Epoch 30/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8508\n",
      "Epoch 30: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3823 - accuracy: 0.8509 - val_loss: 0.7473 - val_accuracy: 0.7583\n",
      "Epoch 31/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.8516\n",
      "Epoch 31: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3824 - accuracy: 0.8516 - val_loss: 0.7349 - val_accuracy: 0.7578\n",
      "Epoch 32/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8537\n",
      "Epoch 32: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3763 - accuracy: 0.8538 - val_loss: 0.7509 - val_accuracy: 0.7592\n",
      "Epoch 33/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8539\n",
      "Epoch 33: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3752 - accuracy: 0.8538 - val_loss: 0.7457 - val_accuracy: 0.7573\n",
      "Epoch 34/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.8538\n",
      "Epoch 34: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3740 - accuracy: 0.8538 - val_loss: 0.7470 - val_accuracy: 0.7583\n",
      "Epoch 35/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8562\n",
      "Epoch 35: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3699 - accuracy: 0.8563 - val_loss: 0.7334 - val_accuracy: 0.7596\n",
      "Epoch 36/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8576\n",
      "Epoch 36: val_accuracy did not improve from 0.75965\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3684 - accuracy: 0.8576 - val_loss: 0.7646 - val_accuracy: 0.7559\n",
      "Epoch 37/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.8585\n",
      "Epoch 37: val_accuracy improved from 0.75965 to 0.76491, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3645 - accuracy: 0.8584 - val_loss: 0.7466 - val_accuracy: 0.7649\n",
      "Epoch 38/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8602\n",
      "Epoch 38: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3602 - accuracy: 0.8601 - val_loss: 0.7647 - val_accuracy: 0.7579\n",
      "Epoch 39/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.8601\n",
      "Epoch 39: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3571 - accuracy: 0.8602 - val_loss: 0.7586 - val_accuracy: 0.7554\n",
      "Epoch 40/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8619\n",
      "Epoch 40: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3564 - accuracy: 0.8619 - val_loss: 0.7763 - val_accuracy: 0.7558\n",
      "Epoch 41/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3534 - accuracy: 0.8623\n",
      "Epoch 41: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3534 - accuracy: 0.8624 - val_loss: 0.7575 - val_accuracy: 0.7610\n",
      "Epoch 42/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8646\n",
      "Epoch 42: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3507 - accuracy: 0.8645 - val_loss: 0.7688 - val_accuracy: 0.7638\n",
      "Epoch 43/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.8646\n",
      "Epoch 43: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3493 - accuracy: 0.8646 - val_loss: 0.7583 - val_accuracy: 0.7575\n",
      "Epoch 44/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8650\n",
      "Epoch 44: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3475 - accuracy: 0.8651 - val_loss: 0.7817 - val_accuracy: 0.7549\n",
      "Epoch 45/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3467 - accuracy: 0.8657\n",
      "Epoch 45: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3470 - accuracy: 0.8657 - val_loss: 0.7736 - val_accuracy: 0.7604\n",
      "Epoch 46/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8669\n",
      "Epoch 46: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3441 - accuracy: 0.8669 - val_loss: 0.7695 - val_accuracy: 0.7620\n",
      "Epoch 47/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8679\n",
      "Epoch 47: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3420 - accuracy: 0.8680 - val_loss: 0.7829 - val_accuracy: 0.7568\n",
      "Epoch 48/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3401 - accuracy: 0.8686\n",
      "Epoch 48: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3400 - accuracy: 0.8685 - val_loss: 0.7837 - val_accuracy: 0.7562\n",
      "Epoch 49/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8694\n",
      "Epoch 49: val_accuracy did not improve from 0.76491\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3374 - accuracy: 0.8694 - val_loss: 0.7929 - val_accuracy: 0.7551\n",
      "Epoch 50/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8710\n",
      "Epoch 50: val_accuracy improved from 0.76491 to 0.76508, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3359 - accuracy: 0.8710 - val_loss: 0.7641 - val_accuracy: 0.7651\n",
      "Epoch 51/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8713\n",
      "Epoch 51: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3334 - accuracy: 0.8713 - val_loss: 0.7832 - val_accuracy: 0.7629\n",
      "Epoch 52/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3330 - accuracy: 0.8708\n",
      "Epoch 52: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3330 - accuracy: 0.8708 - val_loss: 0.7733 - val_accuracy: 0.7578\n",
      "Epoch 53/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.8716\n",
      "Epoch 53: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3327 - accuracy: 0.8715 - val_loss: 0.8106 - val_accuracy: 0.7555\n",
      "Epoch 54/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8732\n",
      "Epoch 54: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3283 - accuracy: 0.8732 - val_loss: 0.7845 - val_accuracy: 0.7550\n",
      "Epoch 55/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8745\n",
      "Epoch 55: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3270 - accuracy: 0.8745 - val_loss: 0.7902 - val_accuracy: 0.7570\n",
      "Epoch 56/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3271 - accuracy: 0.8734\n",
      "Epoch 56: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3271 - accuracy: 0.8734 - val_loss: 0.7903 - val_accuracy: 0.7542\n",
      "Epoch 57/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.8747\n",
      "Epoch 57: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3264 - accuracy: 0.8745 - val_loss: 0.8038 - val_accuracy: 0.7497\n",
      "Epoch 58/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8762\n",
      "Epoch 58: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3227 - accuracy: 0.8763 - val_loss: 0.7949 - val_accuracy: 0.7566\n",
      "Epoch 59/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8753\n",
      "Epoch 59: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3234 - accuracy: 0.8752 - val_loss: 0.7957 - val_accuracy: 0.7532\n",
      "Epoch 60/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8787\n",
      "Epoch 60: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3202 - accuracy: 0.8786 - val_loss: 0.8042 - val_accuracy: 0.7523\n",
      "Epoch 61/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3199 - accuracy: 0.8766\n",
      "Epoch 61: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3198 - accuracy: 0.8766 - val_loss: 0.8175 - val_accuracy: 0.7519\n",
      "Epoch 62/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8771\n",
      "Epoch 62: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3201 - accuracy: 0.8772 - val_loss: 0.7858 - val_accuracy: 0.7573\n",
      "Epoch 63/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3157 - accuracy: 0.8783\n",
      "Epoch 63: val_accuracy did not improve from 0.76508\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3158 - accuracy: 0.8784 - val_loss: 0.7830 - val_accuracy: 0.7528\n",
      "Epoch 64/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3181 - accuracy: 0.8771\n",
      "Epoch 64: val_accuracy improved from 0.76508 to 0.76516, saving model to best_weights_dense_3.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3181 - accuracy: 0.8771 - val_loss: 0.7976 - val_accuracy: 0.7652\n",
      "Epoch 65/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3136 - accuracy: 0.8790\n",
      "Epoch 65: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3136 - accuracy: 0.8789 - val_loss: 0.8191 - val_accuracy: 0.7530\n",
      "Epoch 66/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8796\n",
      "Epoch 66: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3125 - accuracy: 0.8796 - val_loss: 0.7942 - val_accuracy: 0.7585\n",
      "Epoch 67/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3149 - accuracy: 0.8788\n",
      "Epoch 67: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3147 - accuracy: 0.8788 - val_loss: 0.7953 - val_accuracy: 0.7579\n",
      "Epoch 68/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8817\n",
      "Epoch 68: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3101 - accuracy: 0.8817 - val_loss: 0.8042 - val_accuracy: 0.7557\n",
      "Epoch 69/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.8792\n",
      "Epoch 69: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3121 - accuracy: 0.8793 - val_loss: 0.8334 - val_accuracy: 0.7544\n",
      "Epoch 70/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8819\n",
      "Epoch 70: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3085 - accuracy: 0.8819 - val_loss: 0.8317 - val_accuracy: 0.7570\n",
      "Epoch 71/1000\n",
      "840/852 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8830\n",
      "Epoch 71: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3040 - accuracy: 0.8829 - val_loss: 0.8250 - val_accuracy: 0.7516\n",
      "Epoch 72/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8809\n",
      "Epoch 72: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3075 - accuracy: 0.8810 - val_loss: 0.8156 - val_accuracy: 0.7580\n",
      "Epoch 73/1000\n",
      "840/852 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.8827\n",
      "Epoch 73: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3062 - accuracy: 0.8827 - val_loss: 0.8211 - val_accuracy: 0.7564\n",
      "Epoch 74/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8825\n",
      "Epoch 74: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3050 - accuracy: 0.8823 - val_loss: 0.8058 - val_accuracy: 0.7593\n",
      "Epoch 75/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8831\n",
      "Epoch 75: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3054 - accuracy: 0.8829 - val_loss: 0.8125 - val_accuracy: 0.7516\n",
      "Epoch 76/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8840\n",
      "Epoch 76: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3027 - accuracy: 0.8840 - val_loss: 0.8264 - val_accuracy: 0.7538\n",
      "Epoch 77/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.8834\n",
      "Epoch 77: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3020 - accuracy: 0.8835 - val_loss: 0.8161 - val_accuracy: 0.7564\n",
      "Epoch 78/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8844\n",
      "Epoch 78: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3028 - accuracy: 0.8844 - val_loss: 0.8297 - val_accuracy: 0.7517\n",
      "Epoch 79/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8861\n",
      "Epoch 79: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2977 - accuracy: 0.8861 - val_loss: 0.8227 - val_accuracy: 0.7527\n",
      "Epoch 80/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.8859\n",
      "Epoch 80: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2980 - accuracy: 0.8859 - val_loss: 0.8108 - val_accuracy: 0.7610\n",
      "Epoch 81/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8861\n",
      "Epoch 81: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2967 - accuracy: 0.8862 - val_loss: 0.8236 - val_accuracy: 0.7580\n",
      "Epoch 82/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8862\n",
      "Epoch 82: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2973 - accuracy: 0.8863 - val_loss: 0.8160 - val_accuracy: 0.7571\n",
      "Epoch 83/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8857\n",
      "Epoch 83: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2956 - accuracy: 0.8857 - val_loss: 0.8143 - val_accuracy: 0.7533\n",
      "Epoch 84/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8864\n",
      "Epoch 84: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2956 - accuracy: 0.8865 - val_loss: 0.8368 - val_accuracy: 0.7532\n",
      "Epoch 85/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8876\n",
      "Epoch 85: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2939 - accuracy: 0.8877 - val_loss: 0.8392 - val_accuracy: 0.7551\n",
      "Epoch 86/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8861\n",
      "Epoch 86: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2942 - accuracy: 0.8861 - val_loss: 0.8411 - val_accuracy: 0.7548\n",
      "Epoch 87/1000\n",
      "838/852 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8863\n",
      "Epoch 87: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2963 - accuracy: 0.8863 - val_loss: 0.8253 - val_accuracy: 0.7579\n",
      "Epoch 88/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8887\n",
      "Epoch 88: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2919 - accuracy: 0.8886 - val_loss: 0.8383 - val_accuracy: 0.7536\n",
      "Epoch 89/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8869\n",
      "Epoch 89: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2958 - accuracy: 0.8869 - val_loss: 0.8215 - val_accuracy: 0.7562\n",
      "Epoch 90/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8887\n",
      "Epoch 90: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2913 - accuracy: 0.8886 - val_loss: 0.8452 - val_accuracy: 0.7504\n",
      "Epoch 91/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.8889\n",
      "Epoch 91: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2906 - accuracy: 0.8889 - val_loss: 0.8153 - val_accuracy: 0.7551\n",
      "Epoch 92/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.8881\n",
      "Epoch 92: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2904 - accuracy: 0.8881 - val_loss: 0.8554 - val_accuracy: 0.7540\n",
      "Epoch 93/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.2886 - accuracy: 0.8895\n",
      "Epoch 93: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2886 - accuracy: 0.8894 - val_loss: 0.8291 - val_accuracy: 0.7545\n",
      "Epoch 94/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8883\n",
      "Epoch 94: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2911 - accuracy: 0.8882 - val_loss: 0.8369 - val_accuracy: 0.7571\n",
      "Epoch 95/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.8900\n",
      "Epoch 95: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2857 - accuracy: 0.8900 - val_loss: 0.8294 - val_accuracy: 0.7561\n",
      "Epoch 96/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.2859 - accuracy: 0.8911\n",
      "Epoch 96: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2858 - accuracy: 0.8912 - val_loss: 0.8472 - val_accuracy: 0.7527\n",
      "Epoch 97/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.8902\n",
      "Epoch 97: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2864 - accuracy: 0.8902 - val_loss: 0.8631 - val_accuracy: 0.7513\n",
      "Epoch 98/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.2861 - accuracy: 0.8912\n",
      "Epoch 98: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2864 - accuracy: 0.8911 - val_loss: 0.8441 - val_accuracy: 0.7579\n",
      "Epoch 99/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.8905\n",
      "Epoch 99: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2858 - accuracy: 0.8905 - val_loss: 0.8318 - val_accuracy: 0.7531\n",
      "Epoch 100/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8909\n",
      "Epoch 100: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2839 - accuracy: 0.8909 - val_loss: 0.8350 - val_accuracy: 0.7581\n",
      "Epoch 101/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.8918\n",
      "Epoch 101: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2822 - accuracy: 0.8918 - val_loss: 0.8668 - val_accuracy: 0.7556\n",
      "Epoch 102/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.2819 - accuracy: 0.8922\n",
      "Epoch 102: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2821 - accuracy: 0.8922 - val_loss: 0.8470 - val_accuracy: 0.7530\n",
      "Epoch 103/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.2855 - accuracy: 0.8904\n",
      "Epoch 103: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2856 - accuracy: 0.8904 - val_loss: 0.8576 - val_accuracy: 0.7545\n",
      "Epoch 104/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.2801 - accuracy: 0.8926\n",
      "Epoch 104: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2801 - accuracy: 0.8926 - val_loss: 0.8598 - val_accuracy: 0.7543\n",
      "Epoch 105/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.8933\n",
      "Epoch 105: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2793 - accuracy: 0.8933 - val_loss: 0.8369 - val_accuracy: 0.7553\n",
      "Epoch 106/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.2829 - accuracy: 0.8929\n",
      "Epoch 106: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2832 - accuracy: 0.8928 - val_loss: 0.8844 - val_accuracy: 0.7540\n",
      "Epoch 107/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.8927\n",
      "Epoch 107: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2808 - accuracy: 0.8926 - val_loss: 0.8529 - val_accuracy: 0.7559\n",
      "Epoch 108/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.8928\n",
      "Epoch 108: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2803 - accuracy: 0.8928 - val_loss: 0.8403 - val_accuracy: 0.7545\n",
      "Epoch 109/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.8938\n",
      "Epoch 109: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2774 - accuracy: 0.8937 - val_loss: 0.8598 - val_accuracy: 0.7528\n",
      "Epoch 110/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.2788 - accuracy: 0.8930\n",
      "Epoch 110: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2788 - accuracy: 0.8929 - val_loss: 0.8587 - val_accuracy: 0.7563\n",
      "Epoch 111/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.8939\n",
      "Epoch 111: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2794 - accuracy: 0.8938 - val_loss: 0.8353 - val_accuracy: 0.7560\n",
      "Epoch 112/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8926\n",
      "Epoch 112: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2785 - accuracy: 0.8927 - val_loss: 0.8690 - val_accuracy: 0.7531\n",
      "Epoch 113/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.8949\n",
      "Epoch 113: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.2763 - accuracy: 0.8949 - val_loss: 0.8379 - val_accuracy: 0.7582\n",
      "Epoch 114/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8958\n",
      "Epoch 114: val_accuracy did not improve from 0.76516\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.2732 - accuracy: 0.8958 - val_loss: 0.8765 - val_accuracy: 0.7530\n",
      "380/380 [==============================] - 0s 748us/step\n",
      "0.7448110737497358\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_25 (Dense)            (109360, 256)             3584      \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (109360, 256)             0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (109360, 256)             65792     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (109360, 256)             0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (109360, 128)             32896     \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (109360, 128)             0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (109360, 128)             16512     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (109360, 128)             0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (109360, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "850/855 [============================>.] - ETA: 0s - loss: 0.8905 - accuracy: 0.6540\n",
      "Epoch 1: val_accuracy improved from -inf to 0.70795, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.8897 - accuracy: 0.6542 - val_loss: 0.7569 - val_accuracy: 0.7079\n",
      "Epoch 2/1000\n",
      "845/855 [============================>.] - ETA: 0s - loss: 0.7468 - accuracy: 0.7103\n",
      "Epoch 2: val_accuracy improved from 0.70795 to 0.73641, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.7467 - accuracy: 0.7105 - val_loss: 0.7036 - val_accuracy: 0.7364\n",
      "Epoch 3/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.7321\n",
      "Epoch 3: val_accuracy improved from 0.73641 to 0.74665, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.6884 - accuracy: 0.7323 - val_loss: 0.6784 - val_accuracy: 0.7467\n",
      "Epoch 4/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.6483 - accuracy: 0.7487\n",
      "Epoch 4: val_accuracy improved from 0.74665 to 0.74928, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.6479 - accuracy: 0.7488 - val_loss: 0.6556 - val_accuracy: 0.7493\n",
      "Epoch 5/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.6197 - accuracy: 0.7605\n",
      "Epoch 5: val_accuracy improved from 0.74928 to 0.75360, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 3s 4ms/step - loss: 0.6193 - accuracy: 0.7605 - val_loss: 0.6671 - val_accuracy: 0.7536\n",
      "Epoch 6/1000\n",
      "851/855 [============================>.] - ETA: 0s - loss: 0.5943 - accuracy: 0.7702\n",
      "Epoch 6: val_accuracy improved from 0.75360 to 0.75496, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.5944 - accuracy: 0.7702 - val_loss: 0.6579 - val_accuracy: 0.7550\n",
      "Epoch 7/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.5745 - accuracy: 0.7780\n",
      "Epoch 7: val_accuracy improved from 0.75496 to 0.75589, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.5745 - accuracy: 0.7778 - val_loss: 0.6560 - val_accuracy: 0.7559\n",
      "Epoch 8/1000\n",
      "843/855 [============================>.] - ETA: 0s - loss: 0.5548 - accuracy: 0.7835\n",
      "Epoch 8: val_accuracy improved from 0.75589 to 0.76453, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.5548 - accuracy: 0.7836 - val_loss: 0.6416 - val_accuracy: 0.7645\n",
      "Epoch 9/1000\n",
      "844/855 [============================>.] - ETA: 0s - loss: 0.5395 - accuracy: 0.7913\n",
      "Epoch 9: val_accuracy did not improve from 0.76453\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.5394 - accuracy: 0.7914 - val_loss: 0.6420 - val_accuracy: 0.7630\n",
      "Epoch 10/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.5234 - accuracy: 0.7967\n",
      "Epoch 10: val_accuracy did not improve from 0.76453\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.5236 - accuracy: 0.7966 - val_loss: 0.6434 - val_accuracy: 0.7622\n",
      "Epoch 11/1000\n",
      "850/855 [============================>.] - ETA: 0s - loss: 0.5095 - accuracy: 0.8038\n",
      "Epoch 11: val_accuracy did not improve from 0.76453\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.5097 - accuracy: 0.8039 - val_loss: 0.6491 - val_accuracy: 0.7635\n",
      "Epoch 12/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.4988 - accuracy: 0.8060\n",
      "Epoch 12: val_accuracy did not improve from 0.76453\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4988 - accuracy: 0.8061 - val_loss: 0.6578 - val_accuracy: 0.7633\n",
      "Epoch 13/1000\n",
      "850/855 [============================>.] - ETA: 0s - loss: 0.4897 - accuracy: 0.8102\n",
      "Epoch 13: val_accuracy improved from 0.76453 to 0.76478, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4899 - accuracy: 0.8101 - val_loss: 0.6533 - val_accuracy: 0.7648\n",
      "Epoch 14/1000\n",
      "850/855 [============================>.] - ETA: 0s - loss: 0.4773 - accuracy: 0.8157\n",
      "Epoch 14: val_accuracy did not improve from 0.76478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4774 - accuracy: 0.8156 - val_loss: 0.6530 - val_accuracy: 0.7626\n",
      "Epoch 15/1000\n",
      "845/855 [============================>.] - ETA: 0s - loss: 0.4688 - accuracy: 0.8190\n",
      "Epoch 15: val_accuracy improved from 0.76478 to 0.76512, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4689 - accuracy: 0.8190 - val_loss: 0.6617 - val_accuracy: 0.7651\n",
      "Epoch 16/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.4606 - accuracy: 0.8209\n",
      "Epoch 16: val_accuracy improved from 0.76512 to 0.76749, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4608 - accuracy: 0.8208 - val_loss: 0.6654 - val_accuracy: 0.7675\n",
      "Epoch 17/1000\n",
      "851/855 [============================>.] - ETA: 0s - loss: 0.4513 - accuracy: 0.8247\n",
      "Epoch 17: val_accuracy did not improve from 0.76749\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4511 - accuracy: 0.8248 - val_loss: 0.6910 - val_accuracy: 0.7658\n",
      "Epoch 18/1000\n",
      "848/855 [============================>.] - ETA: 0s - loss: 0.4447 - accuracy: 0.8281\n",
      "Epoch 18: val_accuracy did not improve from 0.76749\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.4448 - accuracy: 0.8281 - val_loss: 0.6797 - val_accuracy: 0.7594\n",
      "Epoch 19/1000\n",
      "845/855 [============================>.] - ETA: 0s - loss: 0.4415 - accuracy: 0.8286\n",
      "Epoch 19: val_accuracy did not improve from 0.76749\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.4416 - accuracy: 0.8285 - val_loss: 0.6871 - val_accuracy: 0.7597\n",
      "Epoch 20/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.4333 - accuracy: 0.8313\n",
      "Epoch 20: val_accuracy did not improve from 0.76749\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.4332 - accuracy: 0.8312 - val_loss: 0.6811 - val_accuracy: 0.7637\n",
      "Epoch 21/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.4266 - accuracy: 0.8348\n",
      "Epoch 21: val_accuracy did not improve from 0.76749\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.4265 - accuracy: 0.8349 - val_loss: 0.6848 - val_accuracy: 0.7661\n",
      "Epoch 22/1000\n",
      "844/855 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8364\n",
      "Epoch 22: val_accuracy did not improve from 0.76749\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4207 - accuracy: 0.8363 - val_loss: 0.7140 - val_accuracy: 0.7633\n",
      "Epoch 23/1000\n",
      "852/855 [============================>.] - ETA: 0s - loss: 0.4149 - accuracy: 0.8396\n",
      "Epoch 23: val_accuracy did not improve from 0.76749\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.4147 - accuracy: 0.8396 - val_loss: 0.6984 - val_accuracy: 0.7655\n",
      "Epoch 24/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.4084 - accuracy: 0.8404\n",
      "Epoch 24: val_accuracy improved from 0.76749 to 0.77300, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.4088 - accuracy: 0.8403 - val_loss: 0.7040 - val_accuracy: 0.7730\n",
      "Epoch 25/1000\n",
      "847/855 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8436\n",
      "Epoch 25: val_accuracy did not improve from 0.77300\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.4049 - accuracy: 0.8435 - val_loss: 0.6826 - val_accuracy: 0.7707\n",
      "Epoch 26/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.4008 - accuracy: 0.8453\n",
      "Epoch 26: val_accuracy did not improve from 0.77300\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.4006 - accuracy: 0.8453 - val_loss: 0.7216 - val_accuracy: 0.7638\n",
      "Epoch 27/1000\n",
      "847/855 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8453\n",
      "Epoch 27: val_accuracy did not improve from 0.77300\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3982 - accuracy: 0.8453 - val_loss: 0.7075 - val_accuracy: 0.7635\n",
      "Epoch 28/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.8474\n",
      "Epoch 28: val_accuracy improved from 0.77300 to 0.77478, saving model to best_weights_dense_4.hdf5\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3949 - accuracy: 0.8474 - val_loss: 0.6831 - val_accuracy: 0.7748\n",
      "Epoch 29/1000\n",
      "848/855 [============================>.] - ETA: 0s - loss: 0.3918 - accuracy: 0.8491\n",
      "Epoch 29: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3917 - accuracy: 0.8491 - val_loss: 0.7209 - val_accuracy: 0.7677\n",
      "Epoch 30/1000\n",
      "844/855 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.8509\n",
      "Epoch 30: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3860 - accuracy: 0.8510 - val_loss: 0.7164 - val_accuracy: 0.7661\n",
      "Epoch 31/1000\n",
      "848/855 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.8513\n",
      "Epoch 31: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3844 - accuracy: 0.8513 - val_loss: 0.7106 - val_accuracy: 0.7683\n",
      "Epoch 32/1000\n",
      "855/855 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8536\n",
      "Epoch 32: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3798 - accuracy: 0.8536 - val_loss: 0.6974 - val_accuracy: 0.7630\n",
      "Epoch 33/1000\n",
      "851/855 [============================>.] - ETA: 0s - loss: 0.3747 - accuracy: 0.8554\n",
      "Epoch 33: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3748 - accuracy: 0.8553 - val_loss: 0.7108 - val_accuracy: 0.7705\n",
      "Epoch 34/1000\n",
      "847/855 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8559\n",
      "Epoch 34: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3736 - accuracy: 0.8560 - val_loss: 0.7202 - val_accuracy: 0.7716\n",
      "Epoch 35/1000\n",
      "848/855 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8571\n",
      "Epoch 35: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3672 - accuracy: 0.8571 - val_loss: 0.7279 - val_accuracy: 0.7701\n",
      "Epoch 36/1000\n",
      "847/855 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8578\n",
      "Epoch 36: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3683 - accuracy: 0.8578 - val_loss: 0.7228 - val_accuracy: 0.7729\n",
      "Epoch 37/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8587\n",
      "Epoch 37: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3659 - accuracy: 0.8588 - val_loss: 0.7204 - val_accuracy: 0.7667\n",
      "Epoch 38/1000\n",
      "845/855 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.8596\n",
      "Epoch 38: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3625 - accuracy: 0.8596 - val_loss: 0.7497 - val_accuracy: 0.7690\n",
      "Epoch 39/1000\n",
      "852/855 [============================>.] - ETA: 0s - loss: 0.3581 - accuracy: 0.8623\n",
      "Epoch 39: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3580 - accuracy: 0.8623 - val_loss: 0.7420 - val_accuracy: 0.7688\n",
      "Epoch 40/1000\n",
      "847/855 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8623\n",
      "Epoch 40: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3566 - accuracy: 0.8621 - val_loss: 0.7210 - val_accuracy: 0.7638\n",
      "Epoch 41/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.3541 - accuracy: 0.8634\n",
      "Epoch 41: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3541 - accuracy: 0.8634 - val_loss: 0.7385 - val_accuracy: 0.7688\n",
      "Epoch 42/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.8632\n",
      "Epoch 42: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3546 - accuracy: 0.8632 - val_loss: 0.7509 - val_accuracy: 0.7632\n",
      "Epoch 43/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.3507 - accuracy: 0.8652\n",
      "Epoch 43: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3508 - accuracy: 0.8651 - val_loss: 0.7503 - val_accuracy: 0.7624\n",
      "Epoch 44/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8663\n",
      "Epoch 44: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3487 - accuracy: 0.8661 - val_loss: 0.7262 - val_accuracy: 0.7728\n",
      "Epoch 45/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.3460 - accuracy: 0.8663\n",
      "Epoch 45: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3461 - accuracy: 0.8662 - val_loss: 0.7489 - val_accuracy: 0.7714\n",
      "Epoch 46/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.3444 - accuracy: 0.8677\n",
      "Epoch 46: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3444 - accuracy: 0.8677 - val_loss: 0.7557 - val_accuracy: 0.7625\n",
      "Epoch 47/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8671\n",
      "Epoch 47: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3449 - accuracy: 0.8671 - val_loss: 0.7780 - val_accuracy: 0.7550\n",
      "Epoch 48/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8689\n",
      "Epoch 48: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3422 - accuracy: 0.8689 - val_loss: 0.7394 - val_accuracy: 0.7621\n",
      "Epoch 49/1000\n",
      "850/855 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8689\n",
      "Epoch 49: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3414 - accuracy: 0.8689 - val_loss: 0.7625 - val_accuracy: 0.7637\n",
      "Epoch 50/1000\n",
      "844/855 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.8703\n",
      "Epoch 50: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3340 - accuracy: 0.8701 - val_loss: 0.7634 - val_accuracy: 0.7683\n",
      "Epoch 51/1000\n",
      "848/855 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8703\n",
      "Epoch 51: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3373 - accuracy: 0.8702 - val_loss: 0.7574 - val_accuracy: 0.7687\n",
      "Epoch 52/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8702\n",
      "Epoch 52: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3350 - accuracy: 0.8702 - val_loss: 0.7790 - val_accuracy: 0.7655\n",
      "Epoch 53/1000\n",
      "844/855 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.8720\n",
      "Epoch 53: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3319 - accuracy: 0.8723 - val_loss: 0.7762 - val_accuracy: 0.7673\n",
      "Epoch 54/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8730\n",
      "Epoch 54: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3304 - accuracy: 0.8730 - val_loss: 0.7613 - val_accuracy: 0.7701\n",
      "Epoch 55/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8724\n",
      "Epoch 55: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3316 - accuracy: 0.8724 - val_loss: 0.7657 - val_accuracy: 0.7663\n",
      "Epoch 56/1000\n",
      "845/855 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.8736\n",
      "Epoch 56: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3298 - accuracy: 0.8736 - val_loss: 0.7590 - val_accuracy: 0.7674\n",
      "Epoch 57/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.3267 - accuracy: 0.8739\n",
      "Epoch 57: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3263 - accuracy: 0.8740 - val_loss: 0.7771 - val_accuracy: 0.7659\n",
      "Epoch 58/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8752\n",
      "Epoch 58: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3250 - accuracy: 0.8752 - val_loss: 0.7609 - val_accuracy: 0.7652\n",
      "Epoch 59/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.3232 - accuracy: 0.8755\n",
      "Epoch 59: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3234 - accuracy: 0.8753 - val_loss: 0.7750 - val_accuracy: 0.7641\n",
      "Epoch 60/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8767\n",
      "Epoch 60: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3223 - accuracy: 0.8767 - val_loss: 0.7718 - val_accuracy: 0.7708\n",
      "Epoch 61/1000\n",
      "844/855 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8765\n",
      "Epoch 61: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3220 - accuracy: 0.8765 - val_loss: 0.7574 - val_accuracy: 0.7688\n",
      "Epoch 62/1000\n",
      "851/855 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8769\n",
      "Epoch 62: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3212 - accuracy: 0.8770 - val_loss: 0.7724 - val_accuracy: 0.7691\n",
      "Epoch 63/1000\n",
      "847/855 [============================>.] - ETA: 0s - loss: 0.3225 - accuracy: 0.8759\n",
      "Epoch 63: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3224 - accuracy: 0.8760 - val_loss: 0.7755 - val_accuracy: 0.7681\n",
      "Epoch 64/1000\n",
      "847/855 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8792\n",
      "Epoch 64: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3172 - accuracy: 0.8793 - val_loss: 0.7897 - val_accuracy: 0.7663\n",
      "Epoch 65/1000\n",
      "850/855 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.8776\n",
      "Epoch 65: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3178 - accuracy: 0.8777 - val_loss: 0.7813 - val_accuracy: 0.7701\n",
      "Epoch 66/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.8789\n",
      "Epoch 66: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3174 - accuracy: 0.8788 - val_loss: 0.7655 - val_accuracy: 0.7688\n",
      "Epoch 67/1000\n",
      "851/855 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8777\n",
      "Epoch 67: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3151 - accuracy: 0.8777 - val_loss: 0.7718 - val_accuracy: 0.7672\n",
      "Epoch 68/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8792\n",
      "Epoch 68: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3135 - accuracy: 0.8793 - val_loss: 0.7990 - val_accuracy: 0.7705\n",
      "Epoch 69/1000\n",
      "853/855 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8803\n",
      "Epoch 69: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3119 - accuracy: 0.8803 - val_loss: 0.7714 - val_accuracy: 0.7687\n",
      "Epoch 70/1000\n",
      "850/855 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8798\n",
      "Epoch 70: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3102 - accuracy: 0.8799 - val_loss: 0.7931 - val_accuracy: 0.7681\n",
      "Epoch 71/1000\n",
      "848/855 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8802\n",
      "Epoch 71: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 5ms/step - loss: 0.3109 - accuracy: 0.8801 - val_loss: 0.7878 - val_accuracy: 0.7691\n",
      "Epoch 72/1000\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.8811\n",
      "Epoch 72: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3096 - accuracy: 0.8811 - val_loss: 0.7743 - val_accuracy: 0.7712\n",
      "Epoch 73/1000\n",
      "846/855 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8823\n",
      "Epoch 73: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3077 - accuracy: 0.8823 - val_loss: 0.7820 - val_accuracy: 0.7710\n",
      "Epoch 74/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.3064 - accuracy: 0.8822\n",
      "Epoch 74: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3066 - accuracy: 0.8822 - val_loss: 0.7637 - val_accuracy: 0.7719\n",
      "Epoch 75/1000\n",
      "843/855 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8816\n",
      "Epoch 75: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3069 - accuracy: 0.8818 - val_loss: 0.7985 - val_accuracy: 0.7667\n",
      "Epoch 76/1000\n",
      "844/855 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8825\n",
      "Epoch 76: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3059 - accuracy: 0.8825 - val_loss: 0.7860 - val_accuracy: 0.7710\n",
      "Epoch 77/1000\n",
      "849/855 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8847\n",
      "Epoch 77: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3023 - accuracy: 0.8847 - val_loss: 0.7826 - val_accuracy: 0.7706\n",
      "Epoch 78/1000\n",
      "843/855 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8834\n",
      "Epoch 78: val_accuracy did not improve from 0.77478\n",
      "855/855 [==============================] - 4s 4ms/step - loss: 0.3045 - accuracy: 0.8833 - val_loss: 0.7920 - val_accuracy: 0.7684\n",
      "369/369 [==============================] - 0s 823us/step\n",
      "0.7635473079328323\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (109059, 256)             3584      \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (109059, 256)             0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (109059, 256)             65792     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (109059, 256)             0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (109059, 128)             32896     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (109059, 128)             0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (109059, 128)             16512     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (109059, 128)             0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (109059, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.8884 - accuracy: 0.6544\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69968, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.8875 - accuracy: 0.6547 - val_loss: 0.7676 - val_accuracy: 0.6997\n",
      "Epoch 2/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.7512 - accuracy: 0.7084\n",
      "Epoch 2: val_accuracy improved from 0.69968 to 0.70769, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.7509 - accuracy: 0.7086 - val_loss: 0.7504 - val_accuracy: 0.7077\n",
      "Epoch 3/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.7332\n",
      "Epoch 3: val_accuracy improved from 0.70769 to 0.72429, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.6871 - accuracy: 0.7332 - val_loss: 0.7029 - val_accuracy: 0.7243\n",
      "Epoch 4/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.6495 - accuracy: 0.7493\n",
      "Epoch 4: val_accuracy improved from 0.72429 to 0.74205, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.6494 - accuracy: 0.7493 - val_loss: 0.6646 - val_accuracy: 0.7421\n",
      "Epoch 5/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.6169 - accuracy: 0.7607\n",
      "Epoch 5: val_accuracy did not improve from 0.74205\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.6167 - accuracy: 0.7608 - val_loss: 0.6726 - val_accuracy: 0.7354\n",
      "Epoch 6/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.5915 - accuracy: 0.7693\n",
      "Epoch 6: val_accuracy did not improve from 0.74205\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5912 - accuracy: 0.7695 - val_loss: 0.6748 - val_accuracy: 0.7396\n",
      "Epoch 7/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.7797\n",
      "Epoch 7: val_accuracy improved from 0.74205 to 0.74403, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5716 - accuracy: 0.7797 - val_loss: 0.6685 - val_accuracy: 0.7440\n",
      "Epoch 8/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.5532 - accuracy: 0.7853\n",
      "Epoch 8: val_accuracy improved from 0.74403 to 0.75056, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5534 - accuracy: 0.7852 - val_loss: 0.6609 - val_accuracy: 0.7506\n",
      "Epoch 9/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.5363 - accuracy: 0.7914\n",
      "Epoch 9: val_accuracy improved from 0.75056 to 0.75626, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5362 - accuracy: 0.7915 - val_loss: 0.6633 - val_accuracy: 0.7563\n",
      "Epoch 10/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.5223 - accuracy: 0.7983\n",
      "Epoch 10: val_accuracy did not improve from 0.75626\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.5223 - accuracy: 0.7982 - val_loss: 0.6726 - val_accuracy: 0.7511\n",
      "Epoch 11/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.5091 - accuracy: 0.8024\n",
      "Epoch 11: val_accuracy did not improve from 0.75626\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5091 - accuracy: 0.8024 - val_loss: 0.6833 - val_accuracy: 0.7510\n",
      "Epoch 12/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.8072\n",
      "Epoch 12: val_accuracy did not improve from 0.75626\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4966 - accuracy: 0.8072 - val_loss: 0.6796 - val_accuracy: 0.7502\n",
      "Epoch 13/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.8106\n",
      "Epoch 13: val_accuracy did not improve from 0.75626\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4886 - accuracy: 0.8106 - val_loss: 0.6882 - val_accuracy: 0.7538\n",
      "Epoch 14/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.8161\n",
      "Epoch 14: val_accuracy improved from 0.75626 to 0.76022, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4756 - accuracy: 0.8160 - val_loss: 0.6856 - val_accuracy: 0.7602\n",
      "Epoch 15/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.4701 - accuracy: 0.8165\n",
      "Epoch 15: val_accuracy did not improve from 0.76022\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4700 - accuracy: 0.8167 - val_loss: 0.7010 - val_accuracy: 0.7511\n",
      "Epoch 16/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.4597 - accuracy: 0.8209\n",
      "Epoch 16: val_accuracy did not improve from 0.76022\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4594 - accuracy: 0.8210 - val_loss: 0.6984 - val_accuracy: 0.7542\n",
      "Epoch 17/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4506 - accuracy: 0.8255\n",
      "Epoch 17: val_accuracy did not improve from 0.76022\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4507 - accuracy: 0.8255 - val_loss: 0.6976 - val_accuracy: 0.7593\n",
      "Epoch 18/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.4436 - accuracy: 0.8272\n",
      "Epoch 18: val_accuracy improved from 0.76022 to 0.76179, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4437 - accuracy: 0.8271 - val_loss: 0.6981 - val_accuracy: 0.7618\n",
      "Epoch 19/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.4393 - accuracy: 0.8293\n",
      "Epoch 19: val_accuracy did not improve from 0.76179\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4394 - accuracy: 0.8293 - val_loss: 0.6863 - val_accuracy: 0.7591\n",
      "Epoch 20/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.4335 - accuracy: 0.8306\n",
      "Epoch 20: val_accuracy improved from 0.76179 to 0.76295, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4334 - accuracy: 0.8306 - val_loss: 0.7048 - val_accuracy: 0.7629\n",
      "Epoch 21/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8353\n",
      "Epoch 21: val_accuracy did not improve from 0.76295\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4222 - accuracy: 0.8353 - val_loss: 0.6949 - val_accuracy: 0.7610\n",
      "Epoch 22/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8362\n",
      "Epoch 22: val_accuracy did not improve from 0.76295\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4191 - accuracy: 0.8362 - val_loss: 0.7218 - val_accuracy: 0.7553\n",
      "Epoch 23/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.8412\n",
      "Epoch 23: val_accuracy did not improve from 0.76295\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4114 - accuracy: 0.8412 - val_loss: 0.7320 - val_accuracy: 0.7563\n",
      "Epoch 24/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.8405\n",
      "Epoch 24: val_accuracy did not improve from 0.76295\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4084 - accuracy: 0.8405 - val_loss: 0.7145 - val_accuracy: 0.7591\n",
      "Epoch 25/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.4055 - accuracy: 0.8422\n",
      "Epoch 25: val_accuracy improved from 0.76295 to 0.76311, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4056 - accuracy: 0.8421 - val_loss: 0.7257 - val_accuracy: 0.7631\n",
      "Epoch 26/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8453\n",
      "Epoch 26: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3988 - accuracy: 0.8454 - val_loss: 0.7367 - val_accuracy: 0.7614\n",
      "Epoch 27/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.8472\n",
      "Epoch 27: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3929 - accuracy: 0.8473 - val_loss: 0.7378 - val_accuracy: 0.7601\n",
      "Epoch 28/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3912 - accuracy: 0.8485\n",
      "Epoch 28: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3912 - accuracy: 0.8485 - val_loss: 0.7421 - val_accuracy: 0.7563\n",
      "Epoch 29/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3868 - accuracy: 0.8504\n",
      "Epoch 29: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3866 - accuracy: 0.8505 - val_loss: 0.7459 - val_accuracy: 0.7629\n",
      "Epoch 30/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3807 - accuracy: 0.8520\n",
      "Epoch 30: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3807 - accuracy: 0.8520 - val_loss: 0.7689 - val_accuracy: 0.7587\n",
      "Epoch 31/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.8531\n",
      "Epoch 31: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3785 - accuracy: 0.8531 - val_loss: 0.7375 - val_accuracy: 0.7615\n",
      "Epoch 32/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8557\n",
      "Epoch 32: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3768 - accuracy: 0.8556 - val_loss: 0.7402 - val_accuracy: 0.7616\n",
      "Epoch 33/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8558\n",
      "Epoch 33: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3729 - accuracy: 0.8557 - val_loss: 0.7512 - val_accuracy: 0.7609\n",
      "Epoch 34/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8565\n",
      "Epoch 34: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3697 - accuracy: 0.8565 - val_loss: 0.7679 - val_accuracy: 0.7553\n",
      "Epoch 35/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8589\n",
      "Epoch 35: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3649 - accuracy: 0.8589 - val_loss: 0.7586 - val_accuracy: 0.7582\n",
      "Epoch 36/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8596\n",
      "Epoch 36: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3636 - accuracy: 0.8595 - val_loss: 0.7561 - val_accuracy: 0.7577\n",
      "Epoch 37/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3634 - accuracy: 0.8599\n",
      "Epoch 37: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3634 - accuracy: 0.8599 - val_loss: 0.7687 - val_accuracy: 0.7596\n",
      "Epoch 38/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8612\n",
      "Epoch 38: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3605 - accuracy: 0.8611 - val_loss: 0.7631 - val_accuracy: 0.7599\n",
      "Epoch 39/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.8629\n",
      "Epoch 39: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3533 - accuracy: 0.8630 - val_loss: 0.8031 - val_accuracy: 0.7550\n",
      "Epoch 40/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3531 - accuracy: 0.8628\n",
      "Epoch 40: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3531 - accuracy: 0.8628 - val_loss: 0.7858 - val_accuracy: 0.7613\n",
      "Epoch 41/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8612\n",
      "Epoch 41: val_accuracy did not improve from 0.76311\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3575 - accuracy: 0.8613 - val_loss: 0.7896 - val_accuracy: 0.7596\n",
      "Epoch 42/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3485 - accuracy: 0.8655\n",
      "Epoch 42: val_accuracy improved from 0.76311 to 0.76394, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3488 - accuracy: 0.8654 - val_loss: 0.7689 - val_accuracy: 0.7639\n",
      "Epoch 43/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3476 - accuracy: 0.8659\n",
      "Epoch 43: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3480 - accuracy: 0.8657 - val_loss: 0.7798 - val_accuracy: 0.7600\n",
      "Epoch 44/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.3453 - accuracy: 0.8670\n",
      "Epoch 44: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3456 - accuracy: 0.8670 - val_loss: 0.7805 - val_accuracy: 0.7606\n",
      "Epoch 45/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.8691\n",
      "Epoch 45: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3406 - accuracy: 0.8691 - val_loss: 0.7945 - val_accuracy: 0.7557\n",
      "Epoch 46/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3405 - accuracy: 0.8684\n",
      "Epoch 46: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3404 - accuracy: 0.8684 - val_loss: 0.7833 - val_accuracy: 0.7601\n",
      "Epoch 47/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.8700\n",
      "Epoch 47: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3382 - accuracy: 0.8700 - val_loss: 0.7830 - val_accuracy: 0.7631\n",
      "Epoch 48/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.8700\n",
      "Epoch 48: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3377 - accuracy: 0.8700 - val_loss: 0.7881 - val_accuracy: 0.7634\n",
      "Epoch 49/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8710\n",
      "Epoch 49: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3350 - accuracy: 0.8708 - val_loss: 0.7703 - val_accuracy: 0.7607\n",
      "Epoch 50/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8708\n",
      "Epoch 50: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3355 - accuracy: 0.8707 - val_loss: 0.7667 - val_accuracy: 0.7606\n",
      "Epoch 51/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.8713\n",
      "Epoch 51: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3323 - accuracy: 0.8713 - val_loss: 0.7997 - val_accuracy: 0.7633\n",
      "Epoch 52/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.8733\n",
      "Epoch 52: val_accuracy did not improve from 0.76394\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3287 - accuracy: 0.8732 - val_loss: 0.7932 - val_accuracy: 0.7582\n",
      "Epoch 53/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3285 - accuracy: 0.8719\n",
      "Epoch 53: val_accuracy improved from 0.76394 to 0.76526, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3286 - accuracy: 0.8718 - val_loss: 0.7965 - val_accuracy: 0.7653\n",
      "Epoch 54/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8725\n",
      "Epoch 54: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3302 - accuracy: 0.8725 - val_loss: 0.8042 - val_accuracy: 0.7568\n",
      "Epoch 55/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8742\n",
      "Epoch 55: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3254 - accuracy: 0.8743 - val_loss: 0.8017 - val_accuracy: 0.7651\n",
      "Epoch 56/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8768\n",
      "Epoch 56: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3213 - accuracy: 0.8769 - val_loss: 0.8153 - val_accuracy: 0.7570\n",
      "Epoch 57/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.8751\n",
      "Epoch 57: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3236 - accuracy: 0.8752 - val_loss: 0.7966 - val_accuracy: 0.7609\n",
      "Epoch 58/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.8766\n",
      "Epoch 58: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3203 - accuracy: 0.8766 - val_loss: 0.7995 - val_accuracy: 0.7547\n",
      "Epoch 59/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8756\n",
      "Epoch 59: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3223 - accuracy: 0.8757 - val_loss: 0.8167 - val_accuracy: 0.7618\n",
      "Epoch 60/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.8789\n",
      "Epoch 60: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3177 - accuracy: 0.8789 - val_loss: 0.7928 - val_accuracy: 0.7629\n",
      "Epoch 61/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8775\n",
      "Epoch 61: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3199 - accuracy: 0.8774 - val_loss: 0.7939 - val_accuracy: 0.7648\n",
      "Epoch 62/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8759\n",
      "Epoch 62: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3205 - accuracy: 0.8759 - val_loss: 0.8153 - val_accuracy: 0.7587\n",
      "Epoch 63/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3116 - accuracy: 0.8804\n",
      "Epoch 63: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3118 - accuracy: 0.8804 - val_loss: 0.8050 - val_accuracy: 0.7591\n",
      "Epoch 64/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3149 - accuracy: 0.8789\n",
      "Epoch 64: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3148 - accuracy: 0.8790 - val_loss: 0.8221 - val_accuracy: 0.7605\n",
      "Epoch 65/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8799\n",
      "Epoch 65: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3116 - accuracy: 0.8799 - val_loss: 0.8221 - val_accuracy: 0.7612\n",
      "Epoch 66/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.8789\n",
      "Epoch 66: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3155 - accuracy: 0.8788 - val_loss: 0.7997 - val_accuracy: 0.7601\n",
      "Epoch 67/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8790\n",
      "Epoch 67: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3115 - accuracy: 0.8790 - val_loss: 0.8080 - val_accuracy: 0.7626\n",
      "Epoch 68/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8817\n",
      "Epoch 68: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3084 - accuracy: 0.8817 - val_loss: 0.8316 - val_accuracy: 0.7585\n",
      "Epoch 69/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.8806\n",
      "Epoch 69: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3090 - accuracy: 0.8806 - val_loss: 0.8491 - val_accuracy: 0.7570\n",
      "Epoch 70/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8804\n",
      "Epoch 70: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3075 - accuracy: 0.8804 - val_loss: 0.8066 - val_accuracy: 0.7610\n",
      "Epoch 71/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8824\n",
      "Epoch 71: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3048 - accuracy: 0.8822 - val_loss: 0.8127 - val_accuracy: 0.7589\n",
      "Epoch 72/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8837\n",
      "Epoch 72: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3014 - accuracy: 0.8836 - val_loss: 0.8376 - val_accuracy: 0.7590\n",
      "Epoch 73/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8841\n",
      "Epoch 73: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3032 - accuracy: 0.8841 - val_loss: 0.8362 - val_accuracy: 0.7607\n",
      "Epoch 74/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8831\n",
      "Epoch 74: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3040 - accuracy: 0.8829 - val_loss: 0.8311 - val_accuracy: 0.7607\n",
      "Epoch 75/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8831\n",
      "Epoch 75: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3046 - accuracy: 0.8831 - val_loss: 0.8261 - val_accuracy: 0.7609\n",
      "Epoch 76/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.8856\n",
      "Epoch 76: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3017 - accuracy: 0.8856 - val_loss: 0.8276 - val_accuracy: 0.7578\n",
      "Epoch 77/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8844\n",
      "Epoch 77: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3005 - accuracy: 0.8845 - val_loss: 0.8424 - val_accuracy: 0.7610\n",
      "Epoch 78/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8843\n",
      "Epoch 78: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3004 - accuracy: 0.8842 - val_loss: 0.8305 - val_accuracy: 0.7583\n",
      "Epoch 79/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8862\n",
      "Epoch 79: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2979 - accuracy: 0.8861 - val_loss: 0.8360 - val_accuracy: 0.7630\n",
      "Epoch 80/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8868\n",
      "Epoch 80: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2952 - accuracy: 0.8868 - val_loss: 0.8358 - val_accuracy: 0.7619\n",
      "Epoch 81/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8868\n",
      "Epoch 81: val_accuracy did not improve from 0.76526\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2972 - accuracy: 0.8868 - val_loss: 0.8362 - val_accuracy: 0.7577\n",
      "Epoch 82/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8843\n",
      "Epoch 82: val_accuracy improved from 0.76526 to 0.76633, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3033 - accuracy: 0.8842 - val_loss: 0.8206 - val_accuracy: 0.7663\n",
      "Epoch 83/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8876\n",
      "Epoch 83: val_accuracy did not improve from 0.76633\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2925 - accuracy: 0.8877 - val_loss: 0.8375 - val_accuracy: 0.7601\n",
      "Epoch 84/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8884\n",
      "Epoch 84: val_accuracy improved from 0.76633 to 0.76716, saving model to best_weights_dense_5.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2926 - accuracy: 0.8884 - val_loss: 0.8331 - val_accuracy: 0.7672\n",
      "Epoch 85/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8879\n",
      "Epoch 85: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2925 - accuracy: 0.8879 - val_loss: 0.8490 - val_accuracy: 0.7610\n",
      "Epoch 86/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.8893\n",
      "Epoch 86: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2881 - accuracy: 0.8894 - val_loss: 0.8469 - val_accuracy: 0.7644\n",
      "Epoch 87/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8893\n",
      "Epoch 87: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2912 - accuracy: 0.8892 - val_loss: 0.8605 - val_accuracy: 0.7583\n",
      "Epoch 88/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8852\n",
      "Epoch 88: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2984 - accuracy: 0.8851 - val_loss: 0.8229 - val_accuracy: 0.7611\n",
      "Epoch 89/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.8903\n",
      "Epoch 89: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2835 - accuracy: 0.8903 - val_loss: 0.8482 - val_accuracy: 0.7666\n",
      "Epoch 90/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.8876\n",
      "Epoch 90: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2909 - accuracy: 0.8876 - val_loss: 0.8595 - val_accuracy: 0.7614\n",
      "Epoch 91/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.8904\n",
      "Epoch 91: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2877 - accuracy: 0.8904 - val_loss: 0.8377 - val_accuracy: 0.7629\n",
      "Epoch 92/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.8912\n",
      "Epoch 92: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2852 - accuracy: 0.8913 - val_loss: 0.8717 - val_accuracy: 0.7553\n",
      "Epoch 93/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8905\n",
      "Epoch 93: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2853 - accuracy: 0.8906 - val_loss: 0.8928 - val_accuracy: 0.7568\n",
      "Epoch 94/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.8918\n",
      "Epoch 94: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2844 - accuracy: 0.8917 - val_loss: 0.8751 - val_accuracy: 0.7587\n",
      "Epoch 95/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.8905\n",
      "Epoch 95: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2875 - accuracy: 0.8905 - val_loss: 0.8699 - val_accuracy: 0.7577\n",
      "Epoch 96/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8920\n",
      "Epoch 96: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2859 - accuracy: 0.8919 - val_loss: 0.8445 - val_accuracy: 0.7629\n",
      "Epoch 97/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2824 - accuracy: 0.8931\n",
      "Epoch 97: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2823 - accuracy: 0.8931 - val_loss: 0.8503 - val_accuracy: 0.7563\n",
      "Epoch 98/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8902\n",
      "Epoch 98: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2856 - accuracy: 0.8902 - val_loss: 0.8710 - val_accuracy: 0.7596\n",
      "Epoch 99/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8930\n",
      "Epoch 99: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2786 - accuracy: 0.8930 - val_loss: 0.8982 - val_accuracy: 0.7539\n",
      "Epoch 100/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.2813 - accuracy: 0.8917\n",
      "Epoch 100: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2811 - accuracy: 0.8918 - val_loss: 0.8623 - val_accuracy: 0.7647\n",
      "Epoch 101/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8928\n",
      "Epoch 101: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2811 - accuracy: 0.8929 - val_loss: 0.8974 - val_accuracy: 0.7555\n",
      "Epoch 102/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8917\n",
      "Epoch 102: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2828 - accuracy: 0.8917 - val_loss: 0.8793 - val_accuracy: 0.7555\n",
      "Epoch 103/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.2814 - accuracy: 0.8935\n",
      "Epoch 103: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2814 - accuracy: 0.8935 - val_loss: 0.8589 - val_accuracy: 0.7601\n",
      "Epoch 104/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.8936\n",
      "Epoch 104: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2806 - accuracy: 0.8936 - val_loss: 0.8443 - val_accuracy: 0.7603\n",
      "Epoch 105/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.8942\n",
      "Epoch 105: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2792 - accuracy: 0.8942 - val_loss: 0.8740 - val_accuracy: 0.7532\n",
      "Epoch 106/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.8934\n",
      "Epoch 106: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2779 - accuracy: 0.8935 - val_loss: 0.8613 - val_accuracy: 0.7587\n",
      "Epoch 107/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.8940\n",
      "Epoch 107: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2793 - accuracy: 0.8940 - val_loss: 0.8764 - val_accuracy: 0.7602\n",
      "Epoch 108/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8950\n",
      "Epoch 108: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2786 - accuracy: 0.8949 - val_loss: 0.8647 - val_accuracy: 0.7616\n",
      "Epoch 109/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.2724 - accuracy: 0.8954\n",
      "Epoch 109: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2723 - accuracy: 0.8954 - val_loss: 0.8687 - val_accuracy: 0.7583\n",
      "Epoch 110/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.8943\n",
      "Epoch 110: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2789 - accuracy: 0.8943 - val_loss: 0.8643 - val_accuracy: 0.7645\n",
      "Epoch 111/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.8968\n",
      "Epoch 111: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2722 - accuracy: 0.8967 - val_loss: 0.8882 - val_accuracy: 0.7601\n",
      "Epoch 112/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2757 - accuracy: 0.8952\n",
      "Epoch 112: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2756 - accuracy: 0.8952 - val_loss: 0.8697 - val_accuracy: 0.7563\n",
      "Epoch 113/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8961\n",
      "Epoch 113: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2737 - accuracy: 0.8961 - val_loss: 0.8714 - val_accuracy: 0.7587\n",
      "Epoch 114/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.8957\n",
      "Epoch 114: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2727 - accuracy: 0.8957 - val_loss: 0.8628 - val_accuracy: 0.7575\n",
      "Epoch 115/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.8966\n",
      "Epoch 115: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2727 - accuracy: 0.8966 - val_loss: 0.8715 - val_accuracy: 0.7593\n",
      "Epoch 116/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.8964\n",
      "Epoch 116: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2723 - accuracy: 0.8964 - val_loss: 0.8400 - val_accuracy: 0.7639\n",
      "Epoch 117/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8977\n",
      "Epoch 117: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2696 - accuracy: 0.8977 - val_loss: 0.8689 - val_accuracy: 0.7615\n",
      "Epoch 118/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2743 - accuracy: 0.8956\n",
      "Epoch 118: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2741 - accuracy: 0.8956 - val_loss: 0.8692 - val_accuracy: 0.7623\n",
      "Epoch 119/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2706 - accuracy: 0.8970\n",
      "Epoch 119: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2704 - accuracy: 0.8971 - val_loss: 0.8374 - val_accuracy: 0.7617\n",
      "Epoch 120/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.8976\n",
      "Epoch 120: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2707 - accuracy: 0.8977 - val_loss: 0.8776 - val_accuracy: 0.7607\n",
      "Epoch 121/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.8976\n",
      "Epoch 121: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2682 - accuracy: 0.8975 - val_loss: 0.8662 - val_accuracy: 0.7610\n",
      "Epoch 122/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2679 - accuracy: 0.8981\n",
      "Epoch 122: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2677 - accuracy: 0.8982 - val_loss: 0.8622 - val_accuracy: 0.7601\n",
      "Epoch 123/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.8974\n",
      "Epoch 123: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2717 - accuracy: 0.8975 - val_loss: 0.8835 - val_accuracy: 0.7599\n",
      "Epoch 124/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2703 - accuracy: 0.8972\n",
      "Epoch 124: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2702 - accuracy: 0.8972 - val_loss: 0.8853 - val_accuracy: 0.7594\n",
      "Epoch 125/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2674 - accuracy: 0.8980\n",
      "Epoch 125: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2677 - accuracy: 0.8980 - val_loss: 0.8580 - val_accuracy: 0.7595\n",
      "Epoch 126/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.8977\n",
      "Epoch 126: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2693 - accuracy: 0.8976 - val_loss: 0.8889 - val_accuracy: 0.7617\n",
      "Epoch 127/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.8978\n",
      "Epoch 127: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2688 - accuracy: 0.8976 - val_loss: 0.8660 - val_accuracy: 0.7556\n",
      "Epoch 128/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2654 - accuracy: 0.8999\n",
      "Epoch 128: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.2655 - accuracy: 0.8999 - val_loss: 0.8814 - val_accuracy: 0.7540\n",
      "Epoch 129/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2664 - accuracy: 0.8987\n",
      "Epoch 129: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2663 - accuracy: 0.8987 - val_loss: 0.8769 - val_accuracy: 0.7628\n",
      "Epoch 130/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.8990\n",
      "Epoch 130: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2661 - accuracy: 0.8989 - val_loss: 0.8683 - val_accuracy: 0.7588\n",
      "Epoch 131/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.8983\n",
      "Epoch 131: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2662 - accuracy: 0.8984 - val_loss: 0.8729 - val_accuracy: 0.7607\n",
      "Epoch 132/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8998\n",
      "Epoch 132: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2638 - accuracy: 0.8998 - val_loss: 0.8727 - val_accuracy: 0.7607\n",
      "Epoch 133/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8991\n",
      "Epoch 133: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2637 - accuracy: 0.8991 - val_loss: 0.8761 - val_accuracy: 0.7560\n",
      "Epoch 134/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.2651 - accuracy: 0.8997\n",
      "Epoch 134: val_accuracy did not improve from 0.76716\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2651 - accuracy: 0.8997 - val_loss: 0.8505 - val_accuracy: 0.7596\n",
      "379/379 [==============================] - 1s 887us/step\n",
      "0.7467141194459728\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_35 (Dense)            (109072, 256)             3584      \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (109072, 256)             0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (109072, 256)             65792     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (109072, 256)             0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (109072, 128)             32896     \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (109072, 128)             0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (109072, 128)             16512     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (109072, 128)             0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (109072, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.8923 - accuracy: 0.6521\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69489, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.8919 - accuracy: 0.6523 - val_loss: 0.7799 - val_accuracy: 0.6949\n",
      "Epoch 2/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.7451 - accuracy: 0.7094\n",
      "Epoch 2: val_accuracy improved from 0.69489 to 0.72672, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.7450 - accuracy: 0.7094 - val_loss: 0.7074 - val_accuracy: 0.7267\n",
      "Epoch 3/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.6886 - accuracy: 0.7329\n",
      "Epoch 3: val_accuracy improved from 0.72672 to 0.73987, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.6880 - accuracy: 0.7330 - val_loss: 0.6702 - val_accuracy: 0.7399\n",
      "Epoch 4/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.6468 - accuracy: 0.7503\n",
      "Epoch 4: val_accuracy improved from 0.73987 to 0.74425, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.6467 - accuracy: 0.7503 - val_loss: 0.6673 - val_accuracy: 0.7443\n",
      "Epoch 5/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.6156 - accuracy: 0.7622\n",
      "Epoch 5: val_accuracy did not improve from 0.74425\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.6156 - accuracy: 0.7622 - val_loss: 0.6681 - val_accuracy: 0.7424\n",
      "Epoch 6/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.5901 - accuracy: 0.7717\n",
      "Epoch 6: val_accuracy improved from 0.74425 to 0.74864, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5900 - accuracy: 0.7718 - val_loss: 0.6716 - val_accuracy: 0.7486\n",
      "Epoch 7/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.5678 - accuracy: 0.7807\n",
      "Epoch 7: val_accuracy improved from 0.74864 to 0.75732, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.5677 - accuracy: 0.7808 - val_loss: 0.6493 - val_accuracy: 0.7573\n",
      "Epoch 8/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.5502 - accuracy: 0.7863\n",
      "Epoch 8: val_accuracy did not improve from 0.75732\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.5501 - accuracy: 0.7863 - val_loss: 0.6530 - val_accuracy: 0.7544\n",
      "Epoch 9/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.5301 - accuracy: 0.7939\n",
      "Epoch 9: val_accuracy did not improve from 0.75732\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5309 - accuracy: 0.7934 - val_loss: 0.6446 - val_accuracy: 0.7571\n",
      "Epoch 10/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.5180 - accuracy: 0.7991\n",
      "Epoch 10: val_accuracy did not improve from 0.75732\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.5181 - accuracy: 0.7990 - val_loss: 0.6527 - val_accuracy: 0.7488\n",
      "Epoch 11/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.5066 - accuracy: 0.8038\n",
      "Epoch 11: val_accuracy did not improve from 0.75732\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5069 - accuracy: 0.8037 - val_loss: 0.6743 - val_accuracy: 0.7529\n",
      "Epoch 12/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.4905 - accuracy: 0.8088\n",
      "Epoch 12: val_accuracy did not improve from 0.75732\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4903 - accuracy: 0.8090 - val_loss: 0.6708 - val_accuracy: 0.7567\n",
      "Epoch 13/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.4831 - accuracy: 0.8135\n",
      "Epoch 13: val_accuracy improved from 0.75732 to 0.76203, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4834 - accuracy: 0.8133 - val_loss: 0.6577 - val_accuracy: 0.7620\n",
      "Epoch 14/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.4751 - accuracy: 0.8149\n",
      "Epoch 14: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4753 - accuracy: 0.8147 - val_loss: 0.6565 - val_accuracy: 0.7538\n",
      "Epoch 15/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.4656 - accuracy: 0.8189\n",
      "Epoch 15: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4658 - accuracy: 0.8189 - val_loss: 0.6828 - val_accuracy: 0.7527\n",
      "Epoch 16/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.4548 - accuracy: 0.8232\n",
      "Epoch 16: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4547 - accuracy: 0.8231 - val_loss: 0.6824 - val_accuracy: 0.7568\n",
      "Epoch 17/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.4469 - accuracy: 0.8265\n",
      "Epoch 17: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4469 - accuracy: 0.8265 - val_loss: 0.6809 - val_accuracy: 0.7591\n",
      "Epoch 18/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.4424 - accuracy: 0.8283\n",
      "Epoch 18: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4421 - accuracy: 0.8283 - val_loss: 0.6964 - val_accuracy: 0.7538\n",
      "Epoch 19/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.4350 - accuracy: 0.8310\n",
      "Epoch 19: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4350 - accuracy: 0.8310 - val_loss: 0.7039 - val_accuracy: 0.7496\n",
      "Epoch 20/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.4278 - accuracy: 0.8337\n",
      "Epoch 20: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4278 - accuracy: 0.8337 - val_loss: 0.6859 - val_accuracy: 0.7541\n",
      "Epoch 21/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.4214 - accuracy: 0.8355\n",
      "Epoch 21: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4210 - accuracy: 0.8356 - val_loss: 0.7114 - val_accuracy: 0.7519\n",
      "Epoch 22/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.4193 - accuracy: 0.8370\n",
      "Epoch 22: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4193 - accuracy: 0.8370 - val_loss: 0.6913 - val_accuracy: 0.7560\n",
      "Epoch 23/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8408\n",
      "Epoch 23: val_accuracy did not improve from 0.76203\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4109 - accuracy: 0.8408 - val_loss: 0.6943 - val_accuracy: 0.7562\n",
      "Epoch 24/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8425\n",
      "Epoch 24: val_accuracy improved from 0.76203 to 0.76410, saving model to best_weights_dense_6.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4037 - accuracy: 0.8426 - val_loss: 0.7001 - val_accuracy: 0.7641\n",
      "Epoch 25/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.8440\n",
      "Epoch 25: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4030 - accuracy: 0.8441 - val_loss: 0.7009 - val_accuracy: 0.7573\n",
      "Epoch 26/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3966 - accuracy: 0.8455\n",
      "Epoch 26: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3962 - accuracy: 0.8457 - val_loss: 0.7287 - val_accuracy: 0.7550\n",
      "Epoch 27/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3936 - accuracy: 0.8477\n",
      "Epoch 27: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3934 - accuracy: 0.8479 - val_loss: 0.7181 - val_accuracy: 0.7551\n",
      "Epoch 28/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3865 - accuracy: 0.8495\n",
      "Epoch 28: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3868 - accuracy: 0.8493 - val_loss: 0.7510 - val_accuracy: 0.7476\n",
      "Epoch 29/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3845 - accuracy: 0.8506\n",
      "Epoch 29: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3845 - accuracy: 0.8506 - val_loss: 0.7044 - val_accuracy: 0.7592\n",
      "Epoch 30/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.8529\n",
      "Epoch 30: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3786 - accuracy: 0.8528 - val_loss: 0.7246 - val_accuracy: 0.7582\n",
      "Epoch 31/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3771 - accuracy: 0.8545\n",
      "Epoch 31: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3767 - accuracy: 0.8546 - val_loss: 0.7292 - val_accuracy: 0.7624\n",
      "Epoch 32/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8553\n",
      "Epoch 32: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3745 - accuracy: 0.8554 - val_loss: 0.7263 - val_accuracy: 0.7582\n",
      "Epoch 33/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.8568\n",
      "Epoch 33: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3708 - accuracy: 0.8568 - val_loss: 0.7353 - val_accuracy: 0.7577\n",
      "Epoch 34/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3684 - accuracy: 0.8576\n",
      "Epoch 34: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3683 - accuracy: 0.8577 - val_loss: 0.7452 - val_accuracy: 0.7535\n",
      "Epoch 35/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8585\n",
      "Epoch 35: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3679 - accuracy: 0.8585 - val_loss: 0.7451 - val_accuracy: 0.7573\n",
      "Epoch 36/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8601\n",
      "Epoch 36: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3625 - accuracy: 0.8601 - val_loss: 0.7572 - val_accuracy: 0.7502\n",
      "Epoch 37/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3596 - accuracy: 0.8621\n",
      "Epoch 37: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3596 - accuracy: 0.8621 - val_loss: 0.7717 - val_accuracy: 0.7547\n",
      "Epoch 38/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8606\n",
      "Epoch 38: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3583 - accuracy: 0.8606 - val_loss: 0.7696 - val_accuracy: 0.7498\n",
      "Epoch 39/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8619\n",
      "Epoch 39: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3568 - accuracy: 0.8619 - val_loss: 0.7472 - val_accuracy: 0.7533\n",
      "Epoch 40/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.8628\n",
      "Epoch 40: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3541 - accuracy: 0.8628 - val_loss: 0.7576 - val_accuracy: 0.7540\n",
      "Epoch 41/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3523 - accuracy: 0.8637\n",
      "Epoch 41: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3523 - accuracy: 0.8637 - val_loss: 0.7702 - val_accuracy: 0.7498\n",
      "Epoch 42/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8649\n",
      "Epoch 42: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3475 - accuracy: 0.8648 - val_loss: 0.7524 - val_accuracy: 0.7529\n",
      "Epoch 43/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8655\n",
      "Epoch 43: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3477 - accuracy: 0.8655 - val_loss: 0.7561 - val_accuracy: 0.7619\n",
      "Epoch 44/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8673\n",
      "Epoch 44: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3434 - accuracy: 0.8672 - val_loss: 0.7564 - val_accuracy: 0.7594\n",
      "Epoch 45/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8685\n",
      "Epoch 45: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3416 - accuracy: 0.8686 - val_loss: 0.7772 - val_accuracy: 0.7591\n",
      "Epoch 46/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8684\n",
      "Epoch 46: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3411 - accuracy: 0.8682 - val_loss: 0.7735 - val_accuracy: 0.7519\n",
      "Epoch 47/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3381 - accuracy: 0.8703\n",
      "Epoch 47: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3380 - accuracy: 0.8705 - val_loss: 0.7868 - val_accuracy: 0.7510\n",
      "Epoch 48/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3360 - accuracy: 0.8705\n",
      "Epoch 48: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3362 - accuracy: 0.8704 - val_loss: 0.7742 - val_accuracy: 0.7526\n",
      "Epoch 49/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3321 - accuracy: 0.8717\n",
      "Epoch 49: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3324 - accuracy: 0.8717 - val_loss: 0.7832 - val_accuracy: 0.7562\n",
      "Epoch 50/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8729\n",
      "Epoch 50: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3316 - accuracy: 0.8730 - val_loss: 0.7938 - val_accuracy: 0.7504\n",
      "Epoch 51/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8710\n",
      "Epoch 51: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3320 - accuracy: 0.8710 - val_loss: 0.7869 - val_accuracy: 0.7502\n",
      "Epoch 52/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.8728\n",
      "Epoch 52: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3291 - accuracy: 0.8728 - val_loss: 0.7872 - val_accuracy: 0.7557\n",
      "Epoch 53/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.8747\n",
      "Epoch 53: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3258 - accuracy: 0.8747 - val_loss: 0.7771 - val_accuracy: 0.7557\n",
      "Epoch 54/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8724\n",
      "Epoch 54: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3293 - accuracy: 0.8725 - val_loss: 0.7954 - val_accuracy: 0.7553\n",
      "Epoch 55/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8764\n",
      "Epoch 55: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3221 - accuracy: 0.8765 - val_loss: 0.7957 - val_accuracy: 0.7563\n",
      "Epoch 56/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3255 - accuracy: 0.8741\n",
      "Epoch 56: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3256 - accuracy: 0.8740 - val_loss: 0.7940 - val_accuracy: 0.7484\n",
      "Epoch 57/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3248 - accuracy: 0.8758\n",
      "Epoch 57: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3246 - accuracy: 0.8759 - val_loss: 0.7830 - val_accuracy: 0.7527\n",
      "Epoch 58/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3194 - accuracy: 0.8758\n",
      "Epoch 58: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3193 - accuracy: 0.8758 - val_loss: 0.7807 - val_accuracy: 0.7558\n",
      "Epoch 59/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.8779\n",
      "Epoch 59: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3176 - accuracy: 0.8779 - val_loss: 0.7872 - val_accuracy: 0.7570\n",
      "Epoch 60/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.8792\n",
      "Epoch 60: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3176 - accuracy: 0.8792 - val_loss: 0.8086 - val_accuracy: 0.7511\n",
      "Epoch 61/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.8789\n",
      "Epoch 61: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3169 - accuracy: 0.8789 - val_loss: 0.8044 - val_accuracy: 0.7502\n",
      "Epoch 62/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.8788\n",
      "Epoch 62: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3134 - accuracy: 0.8788 - val_loss: 0.8215 - val_accuracy: 0.7522\n",
      "Epoch 63/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8794\n",
      "Epoch 63: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3153 - accuracy: 0.8793 - val_loss: 0.7982 - val_accuracy: 0.7559\n",
      "Epoch 64/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.8806\n",
      "Epoch 64: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3121 - accuracy: 0.8806 - val_loss: 0.8150 - val_accuracy: 0.7505\n",
      "Epoch 65/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.3102 - accuracy: 0.8808\n",
      "Epoch 65: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3104 - accuracy: 0.8806 - val_loss: 0.8075 - val_accuracy: 0.7566\n",
      "Epoch 66/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.8798\n",
      "Epoch 66: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3121 - accuracy: 0.8798 - val_loss: 0.8153 - val_accuracy: 0.7560\n",
      "Epoch 67/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8810\n",
      "Epoch 67: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3081 - accuracy: 0.8811 - val_loss: 0.8222 - val_accuracy: 0.7554\n",
      "Epoch 68/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8820\n",
      "Epoch 68: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3084 - accuracy: 0.8819 - val_loss: 0.8172 - val_accuracy: 0.7512\n",
      "Epoch 69/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8834\n",
      "Epoch 69: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3060 - accuracy: 0.8834 - val_loss: 0.8340 - val_accuracy: 0.7545\n",
      "Epoch 70/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8830\n",
      "Epoch 70: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3043 - accuracy: 0.8830 - val_loss: 0.8124 - val_accuracy: 0.7615\n",
      "Epoch 71/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8829\n",
      "Epoch 71: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3053 - accuracy: 0.8829 - val_loss: 0.8258 - val_accuracy: 0.7508\n",
      "Epoch 72/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8834\n",
      "Epoch 72: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3023 - accuracy: 0.8835 - val_loss: 0.8232 - val_accuracy: 0.7535\n",
      "Epoch 73/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8836\n",
      "Epoch 73: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3026 - accuracy: 0.8836 - val_loss: 0.8382 - val_accuracy: 0.7490\n",
      "Epoch 74/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8836\n",
      "Epoch 74: val_accuracy did not improve from 0.76410\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3036 - accuracy: 0.8837 - val_loss: 0.8327 - val_accuracy: 0.7503\n",
      "378/378 [==============================] - 0s 844us/step\n",
      "0.7430555959745597\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (109245, 256)             3584      \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (109245, 256)             0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (109245, 256)             65792     \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (109245, 256)             0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (109245, 128)             32896     \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (109245, 128)             0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (109245, 128)             16512     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (109245, 128)             0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (109245, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.8865 - accuracy: 0.6560\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68971, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.8857 - accuracy: 0.6563 - val_loss: 0.7907 - val_accuracy: 0.6897\n",
      "Epoch 2/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.7415 - accuracy: 0.7121\n",
      "Epoch 2: val_accuracy improved from 0.68971 to 0.71227, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.7414 - accuracy: 0.7122 - val_loss: 0.7399 - val_accuracy: 0.7123\n",
      "Epoch 3/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.7363\n",
      "Epoch 3: val_accuracy improved from 0.71227 to 0.72427, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.6803 - accuracy: 0.7363 - val_loss: 0.7124 - val_accuracy: 0.7243\n",
      "Epoch 4/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.6427 - accuracy: 0.7500\n",
      "Epoch 4: val_accuracy did not improve from 0.72427\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.6425 - accuracy: 0.7502 - val_loss: 0.7151 - val_accuracy: 0.7170\n",
      "Epoch 5/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.7631\n",
      "Epoch 5: val_accuracy improved from 0.72427 to 0.73165, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.6115 - accuracy: 0.7631 - val_loss: 0.6944 - val_accuracy: 0.7317\n",
      "Epoch 6/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.5872 - accuracy: 0.7725\n",
      "Epoch 6: val_accuracy improved from 0.73165 to 0.73442, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.5873 - accuracy: 0.7725 - val_loss: 0.7129 - val_accuracy: 0.7344\n",
      "Epoch 7/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.5651 - accuracy: 0.7824\n",
      "Epoch 7: val_accuracy improved from 0.73442 to 0.73626, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.5653 - accuracy: 0.7823 - val_loss: 0.6857 - val_accuracy: 0.7363\n",
      "Epoch 8/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.5465 - accuracy: 0.7884\n",
      "Epoch 8: val_accuracy improved from 0.73626 to 0.74138, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.5465 - accuracy: 0.7884 - val_loss: 0.6807 - val_accuracy: 0.7414\n",
      "Epoch 9/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.5286 - accuracy: 0.7950\n",
      "Epoch 9: val_accuracy did not improve from 0.74138\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.5286 - accuracy: 0.7950 - val_loss: 0.6898 - val_accuracy: 0.7406\n",
      "Epoch 10/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.5165 - accuracy: 0.7999\n",
      "Epoch 10: val_accuracy improved from 0.74138 to 0.74599, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.5168 - accuracy: 0.7999 - val_loss: 0.6672 - val_accuracy: 0.7460\n",
      "Epoch 11/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.5006 - accuracy: 0.8074\n",
      "Epoch 11: val_accuracy improved from 0.74599 to 0.74734, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.5008 - accuracy: 0.8073 - val_loss: 0.6785 - val_accuracy: 0.7473\n",
      "Epoch 12/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.4926 - accuracy: 0.8099\n",
      "Epoch 12: val_accuracy did not improve from 0.74734\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4926 - accuracy: 0.8099 - val_loss: 0.6921 - val_accuracy: 0.7377\n",
      "Epoch 13/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.4815 - accuracy: 0.8131\n",
      "Epoch 13: val_accuracy did not improve from 0.74734\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4816 - accuracy: 0.8129 - val_loss: 0.7110 - val_accuracy: 0.7384\n",
      "Epoch 14/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.4723 - accuracy: 0.8182\n",
      "Epoch 14: val_accuracy improved from 0.74734 to 0.74876, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4722 - accuracy: 0.8183 - val_loss: 0.7063 - val_accuracy: 0.7488\n",
      "Epoch 15/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.4632 - accuracy: 0.8195\n",
      "Epoch 15: val_accuracy did not improve from 0.74876\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4632 - accuracy: 0.8195 - val_loss: 0.7258 - val_accuracy: 0.7384\n",
      "Epoch 16/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.4539 - accuracy: 0.8244\n",
      "Epoch 16: val_accuracy did not improve from 0.74876\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4540 - accuracy: 0.8243 - val_loss: 0.7236 - val_accuracy: 0.7437\n",
      "Epoch 17/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.8282\n",
      "Epoch 17: val_accuracy did not improve from 0.74876\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.4462 - accuracy: 0.8282 - val_loss: 0.7149 - val_accuracy: 0.7471\n",
      "Epoch 18/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.8301\n",
      "Epoch 18: val_accuracy improved from 0.74876 to 0.75229, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4387 - accuracy: 0.8301 - val_loss: 0.7160 - val_accuracy: 0.7523\n",
      "Epoch 19/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.4305 - accuracy: 0.8328\n",
      "Epoch 19: val_accuracy did not improve from 0.75229\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4306 - accuracy: 0.8328 - val_loss: 0.7102 - val_accuracy: 0.7470\n",
      "Epoch 20/1000\n",
      "842/854 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.8355\n",
      "Epoch 20: val_accuracy did not improve from 0.75229\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4239 - accuracy: 0.8354 - val_loss: 0.7192 - val_accuracy: 0.7496\n",
      "Epoch 21/1000\n",
      "842/854 [============================>.] - ETA: 0s - loss: 0.4182 - accuracy: 0.8376\n",
      "Epoch 21: val_accuracy did not improve from 0.75229\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4186 - accuracy: 0.8375 - val_loss: 0.7479 - val_accuracy: 0.7457\n",
      "Epoch 22/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.4163 - accuracy: 0.8382\n",
      "Epoch 22: val_accuracy did not improve from 0.75229\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4163 - accuracy: 0.8382 - val_loss: 0.7380 - val_accuracy: 0.7472\n",
      "Epoch 23/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.4085 - accuracy: 0.8416\n",
      "Epoch 23: val_accuracy improved from 0.75229 to 0.75346, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4085 - accuracy: 0.8416 - val_loss: 0.7404 - val_accuracy: 0.7535\n",
      "Epoch 24/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.4051 - accuracy: 0.8421\n",
      "Epoch 24: val_accuracy improved from 0.75346 to 0.75363, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4051 - accuracy: 0.8421 - val_loss: 0.7428 - val_accuracy: 0.7536\n",
      "Epoch 25/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.4017 - accuracy: 0.8443\n",
      "Epoch 25: val_accuracy did not improve from 0.75363\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.4017 - accuracy: 0.8443 - val_loss: 0.7516 - val_accuracy: 0.7500\n",
      "Epoch 26/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.3965 - accuracy: 0.8463\n",
      "Epoch 26: val_accuracy did not improve from 0.75363\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3964 - accuracy: 0.8463 - val_loss: 0.7386 - val_accuracy: 0.7524\n",
      "Epoch 27/1000\n",
      "842/854 [============================>.] - ETA: 0s - loss: 0.3928 - accuracy: 0.8481\n",
      "Epoch 27: val_accuracy did not improve from 0.75363\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3928 - accuracy: 0.8480 - val_loss: 0.7312 - val_accuracy: 0.7457\n",
      "Epoch 28/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8491\n",
      "Epoch 28: val_accuracy did not improve from 0.75363\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3884 - accuracy: 0.8490 - val_loss: 0.7488 - val_accuracy: 0.7536\n",
      "Epoch 29/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.8511\n",
      "Epoch 29: val_accuracy improved from 0.75363 to 0.75497, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3822 - accuracy: 0.8512 - val_loss: 0.7609 - val_accuracy: 0.7550\n",
      "Epoch 30/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.8537\n",
      "Epoch 30: val_accuracy did not improve from 0.75497\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.3803 - accuracy: 0.8537 - val_loss: 0.7557 - val_accuracy: 0.7481\n",
      "Epoch 31/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8528\n",
      "Epoch 31: val_accuracy improved from 0.75497 to 0.75531, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3779 - accuracy: 0.8528 - val_loss: 0.7374 - val_accuracy: 0.7553\n",
      "Epoch 32/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.3744 - accuracy: 0.8565\n",
      "Epoch 32: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3744 - accuracy: 0.8564 - val_loss: 0.7544 - val_accuracy: 0.7509\n",
      "Epoch 33/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.3712 - accuracy: 0.8554\n",
      "Epoch 33: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 3s 4ms/step - loss: 0.3713 - accuracy: 0.8554 - val_loss: 0.7683 - val_accuracy: 0.7533\n",
      "Epoch 34/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8586\n",
      "Epoch 34: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3705 - accuracy: 0.8586 - val_loss: 0.7627 - val_accuracy: 0.7532\n",
      "Epoch 35/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8596\n",
      "Epoch 35: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3625 - accuracy: 0.8596 - val_loss: 0.7646 - val_accuracy: 0.7527\n",
      "Epoch 36/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.3609 - accuracy: 0.8616\n",
      "Epoch 36: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3614 - accuracy: 0.8614 - val_loss: 0.7569 - val_accuracy: 0.7503\n",
      "Epoch 37/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.3595 - accuracy: 0.8613\n",
      "Epoch 37: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3596 - accuracy: 0.8612 - val_loss: 0.7855 - val_accuracy: 0.7530\n",
      "Epoch 38/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8618\n",
      "Epoch 38: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3573 - accuracy: 0.8616 - val_loss: 0.7915 - val_accuracy: 0.7499\n",
      "Epoch 39/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8627\n",
      "Epoch 39: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3567 - accuracy: 0.8627 - val_loss: 0.7753 - val_accuracy: 0.7530\n",
      "Epoch 40/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.8656\n",
      "Epoch 40: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3505 - accuracy: 0.8656 - val_loss: 0.7747 - val_accuracy: 0.7518\n",
      "Epoch 41/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8650\n",
      "Epoch 41: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3481 - accuracy: 0.8650 - val_loss: 0.8072 - val_accuracy: 0.7505\n",
      "Epoch 42/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.3459 - accuracy: 0.8661\n",
      "Epoch 42: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3461 - accuracy: 0.8661 - val_loss: 0.8056 - val_accuracy: 0.7424\n",
      "Epoch 43/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8662\n",
      "Epoch 43: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3485 - accuracy: 0.8661 - val_loss: 0.7827 - val_accuracy: 0.7488\n",
      "Epoch 44/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.8668\n",
      "Epoch 44: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3457 - accuracy: 0.8667 - val_loss: 0.7847 - val_accuracy: 0.7522\n",
      "Epoch 45/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8682\n",
      "Epoch 45: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3410 - accuracy: 0.8682 - val_loss: 0.8158 - val_accuracy: 0.7442\n",
      "Epoch 46/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8681\n",
      "Epoch 46: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3426 - accuracy: 0.8681 - val_loss: 0.7896 - val_accuracy: 0.7454\n",
      "Epoch 47/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.3400 - accuracy: 0.8690\n",
      "Epoch 47: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3399 - accuracy: 0.8691 - val_loss: 0.7921 - val_accuracy: 0.7509\n",
      "Epoch 48/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.8710\n",
      "Epoch 48: val_accuracy did not improve from 0.75531\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3368 - accuracy: 0.8708 - val_loss: 0.8009 - val_accuracy: 0.7483\n",
      "Epoch 49/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.3356 - accuracy: 0.8712\n",
      "Epoch 49: val_accuracy improved from 0.75531 to 0.75556, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3353 - accuracy: 0.8713 - val_loss: 0.8065 - val_accuracy: 0.7556\n",
      "Epoch 50/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8711\n",
      "Epoch 50: val_accuracy did not improve from 0.75556\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3328 - accuracy: 0.8709 - val_loss: 0.8227 - val_accuracy: 0.7460\n",
      "Epoch 51/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8720\n",
      "Epoch 51: val_accuracy did not improve from 0.75556\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3332 - accuracy: 0.8720 - val_loss: 0.8144 - val_accuracy: 0.7467\n",
      "Epoch 52/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8750\n",
      "Epoch 52: val_accuracy did not improve from 0.75556\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3282 - accuracy: 0.8750 - val_loss: 0.8035 - val_accuracy: 0.7515\n",
      "Epoch 53/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.8727\n",
      "Epoch 53: val_accuracy improved from 0.75556 to 0.75782, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3301 - accuracy: 0.8727 - val_loss: 0.8095 - val_accuracy: 0.7578\n",
      "Epoch 54/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.3254 - accuracy: 0.8752\n",
      "Epoch 54: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3256 - accuracy: 0.8752 - val_loss: 0.8060 - val_accuracy: 0.7506\n",
      "Epoch 55/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8746\n",
      "Epoch 55: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3258 - accuracy: 0.8747 - val_loss: 0.8372 - val_accuracy: 0.7509\n",
      "Epoch 56/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.8748\n",
      "Epoch 56: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3211 - accuracy: 0.8750 - val_loss: 0.8245 - val_accuracy: 0.7495\n",
      "Epoch 57/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8778\n",
      "Epoch 57: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3182 - accuracy: 0.8778 - val_loss: 0.8097 - val_accuracy: 0.7514\n",
      "Epoch 58/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8773\n",
      "Epoch 58: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3217 - accuracy: 0.8773 - val_loss: 0.8092 - val_accuracy: 0.7488\n",
      "Epoch 59/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8769\n",
      "Epoch 59: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3189 - accuracy: 0.8770 - val_loss: 0.8302 - val_accuracy: 0.7453\n",
      "Epoch 60/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8770\n",
      "Epoch 60: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3195 - accuracy: 0.8770 - val_loss: 0.8225 - val_accuracy: 0.7516\n",
      "Epoch 61/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8788\n",
      "Epoch 61: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3174 - accuracy: 0.8786 - val_loss: 0.8185 - val_accuracy: 0.7483\n",
      "Epoch 62/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8795\n",
      "Epoch 62: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3152 - accuracy: 0.8795 - val_loss: 0.8101 - val_accuracy: 0.7565\n",
      "Epoch 63/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8785\n",
      "Epoch 63: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3142 - accuracy: 0.8783 - val_loss: 0.8094 - val_accuracy: 0.7538\n",
      "Epoch 64/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.3130 - accuracy: 0.8798\n",
      "Epoch 64: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3130 - accuracy: 0.8798 - val_loss: 0.8380 - val_accuracy: 0.7484\n",
      "Epoch 65/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.8800\n",
      "Epoch 65: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3140 - accuracy: 0.8799 - val_loss: 0.8311 - val_accuracy: 0.7498\n",
      "Epoch 66/1000\n",
      "842/854 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8811\n",
      "Epoch 66: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3110 - accuracy: 0.8807 - val_loss: 0.8142 - val_accuracy: 0.7533\n",
      "Epoch 67/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8816\n",
      "Epoch 67: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3069 - accuracy: 0.8816 - val_loss: 0.8504 - val_accuracy: 0.7499\n",
      "Epoch 68/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.3080 - accuracy: 0.8831\n",
      "Epoch 68: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3079 - accuracy: 0.8832 - val_loss: 0.8494 - val_accuracy: 0.7561\n",
      "Epoch 69/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.3080 - accuracy: 0.8809\n",
      "Epoch 69: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3079 - accuracy: 0.8809 - val_loss: 0.8416 - val_accuracy: 0.7494\n",
      "Epoch 70/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8821\n",
      "Epoch 70: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3072 - accuracy: 0.8821 - val_loss: 0.8472 - val_accuracy: 0.7504\n",
      "Epoch 71/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8836\n",
      "Epoch 71: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3040 - accuracy: 0.8836 - val_loss: 0.8524 - val_accuracy: 0.7456\n",
      "Epoch 72/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8837\n",
      "Epoch 72: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3038 - accuracy: 0.8837 - val_loss: 0.8604 - val_accuracy: 0.7500\n",
      "Epoch 73/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8821\n",
      "Epoch 73: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3053 - accuracy: 0.8820 - val_loss: 0.8309 - val_accuracy: 0.7549\n",
      "Epoch 74/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8846\n",
      "Epoch 74: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3022 - accuracy: 0.8845 - val_loss: 0.8448 - val_accuracy: 0.7481\n",
      "Epoch 75/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8839\n",
      "Epoch 75: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2995 - accuracy: 0.8839 - val_loss: 0.8347 - val_accuracy: 0.7542\n",
      "Epoch 76/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8841\n",
      "Epoch 76: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.3032 - accuracy: 0.8842 - val_loss: 0.8402 - val_accuracy: 0.7536\n",
      "Epoch 77/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.8851\n",
      "Epoch 77: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2989 - accuracy: 0.8850 - val_loss: 0.8547 - val_accuracy: 0.7482\n",
      "Epoch 78/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8859\n",
      "Epoch 78: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2990 - accuracy: 0.8859 - val_loss: 0.8364 - val_accuracy: 0.7572\n",
      "Epoch 79/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8865\n",
      "Epoch 79: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2976 - accuracy: 0.8865 - val_loss: 0.8658 - val_accuracy: 0.7546\n",
      "Epoch 80/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8868\n",
      "Epoch 80: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2961 - accuracy: 0.8867 - val_loss: 0.8523 - val_accuracy: 0.7502\n",
      "Epoch 81/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8879\n",
      "Epoch 81: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2962 - accuracy: 0.8879 - val_loss: 0.8565 - val_accuracy: 0.7561\n",
      "Epoch 82/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8881\n",
      "Epoch 82: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2946 - accuracy: 0.8880 - val_loss: 0.8744 - val_accuracy: 0.7489\n",
      "Epoch 83/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8872\n",
      "Epoch 83: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2947 - accuracy: 0.8873 - val_loss: 0.8593 - val_accuracy: 0.7529\n",
      "Epoch 84/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8877\n",
      "Epoch 84: val_accuracy did not improve from 0.75782\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2963 - accuracy: 0.8876 - val_loss: 0.8367 - val_accuracy: 0.7522\n",
      "Epoch 85/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8882\n",
      "Epoch 85: val_accuracy improved from 0.75782 to 0.75908, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2921 - accuracy: 0.8883 - val_loss: 0.8590 - val_accuracy: 0.7591\n",
      "Epoch 86/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2899 - accuracy: 0.8893\n",
      "Epoch 86: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2899 - accuracy: 0.8893 - val_loss: 0.8656 - val_accuracy: 0.7530\n",
      "Epoch 87/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8878\n",
      "Epoch 87: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2910 - accuracy: 0.8877 - val_loss: 0.8534 - val_accuracy: 0.7586\n",
      "Epoch 88/1000\n",
      "842/854 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8895\n",
      "Epoch 88: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2904 - accuracy: 0.8895 - val_loss: 0.8598 - val_accuracy: 0.7576\n",
      "Epoch 89/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.8897\n",
      "Epoch 89: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2890 - accuracy: 0.8897 - val_loss: 0.8931 - val_accuracy: 0.7509\n",
      "Epoch 90/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8892\n",
      "Epoch 90: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2898 - accuracy: 0.8892 - val_loss: 0.8829 - val_accuracy: 0.7523\n",
      "Epoch 91/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.8894\n",
      "Epoch 91: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2894 - accuracy: 0.8893 - val_loss: 0.8701 - val_accuracy: 0.7520\n",
      "Epoch 92/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.8909\n",
      "Epoch 92: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2875 - accuracy: 0.8908 - val_loss: 0.8771 - val_accuracy: 0.7543\n",
      "Epoch 93/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.8900\n",
      "Epoch 93: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2875 - accuracy: 0.8901 - val_loss: 0.8692 - val_accuracy: 0.7566\n",
      "Epoch 94/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.8898\n",
      "Epoch 94: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2860 - accuracy: 0.8897 - val_loss: 0.9009 - val_accuracy: 0.7489\n",
      "Epoch 95/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.8916\n",
      "Epoch 95: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2835 - accuracy: 0.8917 - val_loss: 0.9002 - val_accuracy: 0.7503\n",
      "Epoch 96/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.8915\n",
      "Epoch 96: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2837 - accuracy: 0.8915 - val_loss: 0.8589 - val_accuracy: 0.7555\n",
      "Epoch 97/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.8921\n",
      "Epoch 97: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 3s 4ms/step - loss: 0.2827 - accuracy: 0.8921 - val_loss: 0.8810 - val_accuracy: 0.7501\n",
      "Epoch 98/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.8931\n",
      "Epoch 98: val_accuracy did not improve from 0.75908\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2796 - accuracy: 0.8931 - val_loss: 0.8593 - val_accuracy: 0.7535\n",
      "Epoch 99/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.2829 - accuracy: 0.8921\n",
      "Epoch 99: val_accuracy improved from 0.75908 to 0.76017, saving model to best_weights_dense_7.hdf5\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2828 - accuracy: 0.8922 - val_loss: 0.8848 - val_accuracy: 0.7602\n",
      "Epoch 100/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.8928\n",
      "Epoch 100: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2811 - accuracy: 0.8928 - val_loss: 0.8794 - val_accuracy: 0.7554\n",
      "Epoch 101/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.8920\n",
      "Epoch 101: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2824 - accuracy: 0.8920 - val_loss: 0.8962 - val_accuracy: 0.7529\n",
      "Epoch 102/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.2799 - accuracy: 0.8943\n",
      "Epoch 102: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2797 - accuracy: 0.8943 - val_loss: 0.8818 - val_accuracy: 0.7505\n",
      "Epoch 103/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.8923\n",
      "Epoch 103: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2822 - accuracy: 0.8922 - val_loss: 0.8615 - val_accuracy: 0.7559\n",
      "Epoch 104/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.8924\n",
      "Epoch 104: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2810 - accuracy: 0.8924 - val_loss: 0.8880 - val_accuracy: 0.7566\n",
      "Epoch 105/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.8930\n",
      "Epoch 105: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2793 - accuracy: 0.8930 - val_loss: 0.9044 - val_accuracy: 0.7498\n",
      "Epoch 106/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.8943\n",
      "Epoch 106: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2781 - accuracy: 0.8943 - val_loss: 0.9133 - val_accuracy: 0.7483\n",
      "Epoch 107/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.2771 - accuracy: 0.8940\n",
      "Epoch 107: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2771 - accuracy: 0.8941 - val_loss: 0.8872 - val_accuracy: 0.7516\n",
      "Epoch 108/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.8950\n",
      "Epoch 108: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2772 - accuracy: 0.8950 - val_loss: 0.8711 - val_accuracy: 0.7497\n",
      "Epoch 109/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.2760 - accuracy: 0.8953\n",
      "Epoch 109: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2761 - accuracy: 0.8952 - val_loss: 0.8905 - val_accuracy: 0.7559\n",
      "Epoch 110/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.8950\n",
      "Epoch 110: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2768 - accuracy: 0.8949 - val_loss: 0.9026 - val_accuracy: 0.7540\n",
      "Epoch 111/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.8959\n",
      "Epoch 111: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2760 - accuracy: 0.8959 - val_loss: 0.9110 - val_accuracy: 0.7480\n",
      "Epoch 112/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.2742 - accuracy: 0.8958\n",
      "Epoch 112: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2739 - accuracy: 0.8958 - val_loss: 0.9071 - val_accuracy: 0.7499\n",
      "Epoch 113/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.2733 - accuracy: 0.8965\n",
      "Epoch 113: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2737 - accuracy: 0.8964 - val_loss: 0.8846 - val_accuracy: 0.7549\n",
      "Epoch 114/1000\n",
      "842/854 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.8950\n",
      "Epoch 114: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2735 - accuracy: 0.8949 - val_loss: 0.9092 - val_accuracy: 0.7533\n",
      "Epoch 115/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.2736 - accuracy: 0.8961\n",
      "Epoch 115: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2736 - accuracy: 0.8961 - val_loss: 0.9159 - val_accuracy: 0.7501\n",
      "Epoch 116/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.8958\n",
      "Epoch 116: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2723 - accuracy: 0.8958 - val_loss: 0.8914 - val_accuracy: 0.7520\n",
      "Epoch 117/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.8967\n",
      "Epoch 117: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2711 - accuracy: 0.8968 - val_loss: 0.9070 - val_accuracy: 0.7510\n",
      "Epoch 118/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.8966\n",
      "Epoch 118: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2708 - accuracy: 0.8966 - val_loss: 0.9007 - val_accuracy: 0.7526\n",
      "Epoch 119/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.2724 - accuracy: 0.8965\n",
      "Epoch 119: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2726 - accuracy: 0.8965 - val_loss: 0.8805 - val_accuracy: 0.7517\n",
      "Epoch 120/1000\n",
      "854/854 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.8967\n",
      "Epoch 120: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2702 - accuracy: 0.8967 - val_loss: 0.9015 - val_accuracy: 0.7526\n",
      "Epoch 121/1000\n",
      "850/854 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.8971\n",
      "Epoch 121: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2711 - accuracy: 0.8972 - val_loss: 0.8728 - val_accuracy: 0.7525\n",
      "Epoch 122/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.8978\n",
      "Epoch 122: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2709 - accuracy: 0.8978 - val_loss: 0.8948 - val_accuracy: 0.7514\n",
      "Epoch 123/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.8978\n",
      "Epoch 123: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2685 - accuracy: 0.8978 - val_loss: 0.9015 - val_accuracy: 0.7490\n",
      "Epoch 124/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.8973\n",
      "Epoch 124: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2698 - accuracy: 0.8973 - val_loss: 0.9122 - val_accuracy: 0.7466\n",
      "Epoch 125/1000\n",
      "844/854 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.8984\n",
      "Epoch 125: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2680 - accuracy: 0.8984 - val_loss: 0.9144 - val_accuracy: 0.7491\n",
      "Epoch 126/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.8986\n",
      "Epoch 126: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2678 - accuracy: 0.8987 - val_loss: 0.9208 - val_accuracy: 0.7520\n",
      "Epoch 127/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.8996\n",
      "Epoch 127: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2661 - accuracy: 0.8998 - val_loss: 0.9006 - val_accuracy: 0.7529\n",
      "Epoch 128/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2669 - accuracy: 0.8985\n",
      "Epoch 128: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2668 - accuracy: 0.8985 - val_loss: 0.8963 - val_accuracy: 0.7504\n",
      "Epoch 129/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.8983\n",
      "Epoch 129: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2658 - accuracy: 0.8983 - val_loss: 0.9083 - val_accuracy: 0.7492\n",
      "Epoch 130/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.2664 - accuracy: 0.8981\n",
      "Epoch 130: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2664 - accuracy: 0.8980 - val_loss: 0.9267 - val_accuracy: 0.7492\n",
      "Epoch 131/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.2668 - accuracy: 0.8993\n",
      "Epoch 131: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2668 - accuracy: 0.8992 - val_loss: 0.9074 - val_accuracy: 0.7521\n",
      "Epoch 132/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.8989\n",
      "Epoch 132: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2656 - accuracy: 0.8989 - val_loss: 0.9132 - val_accuracy: 0.7525\n",
      "Epoch 133/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.8984\n",
      "Epoch 133: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2663 - accuracy: 0.8984 - val_loss: 0.9191 - val_accuracy: 0.7516\n",
      "Epoch 134/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.8997\n",
      "Epoch 134: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2646 - accuracy: 0.8997 - val_loss: 0.9273 - val_accuracy: 0.7513\n",
      "Epoch 135/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.2625 - accuracy: 0.9001\n",
      "Epoch 135: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2627 - accuracy: 0.8999 - val_loss: 0.8994 - val_accuracy: 0.7529\n",
      "Epoch 136/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.2641 - accuracy: 0.8997\n",
      "Epoch 136: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2639 - accuracy: 0.8997 - val_loss: 0.9235 - val_accuracy: 0.7511\n",
      "Epoch 137/1000\n",
      "843/854 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.8995\n",
      "Epoch 137: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2644 - accuracy: 0.8994 - val_loss: 0.9189 - val_accuracy: 0.7515\n",
      "Epoch 138/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.8990\n",
      "Epoch 138: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2659 - accuracy: 0.8990 - val_loss: 0.9208 - val_accuracy: 0.7497\n",
      "Epoch 139/1000\n",
      "845/854 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.8991\n",
      "Epoch 139: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2644 - accuracy: 0.8993 - val_loss: 0.9298 - val_accuracy: 0.7452\n",
      "Epoch 140/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9007\n",
      "Epoch 140: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2605 - accuracy: 0.9006 - val_loss: 0.9182 - val_accuracy: 0.7461\n",
      "Epoch 141/1000\n",
      "849/854 [============================>.] - ETA: 0s - loss: 0.2599 - accuracy: 0.9015\n",
      "Epoch 141: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2599 - accuracy: 0.9015 - val_loss: 0.9271 - val_accuracy: 0.7491\n",
      "Epoch 142/1000\n",
      "852/854 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9007\n",
      "Epoch 142: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2621 - accuracy: 0.9006 - val_loss: 0.9155 - val_accuracy: 0.7476\n",
      "Epoch 143/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9012\n",
      "Epoch 143: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2600 - accuracy: 0.9011 - val_loss: 0.9188 - val_accuracy: 0.7501\n",
      "Epoch 144/1000\n",
      "846/854 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.9020\n",
      "Epoch 144: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2607 - accuracy: 0.9020 - val_loss: 0.9197 - val_accuracy: 0.7512\n",
      "Epoch 145/1000\n",
      "847/854 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.9020\n",
      "Epoch 145: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2586 - accuracy: 0.9020 - val_loss: 0.9332 - val_accuracy: 0.7503\n",
      "Epoch 146/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.9017\n",
      "Epoch 146: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 4ms/step - loss: 0.2620 - accuracy: 0.9017 - val_loss: 0.9269 - val_accuracy: 0.7490\n",
      "Epoch 147/1000\n",
      "848/854 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9020\n",
      "Epoch 147: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2569 - accuracy: 0.9021 - val_loss: 0.9329 - val_accuracy: 0.7495\n",
      "Epoch 148/1000\n",
      "853/854 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.9026\n",
      "Epoch 148: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2581 - accuracy: 0.9026 - val_loss: 0.9261 - val_accuracy: 0.7419\n",
      "Epoch 149/1000\n",
      "851/854 [============================>.] - ETA: 0s - loss: 0.2579 - accuracy: 0.9024\n",
      "Epoch 149: val_accuracy did not improve from 0.76017\n",
      "854/854 [==============================] - 4s 5ms/step - loss: 0.2579 - accuracy: 0.9024 - val_loss: 0.9297 - val_accuracy: 0.7490\n",
      "373/373 [==============================] - 0s 971us/step\n",
      "0.7365633548758949\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_45 (Dense)            (109167, 256)             3584      \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (109167, 256)             0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (109167, 256)             65792     \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (109167, 256)             0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (109167, 128)             32896     \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (109167, 128)             0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (109167, 128)             16512     \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (109167, 128)             0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (109167, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.8829 - accuracy: 0.6568\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69089, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 5s 5ms/step - loss: 0.8816 - accuracy: 0.6572 - val_loss: 0.7895 - val_accuracy: 0.6909\n",
      "Epoch 2/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.7365 - accuracy: 0.7143\n",
      "Epoch 2: val_accuracy improved from 0.69089 to 0.69456, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.7365 - accuracy: 0.7142 - val_loss: 0.7713 - val_accuracy: 0.6946\n",
      "Epoch 3/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.6765 - accuracy: 0.7369\n",
      "Epoch 3: val_accuracy improved from 0.69456 to 0.71081, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.6765 - accuracy: 0.7370 - val_loss: 0.7560 - val_accuracy: 0.7108\n",
      "Epoch 4/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.6381 - accuracy: 0.7534\n",
      "Epoch 4: val_accuracy improved from 0.71081 to 0.71964, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.6382 - accuracy: 0.7534 - val_loss: 0.7354 - val_accuracy: 0.7196\n",
      "Epoch 5/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.7634\n",
      "Epoch 5: val_accuracy improved from 0.71964 to 0.72931, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.6110 - accuracy: 0.7634 - val_loss: 0.7197 - val_accuracy: 0.7293\n",
      "Epoch 6/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7738\n",
      "Epoch 6: val_accuracy improved from 0.72931 to 0.73314, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5829 - accuracy: 0.7738 - val_loss: 0.7186 - val_accuracy: 0.7331\n",
      "Epoch 7/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.7816\n",
      "Epoch 7: val_accuracy improved from 0.73314 to 0.73598, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5627 - accuracy: 0.7815 - val_loss: 0.7135 - val_accuracy: 0.7360\n",
      "Epoch 8/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.5454 - accuracy: 0.7886\n",
      "Epoch 8: val_accuracy did not improve from 0.73598\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5453 - accuracy: 0.7886 - val_loss: 0.7197 - val_accuracy: 0.7341\n",
      "Epoch 9/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.5298 - accuracy: 0.7953\n",
      "Epoch 9: val_accuracy did not improve from 0.73598\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5296 - accuracy: 0.7953 - val_loss: 0.7151 - val_accuracy: 0.7360\n",
      "Epoch 10/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.5162 - accuracy: 0.8005\n",
      "Epoch 10: val_accuracy improved from 0.73598 to 0.74181, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.5163 - accuracy: 0.8004 - val_loss: 0.7131 - val_accuracy: 0.7418\n",
      "Epoch 11/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.5021 - accuracy: 0.8052\n",
      "Epoch 11: val_accuracy improved from 0.74181 to 0.74390, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.5022 - accuracy: 0.8052 - val_loss: 0.7091 - val_accuracy: 0.7439\n",
      "Epoch 12/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4902 - accuracy: 0.8102\n",
      "Epoch 12: val_accuracy did not improve from 0.74390\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4901 - accuracy: 0.8102 - val_loss: 0.7276 - val_accuracy: 0.7356\n",
      "Epoch 13/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.4799 - accuracy: 0.8135\n",
      "Epoch 13: val_accuracy did not improve from 0.74390\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.4799 - accuracy: 0.8136 - val_loss: 0.7363 - val_accuracy: 0.7395\n",
      "Epoch 14/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4692 - accuracy: 0.8173\n",
      "Epoch 14: val_accuracy did not improve from 0.74390\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4694 - accuracy: 0.8172 - val_loss: 0.7338 - val_accuracy: 0.7409\n",
      "Epoch 15/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.4611 - accuracy: 0.8216\n",
      "Epoch 15: val_accuracy did not improve from 0.74390\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4612 - accuracy: 0.8215 - val_loss: 0.7180 - val_accuracy: 0.7401\n",
      "Epoch 16/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.4533 - accuracy: 0.8237\n",
      "Epoch 16: val_accuracy did not improve from 0.74390\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4533 - accuracy: 0.8237 - val_loss: 0.7348 - val_accuracy: 0.7431\n",
      "Epoch 17/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.4429 - accuracy: 0.8277\n",
      "Epoch 17: val_accuracy did not improve from 0.74390\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4429 - accuracy: 0.8277 - val_loss: 0.7400 - val_accuracy: 0.7431\n",
      "Epoch 18/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.4363 - accuracy: 0.8305\n",
      "Epoch 18: val_accuracy did not improve from 0.74390\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4362 - accuracy: 0.8306 - val_loss: 0.7487 - val_accuracy: 0.7373\n",
      "Epoch 19/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.4302 - accuracy: 0.8334\n",
      "Epoch 19: val_accuracy improved from 0.74390 to 0.74548, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4305 - accuracy: 0.8333 - val_loss: 0.7560 - val_accuracy: 0.7455\n",
      "Epoch 20/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4246 - accuracy: 0.8343\n",
      "Epoch 20: val_accuracy did not improve from 0.74548\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4245 - accuracy: 0.8343 - val_loss: 0.7609 - val_accuracy: 0.7418\n",
      "Epoch 21/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.4170 - accuracy: 0.8370\n",
      "Epoch 21: val_accuracy did not improve from 0.74548\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4170 - accuracy: 0.8370 - val_loss: 0.7748 - val_accuracy: 0.7443\n",
      "Epoch 22/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.4141 - accuracy: 0.8401\n",
      "Epoch 22: val_accuracy improved from 0.74548 to 0.74915, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4138 - accuracy: 0.8401 - val_loss: 0.7502 - val_accuracy: 0.7491\n",
      "Epoch 23/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.4076 - accuracy: 0.8421\n",
      "Epoch 23: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4078 - accuracy: 0.8420 - val_loss: 0.7610 - val_accuracy: 0.7436\n",
      "Epoch 24/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.4001 - accuracy: 0.8443\n",
      "Epoch 24: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4003 - accuracy: 0.8442 - val_loss: 0.7892 - val_accuracy: 0.7429\n",
      "Epoch 25/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3960 - accuracy: 0.8469\n",
      "Epoch 25: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3965 - accuracy: 0.8466 - val_loss: 0.7847 - val_accuracy: 0.7456\n",
      "Epoch 26/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8477\n",
      "Epoch 26: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3940 - accuracy: 0.8476 - val_loss: 0.7890 - val_accuracy: 0.7401\n",
      "Epoch 27/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.8491\n",
      "Epoch 27: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3883 - accuracy: 0.8492 - val_loss: 0.7812 - val_accuracy: 0.7452\n",
      "Epoch 28/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8500\n",
      "Epoch 28: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3873 - accuracy: 0.8499 - val_loss: 0.7895 - val_accuracy: 0.7406\n",
      "Epoch 29/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3808 - accuracy: 0.8513\n",
      "Epoch 29: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3807 - accuracy: 0.8515 - val_loss: 0.8040 - val_accuracy: 0.7437\n",
      "Epoch 30/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8537\n",
      "Epoch 30: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3774 - accuracy: 0.8537 - val_loss: 0.8126 - val_accuracy: 0.7386\n",
      "Epoch 31/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8541\n",
      "Epoch 31: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3754 - accuracy: 0.8541 - val_loss: 0.7823 - val_accuracy: 0.7418\n",
      "Epoch 32/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8552\n",
      "Epoch 32: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3726 - accuracy: 0.8552 - val_loss: 0.8013 - val_accuracy: 0.7457\n",
      "Epoch 33/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3679 - accuracy: 0.8566\n",
      "Epoch 33: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3680 - accuracy: 0.8566 - val_loss: 0.8155 - val_accuracy: 0.7424\n",
      "Epoch 34/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8579\n",
      "Epoch 34: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3678 - accuracy: 0.8579 - val_loss: 0.8248 - val_accuracy: 0.7467\n",
      "Epoch 35/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3616 - accuracy: 0.8596\n",
      "Epoch 35: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3614 - accuracy: 0.8597 - val_loss: 0.8153 - val_accuracy: 0.7406\n",
      "Epoch 36/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8602\n",
      "Epoch 36: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3605 - accuracy: 0.8601 - val_loss: 0.8060 - val_accuracy: 0.7424\n",
      "Epoch 37/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8631\n",
      "Epoch 37: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3551 - accuracy: 0.8630 - val_loss: 0.8249 - val_accuracy: 0.7447\n",
      "Epoch 38/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3540 - accuracy: 0.8619\n",
      "Epoch 38: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3539 - accuracy: 0.8620 - val_loss: 0.8319 - val_accuracy: 0.7459\n",
      "Epoch 39/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8641\n",
      "Epoch 39: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3500 - accuracy: 0.8640 - val_loss: 0.8388 - val_accuracy: 0.7432\n",
      "Epoch 40/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3472 - accuracy: 0.8654\n",
      "Epoch 40: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3471 - accuracy: 0.8655 - val_loss: 0.8443 - val_accuracy: 0.7466\n",
      "Epoch 41/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.8662\n",
      "Epoch 41: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3453 - accuracy: 0.8663 - val_loss: 0.8548 - val_accuracy: 0.7462\n",
      "Epoch 42/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3438 - accuracy: 0.8669\n",
      "Epoch 42: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3436 - accuracy: 0.8669 - val_loss: 0.8333 - val_accuracy: 0.7450\n",
      "Epoch 43/1000\n",
      "841/853 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8665\n",
      "Epoch 43: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3438 - accuracy: 0.8663 - val_loss: 0.8292 - val_accuracy: 0.7460\n",
      "Epoch 44/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.8693\n",
      "Epoch 44: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3399 - accuracy: 0.8693 - val_loss: 0.8551 - val_accuracy: 0.7421\n",
      "Epoch 45/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8698\n",
      "Epoch 45: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3376 - accuracy: 0.8698 - val_loss: 0.8526 - val_accuracy: 0.7481\n",
      "Epoch 46/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8689\n",
      "Epoch 46: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3370 - accuracy: 0.8688 - val_loss: 0.8511 - val_accuracy: 0.7470\n",
      "Epoch 47/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.8695\n",
      "Epoch 47: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3382 - accuracy: 0.8694 - val_loss: 0.8473 - val_accuracy: 0.7411\n",
      "Epoch 48/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3314 - accuracy: 0.8728\n",
      "Epoch 48: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3314 - accuracy: 0.8728 - val_loss: 0.8532 - val_accuracy: 0.7426\n",
      "Epoch 49/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3316 - accuracy: 0.8718\n",
      "Epoch 49: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3318 - accuracy: 0.8717 - val_loss: 0.8718 - val_accuracy: 0.7379\n",
      "Epoch 50/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3283 - accuracy: 0.8727\n",
      "Epoch 50: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3284 - accuracy: 0.8726 - val_loss: 0.8761 - val_accuracy: 0.7396\n",
      "Epoch 51/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.8728\n",
      "Epoch 51: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3317 - accuracy: 0.8728 - val_loss: 0.8587 - val_accuracy: 0.7425\n",
      "Epoch 52/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.8735\n",
      "Epoch 52: val_accuracy did not improve from 0.74915\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3272 - accuracy: 0.8735 - val_loss: 0.8710 - val_accuracy: 0.7387\n",
      "Epoch 53/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8739\n",
      "Epoch 53: val_accuracy improved from 0.74915 to 0.75081, saving model to best_weights_dense_8.hdf5\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3266 - accuracy: 0.8739 - val_loss: 0.8721 - val_accuracy: 0.7508\n",
      "Epoch 54/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8751\n",
      "Epoch 54: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3267 - accuracy: 0.8751 - val_loss: 0.8668 - val_accuracy: 0.7440\n",
      "Epoch 55/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.8741\n",
      "Epoch 55: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3215 - accuracy: 0.8742 - val_loss: 0.8728 - val_accuracy: 0.7405\n",
      "Epoch 56/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8761\n",
      "Epoch 56: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3209 - accuracy: 0.8760 - val_loss: 0.8635 - val_accuracy: 0.7423\n",
      "Epoch 57/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.8769\n",
      "Epoch 57: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3185 - accuracy: 0.8769 - val_loss: 0.8749 - val_accuracy: 0.7388\n",
      "Epoch 58/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8772\n",
      "Epoch 58: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3170 - accuracy: 0.8771 - val_loss: 0.8847 - val_accuracy: 0.7471\n",
      "Epoch 59/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8779\n",
      "Epoch 59: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3173 - accuracy: 0.8778 - val_loss: 0.8759 - val_accuracy: 0.7432\n",
      "Epoch 60/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8799\n",
      "Epoch 60: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3143 - accuracy: 0.8799 - val_loss: 0.8574 - val_accuracy: 0.7436\n",
      "Epoch 61/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.8777\n",
      "Epoch 61: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3152 - accuracy: 0.8778 - val_loss: 0.8853 - val_accuracy: 0.7411\n",
      "Epoch 62/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.8788\n",
      "Epoch 62: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3139 - accuracy: 0.8788 - val_loss: 0.8887 - val_accuracy: 0.7419\n",
      "Epoch 63/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3116 - accuracy: 0.8794\n",
      "Epoch 63: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 4ms/step - loss: 0.3115 - accuracy: 0.8795 - val_loss: 0.8935 - val_accuracy: 0.7438\n",
      "Epoch 64/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8785\n",
      "Epoch 64: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3112 - accuracy: 0.8785 - val_loss: 0.8655 - val_accuracy: 0.7397\n",
      "Epoch 65/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8807\n",
      "Epoch 65: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3109 - accuracy: 0.8808 - val_loss: 0.8997 - val_accuracy: 0.7405\n",
      "Epoch 66/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8833\n",
      "Epoch 66: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3031 - accuracy: 0.8833 - val_loss: 0.9115 - val_accuracy: 0.7446\n",
      "Epoch 67/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.3063 - accuracy: 0.8817\n",
      "Epoch 67: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3063 - accuracy: 0.8817 - val_loss: 0.8938 - val_accuracy: 0.7400\n",
      "Epoch 68/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8818\n",
      "Epoch 68: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3071 - accuracy: 0.8818 - val_loss: 0.9113 - val_accuracy: 0.7425\n",
      "Epoch 69/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8821\n",
      "Epoch 69: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3040 - accuracy: 0.8820 - val_loss: 0.9153 - val_accuracy: 0.7367\n",
      "Epoch 70/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8820\n",
      "Epoch 70: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3064 - accuracy: 0.8818 - val_loss: 0.8888 - val_accuracy: 0.7434\n",
      "Epoch 71/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8830\n",
      "Epoch 71: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3027 - accuracy: 0.8830 - val_loss: 0.9113 - val_accuracy: 0.7456\n",
      "Epoch 72/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8822\n",
      "Epoch 72: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3030 - accuracy: 0.8821 - val_loss: 0.9041 - val_accuracy: 0.7418\n",
      "Epoch 73/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8846\n",
      "Epoch 73: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2983 - accuracy: 0.8845 - val_loss: 0.9076 - val_accuracy: 0.7496\n",
      "Epoch 74/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8836\n",
      "Epoch 74: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.3013 - accuracy: 0.8836 - val_loss: 0.9056 - val_accuracy: 0.7412\n",
      "Epoch 75/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.8875\n",
      "Epoch 75: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2959 - accuracy: 0.8875 - val_loss: 0.9258 - val_accuracy: 0.7392\n",
      "Epoch 76/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8863\n",
      "Epoch 76: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2958 - accuracy: 0.8862 - val_loss: 0.8985 - val_accuracy: 0.7427\n",
      "Epoch 77/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8858\n",
      "Epoch 77: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2975 - accuracy: 0.8859 - val_loss: 0.9153 - val_accuracy: 0.7378\n",
      "Epoch 78/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8868\n",
      "Epoch 78: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2956 - accuracy: 0.8868 - val_loss: 0.9162 - val_accuracy: 0.7412\n",
      "Epoch 79/1000\n",
      "851/853 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8878\n",
      "Epoch 79: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2933 - accuracy: 0.8878 - val_loss: 0.9104 - val_accuracy: 0.7429\n",
      "Epoch 80/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8880\n",
      "Epoch 80: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2935 - accuracy: 0.8879 - val_loss: 0.8941 - val_accuracy: 0.7420\n",
      "Epoch 81/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.8894\n",
      "Epoch 81: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2880 - accuracy: 0.8894 - val_loss: 0.9124 - val_accuracy: 0.7457\n",
      "Epoch 82/1000\n",
      "845/853 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8873\n",
      "Epoch 82: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2928 - accuracy: 0.8873 - val_loss: 0.9216 - val_accuracy: 0.7380\n",
      "Epoch 83/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8876\n",
      "Epoch 83: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2935 - accuracy: 0.8876 - val_loss: 0.9321 - val_accuracy: 0.7410\n",
      "Epoch 84/1000\n",
      "853/853 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.8891\n",
      "Epoch 84: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2897 - accuracy: 0.8891 - val_loss: 0.9234 - val_accuracy: 0.7395\n",
      "Epoch 85/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.8903\n",
      "Epoch 85: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2875 - accuracy: 0.8902 - val_loss: 0.9324 - val_accuracy: 0.7444\n",
      "Epoch 86/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8882\n",
      "Epoch 86: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2876 - accuracy: 0.8883 - val_loss: 0.9202 - val_accuracy: 0.7470\n",
      "Epoch 87/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8897\n",
      "Epoch 87: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2876 - accuracy: 0.8898 - val_loss: 0.9026 - val_accuracy: 0.7467\n",
      "Epoch 88/1000\n",
      "844/853 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.8886\n",
      "Epoch 88: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2887 - accuracy: 0.8887 - val_loss: 0.9232 - val_accuracy: 0.7460\n",
      "Epoch 89/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.8895\n",
      "Epoch 89: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2864 - accuracy: 0.8895 - val_loss: 0.9138 - val_accuracy: 0.7476\n",
      "Epoch 90/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.8900\n",
      "Epoch 90: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2883 - accuracy: 0.8900 - val_loss: 0.9039 - val_accuracy: 0.7440\n",
      "Epoch 91/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.8913\n",
      "Epoch 91: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2837 - accuracy: 0.8912 - val_loss: 0.9232 - val_accuracy: 0.7447\n",
      "Epoch 92/1000\n",
      "849/853 [============================>.] - ETA: 0s - loss: 0.2841 - accuracy: 0.8908\n",
      "Epoch 92: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2842 - accuracy: 0.8907 - val_loss: 0.9228 - val_accuracy: 0.7443\n",
      "Epoch 93/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.8916\n",
      "Epoch 93: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2847 - accuracy: 0.8916 - val_loss: 0.9290 - val_accuracy: 0.7421\n",
      "Epoch 94/1000\n",
      "846/853 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.8911\n",
      "Epoch 94: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2844 - accuracy: 0.8912 - val_loss: 0.9364 - val_accuracy: 0.7468\n",
      "Epoch 95/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8917\n",
      "Epoch 95: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2819 - accuracy: 0.8916 - val_loss: 0.9105 - val_accuracy: 0.7478\n",
      "Epoch 96/1000\n",
      "843/853 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.8921\n",
      "Epoch 96: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2810 - accuracy: 0.8919 - val_loss: 0.9294 - val_accuracy: 0.7435\n",
      "Epoch 97/1000\n",
      "848/853 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.8934\n",
      "Epoch 97: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2802 - accuracy: 0.8934 - val_loss: 0.9212 - val_accuracy: 0.7436\n",
      "Epoch 98/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.8935\n",
      "Epoch 98: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2785 - accuracy: 0.8935 - val_loss: 0.9262 - val_accuracy: 0.7451\n",
      "Epoch 99/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.2821 - accuracy: 0.8919\n",
      "Epoch 99: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2822 - accuracy: 0.8919 - val_loss: 0.9288 - val_accuracy: 0.7407\n",
      "Epoch 100/1000\n",
      "842/853 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.8930\n",
      "Epoch 100: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2794 - accuracy: 0.8930 - val_loss: 0.9271 - val_accuracy: 0.7427\n",
      "Epoch 101/1000\n",
      "850/853 [============================>.] - ETA: 0s - loss: 0.2778 - accuracy: 0.8939\n",
      "Epoch 101: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2778 - accuracy: 0.8939 - val_loss: 0.9205 - val_accuracy: 0.7437\n",
      "Epoch 102/1000\n",
      "852/853 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.8945\n",
      "Epoch 102: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2763 - accuracy: 0.8945 - val_loss: 0.9425 - val_accuracy: 0.7430\n",
      "Epoch 103/1000\n",
      "847/853 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.8936\n",
      "Epoch 103: val_accuracy did not improve from 0.75081\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2771 - accuracy: 0.8935 - val_loss: 0.9228 - val_accuracy: 0.7437\n",
      "375/375 [==============================] - 0s 732us/step\n",
      "0.742043025859142\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_50 (Dense)            (109035, 256)             3584      \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (109035, 256)             0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (109035, 256)             65792     \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (109035, 256)             0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (109035, 128)             32896     \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (109035, 128)             0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (109035, 128)             16512     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (109035, 128)             0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (109035, 5)               645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.8898 - accuracy: 0.6520\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67884, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.8896 - accuracy: 0.6520 - val_loss: 0.8239 - val_accuracy: 0.6788\n",
      "Epoch 2/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.7457 - accuracy: 0.7091\n",
      "Epoch 2: val_accuracy improved from 0.67884 to 0.71931, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.7456 - accuracy: 0.7091 - val_loss: 0.7478 - val_accuracy: 0.7193\n",
      "Epoch 3/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.7327\n",
      "Epoch 3: val_accuracy improved from 0.71931 to 0.72566, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.6861 - accuracy: 0.7328 - val_loss: 0.7193 - val_accuracy: 0.7257\n",
      "Epoch 4/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.6449 - accuracy: 0.7480\n",
      "Epoch 4: val_accuracy improved from 0.72566 to 0.73877, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.6449 - accuracy: 0.7481 - val_loss: 0.6949 - val_accuracy: 0.7388\n",
      "Epoch 5/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.6148 - accuracy: 0.7619\n",
      "Epoch 5: val_accuracy improved from 0.73877 to 0.74066, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.6150 - accuracy: 0.7618 - val_loss: 0.6950 - val_accuracy: 0.7407\n",
      "Epoch 6/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.5868 - accuracy: 0.7733\n",
      "Epoch 6: val_accuracy improved from 0.74066 to 0.74866, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5867 - accuracy: 0.7733 - val_loss: 0.6987 - val_accuracy: 0.7487\n",
      "Epoch 7/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7802\n",
      "Epoch 7: val_accuracy did not improve from 0.74866\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.5673 - accuracy: 0.7803 - val_loss: 0.6960 - val_accuracy: 0.7454\n",
      "Epoch 8/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.5460 - accuracy: 0.7878\n",
      "Epoch 8: val_accuracy did not improve from 0.74866\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.5462 - accuracy: 0.7878 - val_loss: 0.6847 - val_accuracy: 0.7444\n",
      "Epoch 9/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.5286 - accuracy: 0.7949\n",
      "Epoch 9: val_accuracy improved from 0.74866 to 0.75633, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.5286 - accuracy: 0.7949 - val_loss: 0.6814 - val_accuracy: 0.7563\n",
      "Epoch 10/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.5144 - accuracy: 0.7995\n",
      "Epoch 10: val_accuracy did not improve from 0.75633\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5142 - accuracy: 0.7996 - val_loss: 0.6821 - val_accuracy: 0.7501\n",
      "Epoch 11/1000\n",
      "846/852 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.8053\n",
      "Epoch 11: val_accuracy did not improve from 0.75633\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.5020 - accuracy: 0.8052 - val_loss: 0.6719 - val_accuracy: 0.7560\n",
      "Epoch 12/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.4891 - accuracy: 0.8116\n",
      "Epoch 12: val_accuracy improved from 0.75633 to 0.75847, saving model to best_weights_dense_9.hdf5\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4891 - accuracy: 0.8115 - val_loss: 0.7012 - val_accuracy: 0.7585\n",
      "Epoch 13/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4789 - accuracy: 0.8152\n",
      "Epoch 13: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4789 - accuracy: 0.8152 - val_loss: 0.6969 - val_accuracy: 0.7515\n",
      "Epoch 14/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.4679 - accuracy: 0.8183\n",
      "Epoch 14: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4679 - accuracy: 0.8184 - val_loss: 0.7132 - val_accuracy: 0.7492\n",
      "Epoch 15/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.4570 - accuracy: 0.8230\n",
      "Epoch 15: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4573 - accuracy: 0.8229 - val_loss: 0.7180 - val_accuracy: 0.7516\n",
      "Epoch 16/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.4508 - accuracy: 0.8251\n",
      "Epoch 16: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4509 - accuracy: 0.8249 - val_loss: 0.7205 - val_accuracy: 0.7557\n",
      "Epoch 17/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.4447 - accuracy: 0.8285\n",
      "Epoch 17: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4447 - accuracy: 0.8285 - val_loss: 0.7362 - val_accuracy: 0.7560\n",
      "Epoch 18/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.4356 - accuracy: 0.8307\n",
      "Epoch 18: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4353 - accuracy: 0.8308 - val_loss: 0.7244 - val_accuracy: 0.7529\n",
      "Epoch 19/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.8341\n",
      "Epoch 19: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4309 - accuracy: 0.8339 - val_loss: 0.7212 - val_accuracy: 0.7509\n",
      "Epoch 20/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.4243 - accuracy: 0.8351\n",
      "Epoch 20: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4245 - accuracy: 0.8351 - val_loss: 0.7370 - val_accuracy: 0.7567\n",
      "Epoch 21/1000\n",
      "840/852 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8380\n",
      "Epoch 21: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4167 - accuracy: 0.8380 - val_loss: 0.7355 - val_accuracy: 0.7548\n",
      "Epoch 22/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8399\n",
      "Epoch 22: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.4137 - accuracy: 0.8400 - val_loss: 0.7433 - val_accuracy: 0.7562\n",
      "Epoch 23/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8423\n",
      "Epoch 23: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4072 - accuracy: 0.8422 - val_loss: 0.7530 - val_accuracy: 0.7496\n",
      "Epoch 24/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.4030 - accuracy: 0.8446\n",
      "Epoch 24: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.4027 - accuracy: 0.8446 - val_loss: 0.7516 - val_accuracy: 0.7508\n",
      "Epoch 25/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3988 - accuracy: 0.8459\n",
      "Epoch 25: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3987 - accuracy: 0.8459 - val_loss: 0.7346 - val_accuracy: 0.7529\n",
      "Epoch 26/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3929 - accuracy: 0.8481\n",
      "Epoch 26: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3933 - accuracy: 0.8478 - val_loss: 0.7654 - val_accuracy: 0.7542\n",
      "Epoch 27/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3891 - accuracy: 0.8494\n",
      "Epoch 27: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3892 - accuracy: 0.8493 - val_loss: 0.7695 - val_accuracy: 0.7553\n",
      "Epoch 28/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3853 - accuracy: 0.8499\n",
      "Epoch 28: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3854 - accuracy: 0.8499 - val_loss: 0.7682 - val_accuracy: 0.7484\n",
      "Epoch 29/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.8527\n",
      "Epoch 29: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3796 - accuracy: 0.8525 - val_loss: 0.7791 - val_accuracy: 0.7495\n",
      "Epoch 30/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3778 - accuracy: 0.8532\n",
      "Epoch 30: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3778 - accuracy: 0.8532 - val_loss: 0.7812 - val_accuracy: 0.7566\n",
      "Epoch 31/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3738 - accuracy: 0.8545\n",
      "Epoch 31: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3738 - accuracy: 0.8545 - val_loss: 0.7793 - val_accuracy: 0.7546\n",
      "Epoch 32/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3703 - accuracy: 0.8564\n",
      "Epoch 32: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3703 - accuracy: 0.8565 - val_loss: 0.7972 - val_accuracy: 0.7568\n",
      "Epoch 33/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8562\n",
      "Epoch 33: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3690 - accuracy: 0.8562 - val_loss: 0.8014 - val_accuracy: 0.7569\n",
      "Epoch 34/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8569\n",
      "Epoch 34: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3656 - accuracy: 0.8569 - val_loss: 0.7926 - val_accuracy: 0.7514\n",
      "Epoch 35/1000\n",
      "843/852 [============================>.] - ETA: 0s - loss: 0.3619 - accuracy: 0.8595\n",
      "Epoch 35: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3619 - accuracy: 0.8596 - val_loss: 0.7946 - val_accuracy: 0.7562\n",
      "Epoch 36/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3608 - accuracy: 0.8605\n",
      "Epoch 36: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3608 - accuracy: 0.8605 - val_loss: 0.7932 - val_accuracy: 0.7534\n",
      "Epoch 37/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8616\n",
      "Epoch 37: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3576 - accuracy: 0.8616 - val_loss: 0.8004 - val_accuracy: 0.7518\n",
      "Epoch 38/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3525 - accuracy: 0.8636\n",
      "Epoch 38: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3525 - accuracy: 0.8635 - val_loss: 0.8170 - val_accuracy: 0.7518\n",
      "Epoch 39/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.8636\n",
      "Epoch 39: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3526 - accuracy: 0.8636 - val_loss: 0.7999 - val_accuracy: 0.7524\n",
      "Epoch 40/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3511 - accuracy: 0.8652\n",
      "Epoch 40: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3513 - accuracy: 0.8651 - val_loss: 0.7979 - val_accuracy: 0.7487\n",
      "Epoch 41/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8663\n",
      "Epoch 41: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3463 - accuracy: 0.8663 - val_loss: 0.8184 - val_accuracy: 0.7525\n",
      "Epoch 42/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3474 - accuracy: 0.8653\n",
      "Epoch 42: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3472 - accuracy: 0.8655 - val_loss: 0.8241 - val_accuracy: 0.7503\n",
      "Epoch 43/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8663\n",
      "Epoch 43: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3446 - accuracy: 0.8663 - val_loss: 0.8275 - val_accuracy: 0.7583\n",
      "Epoch 44/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8679\n",
      "Epoch 44: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3393 - accuracy: 0.8679 - val_loss: 0.8654 - val_accuracy: 0.7520\n",
      "Epoch 45/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3427 - accuracy: 0.8679\n",
      "Epoch 45: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3424 - accuracy: 0.8681 - val_loss: 0.8065 - val_accuracy: 0.7562\n",
      "Epoch 46/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8693\n",
      "Epoch 46: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3369 - accuracy: 0.8694 - val_loss: 0.8207 - val_accuracy: 0.7568\n",
      "Epoch 47/1000\n",
      "847/852 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.8714\n",
      "Epoch 47: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3338 - accuracy: 0.8715 - val_loss: 0.8392 - val_accuracy: 0.7537\n",
      "Epoch 48/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.8694\n",
      "Epoch 48: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3343 - accuracy: 0.8694 - val_loss: 0.8370 - val_accuracy: 0.7505\n",
      "Epoch 49/1000\n",
      "848/852 [============================>.] - ETA: 0s - loss: 0.3320 - accuracy: 0.8717\n",
      "Epoch 49: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3324 - accuracy: 0.8716 - val_loss: 0.8357 - val_accuracy: 0.7521\n",
      "Epoch 50/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8710\n",
      "Epoch 50: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3323 - accuracy: 0.8711 - val_loss: 0.8323 - val_accuracy: 0.7479\n",
      "Epoch 51/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8730\n",
      "Epoch 51: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3305 - accuracy: 0.8728 - val_loss: 0.8253 - val_accuracy: 0.7517\n",
      "Epoch 52/1000\n",
      "842/852 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8744\n",
      "Epoch 52: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3269 - accuracy: 0.8744 - val_loss: 0.8447 - val_accuracy: 0.7421\n",
      "Epoch 53/1000\n",
      "849/852 [============================>.] - ETA: 0s - loss: 0.3269 - accuracy: 0.8750\n",
      "Epoch 53: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3269 - accuracy: 0.8750 - val_loss: 0.8274 - val_accuracy: 0.7501\n",
      "Epoch 54/1000\n",
      "851/852 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8769\n",
      "Epoch 54: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3222 - accuracy: 0.8770 - val_loss: 0.8424 - val_accuracy: 0.7475\n",
      "Epoch 55/1000\n",
      "841/852 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8749\n",
      "Epoch 55: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3248 - accuracy: 0.8749 - val_loss: 0.8348 - val_accuracy: 0.7545\n",
      "Epoch 56/1000\n",
      "844/852 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8755\n",
      "Epoch 56: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3221 - accuracy: 0.8755 - val_loss: 0.8613 - val_accuracy: 0.7459\n",
      "Epoch 57/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8765\n",
      "Epoch 57: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 5ms/step - loss: 0.3195 - accuracy: 0.8765 - val_loss: 0.8576 - val_accuracy: 0.7515\n",
      "Epoch 58/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8786\n",
      "Epoch 58: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3160 - accuracy: 0.8787 - val_loss: 0.8643 - val_accuracy: 0.7536\n",
      "Epoch 59/1000\n",
      "845/852 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.8777\n",
      "Epoch 59: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3184 - accuracy: 0.8778 - val_loss: 0.8531 - val_accuracy: 0.7532\n",
      "Epoch 60/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.8796\n",
      "Epoch 60: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3139 - accuracy: 0.8796 - val_loss: 0.8563 - val_accuracy: 0.7548\n",
      "Epoch 61/1000\n",
      "850/852 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8781\n",
      "Epoch 61: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3154 - accuracy: 0.8781 - val_loss: 0.8410 - val_accuracy: 0.7551\n",
      "Epoch 62/1000\n",
      "852/852 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.8797\n",
      "Epoch 62: val_accuracy did not improve from 0.75847\n",
      "852/852 [==============================] - 4s 4ms/step - loss: 0.3125 - accuracy: 0.8797 - val_loss: 0.8530 - val_accuracy: 0.7525\n",
      "380/380 [==============================] - 0s 520us/step\n",
      "0.7367839719396294\n",
      "\n",
      "0.7645844736113806\n",
      "0.7465841925459248\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "conf_mats = []\n",
    "f1s = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(features['csv'].unique())):\n",
    "    csv_train = features['csv'].unique()[train_index]\n",
    "    csv_test = features['csv'].unique()[test_index]\n",
    "\n",
    "    X_train = features[feats][(features['csv'].isin(csv_train))].to_numpy()\n",
    "    y_train = features['object'][(features['csv'].isin(csv_train))].to_numpy()\n",
    "\n",
    "    X_test = features[feats][(features['csv'].isin(csv_test))].to_numpy()\n",
    "    y_test = features['object'][(features['csv'].isin(csv_test))].to_numpy()\n",
    "    \n",
    "    X_train, X_test = z_score(X_train, X_test)\n",
    "        \n",
    "    Y_train = to_categorical(y_train, num_classes=5)\n",
    "    Y_test = to_categorical(y_test, num_classes=5)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy',patience=50, mode='max')\n",
    "\n",
    "    model.build(input_shape=X_train.shape)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('best_weights_dense_'+str(i)+'.hdf5', \n",
    "                                 monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    model.summary()\n",
    "    model.fit(X_train, Y_train, batch_size=128, epochs=1000, \n",
    "              validation_data=(X_test, Y_test), callbacks=[early_stopping, checkpoint])\n",
    "    model.load_weights('best_weights_dense_'+str(i)+'.hdf5')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    preds = np.argmax(model.predict(X_test),1)\n",
    "    test_acc.append(np.mean(preds==y_test))\n",
    "    print(f1_score(y_test, preds, average='macro'))\n",
    "\n",
    "    conf_mats.append(confusion_matrix(y_test,preds, labels=[0,1,2,3,4]))\n",
    "    f1s.append(f1_score(y_test, preds, average='macro'))\n",
    "print()\n",
    "print(np.mean(test_acc))\n",
    "print(np.mean(f1s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T22:01:13.348235Z",
     "start_time": "2023-12-12T22:01:12.613443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 1000\n",
    "%matplotlib inline\n",
    "classe = [\"Backpack\", \"Chair\", \"Desk\", \"Human\", \"Wall\"]\n",
    "\n",
    "mean_cf = np.mean([i for i in conf_mats],0)\n",
    "mean_cf\n",
    "\n",
    "df_cm = pd.DataFrame(mean_cf.astype(int), index = [i for i in classe],columns = [i for i in classe])\n",
    "plt.figure(figsize = (6,5))\n",
    "\n",
    "\n",
    "ax = sn.heatmap(df_cm, annot=True,fmt='g',cmap='Blues')\n",
    "\n",
    "ax.set_xlabel(xlabel='Actual class', fontsize=14, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel(ylabel='Predicted class', fontsize=14, fontdict=dict(weight='bold'))\n",
    "\n",
    "plt.savefig('conf_mat_ppg_cross_attention.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Subject Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T23:24:54.684364Z",
     "start_time": "2023-12-12T23:21:07.471295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_115 (Dense)           (98860, 256)              3584      \n",
      "                                                                 \n",
      " dense_116 (Dense)           (98860, 256)              65792     \n",
      "                                                                 \n",
      " dense_117 (Dense)           (98860, 128)              32896     \n",
      "                                                                 \n",
      " dense_118 (Dense)           (98860, 128)              16512     \n",
      "                                                                 \n",
      " dense_119 (Dense)           (98860, 5)                645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119,429\n",
      "Trainable params: 119,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "769/773 [============================>.] - ETA: 0s - loss: 0.8541 - accuracy: 0.6675\n",
      "Epoch 1: val_accuracy improved from -inf to 0.71860, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.8535 - accuracy: 0.6677 - val_loss: 0.7392 - val_accuracy: 0.7186\n",
      "Epoch 2/1000\n",
      "768/773 [============================>.] - ETA: 0s - loss: 0.7054 - accuracy: 0.7262\n",
      "Epoch 2: val_accuracy improved from 0.71860 to 0.73550, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.7053 - accuracy: 0.7263 - val_loss: 0.6847 - val_accuracy: 0.7355\n",
      "Epoch 3/1000\n",
      "762/773 [============================>.] - ETA: 0s - loss: 0.6385 - accuracy: 0.7512\n",
      "Epoch 3: val_accuracy improved from 0.73550 to 0.76657, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.6382 - accuracy: 0.7514 - val_loss: 0.6242 - val_accuracy: 0.7666\n",
      "Epoch 4/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.5918 - accuracy: 0.7689\n",
      "Epoch 4: val_accuracy did not improve from 0.76657\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.5917 - accuracy: 0.7689 - val_loss: 0.6506 - val_accuracy: 0.7488\n",
      "Epoch 5/1000\n",
      "768/773 [============================>.] - ETA: 0s - loss: 0.5548 - accuracy: 0.7855\n",
      "Epoch 5: val_accuracy did not improve from 0.76657\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.5547 - accuracy: 0.7855 - val_loss: 0.6211 - val_accuracy: 0.7637\n",
      "Epoch 6/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.5177 - accuracy: 0.7991\n",
      "Epoch 6: val_accuracy did not improve from 0.76657\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.5175 - accuracy: 0.7991 - val_loss: 0.6313 - val_accuracy: 0.7609\n",
      "Epoch 7/1000\n",
      "758/773 [============================>.] - ETA: 0s - loss: 0.4882 - accuracy: 0.8103\n",
      "Epoch 7: val_accuracy improved from 0.76657 to 0.76670, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.4886 - accuracy: 0.8099 - val_loss: 0.6340 - val_accuracy: 0.7667\n",
      "Epoch 8/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.4587 - accuracy: 0.8228\n",
      "Epoch 8: val_accuracy improved from 0.76670 to 0.76845, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.4590 - accuracy: 0.8227 - val_loss: 0.6224 - val_accuracy: 0.7684\n",
      "Epoch 9/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.8336\n",
      "Epoch 9: val_accuracy did not improve from 0.76845\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.4305 - accuracy: 0.8337 - val_loss: 0.6415 - val_accuracy: 0.7670\n",
      "Epoch 10/1000\n",
      "765/773 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8430\n",
      "Epoch 10: val_accuracy did not improve from 0.76845\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.4065 - accuracy: 0.8428 - val_loss: 0.6554 - val_accuracy: 0.7627\n",
      "Epoch 11/1000\n",
      "769/773 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.8501\n",
      "Epoch 11: val_accuracy did not improve from 0.76845\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.3847 - accuracy: 0.8502 - val_loss: 0.7147 - val_accuracy: 0.7627\n",
      "Epoch 12/1000\n",
      "769/773 [============================>.] - ETA: 0s - loss: 0.3622 - accuracy: 0.8609\n",
      "Epoch 12: val_accuracy did not improve from 0.76845\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.3622 - accuracy: 0.8609 - val_loss: 0.6930 - val_accuracy: 0.7603\n",
      "Epoch 13/1000\n",
      "771/773 [============================>.] - ETA: 0s - loss: 0.3459 - accuracy: 0.8671\n",
      "Epoch 13: val_accuracy improved from 0.76845 to 0.77316, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.3460 - accuracy: 0.8671 - val_loss: 0.7016 - val_accuracy: 0.7732\n",
      "Epoch 14/1000\n",
      "769/773 [============================>.] - ETA: 0s - loss: 0.3262 - accuracy: 0.8731\n",
      "Epoch 14: val_accuracy did not improve from 0.77316\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.3262 - accuracy: 0.8731 - val_loss: 0.7351 - val_accuracy: 0.7608\n",
      "Epoch 15/1000\n",
      "758/773 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8820\n",
      "Epoch 15: val_accuracy did not improve from 0.77316\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.3082 - accuracy: 0.8820 - val_loss: 0.7794 - val_accuracy: 0.7600\n",
      "Epoch 16/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8860\n",
      "Epoch 16: val_accuracy improved from 0.77316 to 0.77544, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2943 - accuracy: 0.8860 - val_loss: 0.7092 - val_accuracy: 0.7754\n",
      "Epoch 17/1000\n",
      "769/773 [============================>.] - ETA: 0s - loss: 0.2814 - accuracy: 0.8925\n",
      "Epoch 17: val_accuracy did not improve from 0.77544\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2811 - accuracy: 0.8926 - val_loss: 0.7751 - val_accuracy: 0.7649\n",
      "Epoch 18/1000\n",
      "769/773 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.8981\n",
      "Epoch 18: val_accuracy improved from 0.77544 to 0.77580, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2676 - accuracy: 0.8981 - val_loss: 0.7535 - val_accuracy: 0.7758\n",
      "Epoch 19/1000\n",
      "763/773 [============================>.] - ETA: 0s - loss: 0.2552 - accuracy: 0.9025\n",
      "Epoch 19: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 3ms/step - loss: 0.2554 - accuracy: 0.9023 - val_loss: 0.7958 - val_accuracy: 0.7709\n",
      "Epoch 20/1000\n",
      "765/773 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9066\n",
      "Epoch 20: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2420 - accuracy: 0.9067 - val_loss: 0.8567 - val_accuracy: 0.7717\n",
      "Epoch 21/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9091\n",
      "Epoch 21: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2357 - accuracy: 0.9089 - val_loss: 0.8295 - val_accuracy: 0.7716\n",
      "Epoch 22/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9148\n",
      "Epoch 22: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2211 - accuracy: 0.9149 - val_loss: 0.8903 - val_accuracy: 0.7738\n",
      "Epoch 23/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.2135 - accuracy: 0.9185\n",
      "Epoch 23: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2136 - accuracy: 0.9184 - val_loss: 0.9663 - val_accuracy: 0.7578\n",
      "Epoch 24/1000\n",
      "773/773 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9202\n",
      "Epoch 24: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.2087 - accuracy: 0.9202 - val_loss: 0.8930 - val_accuracy: 0.7758\n",
      "Epoch 25/1000\n",
      "773/773 [==============================] - ETA: 0s - loss: 0.1998 - accuracy: 0.9225\n",
      "Epoch 25: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1998 - accuracy: 0.9225 - val_loss: 0.9688 - val_accuracy: 0.7675\n",
      "Epoch 26/1000\n",
      "763/773 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.9264\n",
      "Epoch 26: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1926 - accuracy: 0.9262 - val_loss: 0.9574 - val_accuracy: 0.7719\n",
      "Epoch 27/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.9310\n",
      "Epoch 27: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1813 - accuracy: 0.9310 - val_loss: 1.0061 - val_accuracy: 0.7719\n",
      "Epoch 28/1000\n",
      "771/773 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9321\n",
      "Epoch 28: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1789 - accuracy: 0.9321 - val_loss: 1.0107 - val_accuracy: 0.7693\n",
      "Epoch 29/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9332\n",
      "Epoch 29: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1751 - accuracy: 0.9332 - val_loss: 1.0502 - val_accuracy: 0.7694\n",
      "Epoch 30/1000\n",
      "771/773 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9364\n",
      "Epoch 30: val_accuracy did not improve from 0.77580\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1662 - accuracy: 0.9363 - val_loss: 1.0406 - val_accuracy: 0.7686\n",
      "Epoch 31/1000\n",
      "765/773 [============================>.] - ETA: 0s - loss: 0.1633 - accuracy: 0.9382\n",
      "Epoch 31: val_accuracy improved from 0.77580 to 0.77966, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1633 - accuracy: 0.9381 - val_loss: 1.0295 - val_accuracy: 0.7797\n",
      "Epoch 32/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.1554 - accuracy: 0.9414\n",
      "Epoch 32: val_accuracy did not improve from 0.77966\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1556 - accuracy: 0.9413 - val_loss: 1.1180 - val_accuracy: 0.7680\n",
      "Epoch 33/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.1517 - accuracy: 0.9430\n",
      "Epoch 33: val_accuracy did not improve from 0.77966\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1519 - accuracy: 0.9430 - val_loss: 1.1672 - val_accuracy: 0.7686\n",
      "Epoch 34/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.1485 - accuracy: 0.9435\n",
      "Epoch 34: val_accuracy did not improve from 0.77966\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1488 - accuracy: 0.9433 - val_loss: 1.1317 - val_accuracy: 0.7788\n",
      "Epoch 35/1000\n",
      "772/773 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.9457\n",
      "Epoch 35: val_accuracy improved from 0.77966 to 0.78297, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1441 - accuracy: 0.9457 - val_loss: 1.0874 - val_accuracy: 0.7830\n",
      "Epoch 36/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9490\n",
      "Epoch 36: val_accuracy did not improve from 0.78297\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1360 - accuracy: 0.9490 - val_loss: 1.1973 - val_accuracy: 0.7684\n",
      "Epoch 37/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.9488\n",
      "Epoch 37: val_accuracy did not improve from 0.78297\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1355 - accuracy: 0.9488 - val_loss: 1.2326 - val_accuracy: 0.7721\n",
      "Epoch 38/1000\n",
      "762/773 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.9508\n",
      "Epoch 38: val_accuracy did not improve from 0.78297\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1317 - accuracy: 0.9505 - val_loss: 1.1506 - val_accuracy: 0.7710\n",
      "Epoch 39/1000\n",
      "760/773 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9505\n",
      "Epoch 39: val_accuracy improved from 0.78297 to 0.78378, saving model to best_weights_dense_losos_1.hdf5\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1310 - accuracy: 0.9506 - val_loss: 1.1814 - val_accuracy: 0.7838\n",
      "Epoch 40/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9540\n",
      "Epoch 40: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1242 - accuracy: 0.9541 - val_loss: 1.3513 - val_accuracy: 0.7588\n",
      "Epoch 41/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9544\n",
      "Epoch 41: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1216 - accuracy: 0.9545 - val_loss: 1.2708 - val_accuracy: 0.7696\n",
      "Epoch 42/1000\n",
      "765/773 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9554\n",
      "Epoch 42: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1200 - accuracy: 0.9553 - val_loss: 1.2228 - val_accuracy: 0.7791\n",
      "Epoch 43/1000\n",
      "762/773 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9571\n",
      "Epoch 43: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1151 - accuracy: 0.9570 - val_loss: 1.2983 - val_accuracy: 0.7743\n",
      "Epoch 44/1000\n",
      "763/773 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9589\n",
      "Epoch 44: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1111 - accuracy: 0.9588 - val_loss: 1.3580 - val_accuracy: 0.7731\n",
      "Epoch 45/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9575\n",
      "Epoch 45: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1139 - accuracy: 0.9574 - val_loss: 1.3514 - val_accuracy: 0.7729\n",
      "Epoch 46/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.9606\n",
      "Epoch 46: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1067 - accuracy: 0.9605 - val_loss: 1.4275 - val_accuracy: 0.7656\n",
      "Epoch 47/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.1024 - accuracy: 0.9607\n",
      "Epoch 47: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1025 - accuracy: 0.9607 - val_loss: 1.3992 - val_accuracy: 0.7792\n",
      "Epoch 48/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9604\n",
      "Epoch 48: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1076 - accuracy: 0.9603 - val_loss: 1.4039 - val_accuracy: 0.7761\n",
      "Epoch 49/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9627\n",
      "Epoch 49: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1003 - accuracy: 0.9626 - val_loss: 1.4103 - val_accuracy: 0.7780\n",
      "Epoch 50/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9624\n",
      "Epoch 50: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.1009 - accuracy: 0.9626 - val_loss: 1.4431 - val_accuracy: 0.7750\n",
      "Epoch 51/1000\n",
      "763/773 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9644\n",
      "Epoch 51: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0966 - accuracy: 0.9643 - val_loss: 1.5646 - val_accuracy: 0.7665\n",
      "Epoch 52/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9636\n",
      "Epoch 52: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0966 - accuracy: 0.9637 - val_loss: 1.5732 - val_accuracy: 0.7636\n",
      "Epoch 53/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 0.9645\n",
      "Epoch 53: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0975 - accuracy: 0.9645 - val_loss: 1.5517 - val_accuracy: 0.7651\n",
      "Epoch 54/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9672\n",
      "Epoch 54: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0897 - accuracy: 0.9672 - val_loss: 1.5300 - val_accuracy: 0.7680\n",
      "Epoch 55/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9651\n",
      "Epoch 55: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0936 - accuracy: 0.9651 - val_loss: 1.5320 - val_accuracy: 0.7745\n",
      "Epoch 56/1000\n",
      "772/773 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9655\n",
      "Epoch 56: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0952 - accuracy: 0.9655 - val_loss: 1.5541 - val_accuracy: 0.7751\n",
      "Epoch 57/1000\n",
      "773/773 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9681\n",
      "Epoch 57: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0879 - accuracy: 0.9681 - val_loss: 1.4715 - val_accuracy: 0.7725\n",
      "Epoch 58/1000\n",
      "760/773 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9709\n",
      "Epoch 58: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0782 - accuracy: 0.9709 - val_loss: 1.5810 - val_accuracy: 0.7788\n",
      "Epoch 59/1000\n",
      "772/773 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9678\n",
      "Epoch 59: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0881 - accuracy: 0.9678 - val_loss: 1.6504 - val_accuracy: 0.7726\n",
      "Epoch 60/1000\n",
      "773/773 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9671\n",
      "Epoch 60: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 3ms/step - loss: 0.0911 - accuracy: 0.9671 - val_loss: 1.5584 - val_accuracy: 0.7754\n",
      "Epoch 61/1000\n",
      "758/773 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9713\n",
      "Epoch 61: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 3ms/step - loss: 0.0789 - accuracy: 0.9712 - val_loss: 1.6315 - val_accuracy: 0.7722\n",
      "Epoch 62/1000\n",
      "758/773 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9713\n",
      "Epoch 62: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0798 - accuracy: 0.9711 - val_loss: 1.6905 - val_accuracy: 0.7722\n",
      "Epoch 63/1000\n",
      "768/773 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9708\n",
      "Epoch 63: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0835 - accuracy: 0.9707 - val_loss: 1.6863 - val_accuracy: 0.7678\n",
      "Epoch 64/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.0819 - accuracy: 0.9703\n",
      "Epoch 64: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0819 - accuracy: 0.9703 - val_loss: 1.6807 - val_accuracy: 0.7766\n",
      "Epoch 65/1000\n",
      "759/773 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9730\n",
      "Epoch 65: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0742 - accuracy: 0.9728 - val_loss: 1.6578 - val_accuracy: 0.7705\n",
      "Epoch 66/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9715\n",
      "Epoch 66: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0786 - accuracy: 0.9714 - val_loss: 1.7139 - val_accuracy: 0.7726\n",
      "Epoch 67/1000\n",
      "760/773 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9736\n",
      "Epoch 67: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0730 - accuracy: 0.9734 - val_loss: 1.7733 - val_accuracy: 0.7696\n",
      "Epoch 68/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9744\n",
      "Epoch 68: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0697 - accuracy: 0.9744 - val_loss: 1.7418 - val_accuracy: 0.7732\n",
      "Epoch 69/1000\n",
      "765/773 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9719\n",
      "Epoch 69: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0780 - accuracy: 0.9719 - val_loss: 1.7074 - val_accuracy: 0.7728\n",
      "Epoch 70/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9736\n",
      "Epoch 70: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0716 - accuracy: 0.9736 - val_loss: 1.8189 - val_accuracy: 0.7712\n",
      "Epoch 71/1000\n",
      "761/773 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9743\n",
      "Epoch 71: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0716 - accuracy: 0.9742 - val_loss: 1.7529 - val_accuracy: 0.7713\n",
      "Epoch 72/1000\n",
      "760/773 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9738\n",
      "Epoch 72: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0740 - accuracy: 0.9737 - val_loss: 1.8014 - val_accuracy: 0.7624\n",
      "Epoch 73/1000\n",
      "764/773 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9755\n",
      "Epoch 73: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0672 - accuracy: 0.9754 - val_loss: 1.7138 - val_accuracy: 0.7764\n",
      "Epoch 74/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9744\n",
      "Epoch 74: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0716 - accuracy: 0.9744 - val_loss: 1.8837 - val_accuracy: 0.7713\n",
      "Epoch 75/1000\n",
      "770/773 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9749\n",
      "Epoch 75: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0700 - accuracy: 0.9749 - val_loss: 1.8468 - val_accuracy: 0.7718\n",
      "Epoch 76/1000\n",
      "769/773 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9771\n",
      "Epoch 76: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0651 - accuracy: 0.9771 - val_loss: 1.9089 - val_accuracy: 0.7696\n",
      "Epoch 77/1000\n",
      "767/773 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9766\n",
      "Epoch 77: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0672 - accuracy: 0.9767 - val_loss: 1.9405 - val_accuracy: 0.7675\n",
      "Epoch 78/1000\n",
      "762/773 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9743\n",
      "Epoch 78: val_accuracy did not improve from 0.78378\n",
      "773/773 [==============================] - 3s 4ms/step - loss: 0.0732 - accuracy: 0.9742 - val_loss: 1.9126 - val_accuracy: 0.7645\n",
      "Epoch 79/1000\n",
      " 60/773 [=>............................] - ETA: 2s - loss: 0.0696 - accuracy: 0.9760"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [279]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_weights_dense_losos_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(s)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     53\u001b[0m                              monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 56\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_weights_dense_losos_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(s)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/MachineLearning/ml-environment/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/MachineLearning/ml-environment/lib/python3.8/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/MachineLearning/ml-environment/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/MachineLearning/ml-environment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:904\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    903\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m without_tracing:\n\u001b[0;32m--> 904\u001b[0m     \u001b[43m_frequent_tracing_detector_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalled_without_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key_for_call_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     _frequent_tracing_detector_manager\u001b[38;5;241m.\u001b[39mcalled_with_tracing(\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_for_call_stats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_function,\n\u001b[1;32m    909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_omit_frequent_tracing_warning)\n",
      "File \u001b[0;32m~/MachineLearning/ml-environment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:195\u001b[0m, in \u001b[0;36m_FrequentTracingDetectorManager.called_without_tracing\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalled_without_tracing\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 195\u001b[0m     detector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     detector\u001b[38;5;241m.\u001b[39mcalled_without_tracing()\n",
      "File \u001b[0;32m~/MachineLearning/ml-environment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:191\u001b[0m, in \u001b[0;36m_FrequentTracingDetectorManager._get_detector\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detectors:\n\u001b[1;32m    190\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detectors[key] \u001b[38;5;241m=\u001b[39m _FrequentTracingDetector()\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/weakref.py:383\u001b[0m, in \u001b[0;36mWeakKeyDictionary.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "conf_mats = []\n",
    "f1s = []\n",
    "\n",
    "for s in range(1,11):\n",
    "    X_train = features[feats][(features['subject']!=s)].to_numpy()\n",
    "    y_train = features['object'][(features['subject']!=s)].to_numpy()\n",
    "\n",
    "    X_test = features[feats][(features['subject']==s)].to_numpy()\n",
    "    y_test = features['object'][(features['subject']==s )].to_numpy()\n",
    "    \n",
    "    X_train, X_test = z_score(X_train, X_test)\n",
    "    \n",
    "    Y_train = to_categorical(y_train, num_classes=5)\n",
    "    Y_test = to_categorical(y_test, num_classes=5)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy',patience=50, mode='max')\n",
    "\n",
    "    model.build(input_shape=X_train.shape)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('best_weights_dense_losos_'+str(s)+'.hdf5', \n",
    "                                 monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "    model.summary()\n",
    "    model.fit(X_train, Y_train, batch_size=128, epochs=1000, \n",
    "              validation_data=(X_test, Y_test), callbacks=[early_stopping, checkpoint])\n",
    "    model.load_weights('best_weights_dense_losos_'+str(s)+'.hdf5')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    preds = np.argmax(model.predict(X_test),1)\n",
    "    test_acc.append(np.mean(preds==y_test))\n",
    "    print(f1_score(y_test, preds, average='macro'))\n",
    "\n",
    "    conf_mats.append(confusion_matrix(y_test,preds, labels=[0,1,2,3,4]))\n",
    "    f1s.append(f1_score(y_test, preds, average='macro'))\n",
    "print()\n",
    "print(np.mean(test_acc))\n",
    "print(np.mean(f1s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T23:06:15.169915Z",
     "start_time": "2023-12-12T23:06:14.403181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 1000\n",
    "%matplotlib inline\n",
    "classe = [\"Backpack\", \"Chair\", \"Desk\", \"Human\", \"Wall\"]\n",
    "\n",
    "mean_cf = np.mean([i for i in conf_mats],0)\n",
    "mean_cf\n",
    "\n",
    "df_cm = pd.DataFrame(mean_cf.astype(int), index = [i for i in classe],columns = [i for i in classe])\n",
    "plt.figure(figsize = (6,5))\n",
    "\n",
    "\n",
    "ax = sn.heatmap(df_cm, annot=True,fmt='g',cmap='Blues')\n",
    "\n",
    "ax.set_xlabel(xlabel='Actual class', fontsize=14, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel(ylabel='Predicted class', fontsize=14, fontdict=dict(weight='bold'))\n",
    "\n",
    "plt.savefig('conf_mat_ppg_cross_attention.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
